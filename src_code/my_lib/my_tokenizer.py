#coding=utf-8
import nltk
import re

def tokenize_english(text,vocabs=None,keep_punc=True,keep_stopword=True,lemmatize=True,lower=True):
    '''
    ä½¿ç”¨nltkåˆ†è¯
    :param text: å¾…åˆ†è¯çš„è‹±æ–‡æ–‡æœ¬
    :param keep_punc: æ˜¯å¦ä¿ç•™æ ‡ç‚¹ç¬¦å·
    :param keep_punc: æ˜¯å¦åšè¯å¹²æå–ï¼Œæ–‡æœ¬åŒ¹é…æ—¶å¾ˆæœ‰ç”¨
    :return: åˆ†è¯åçš„è¯è¯­åˆ—è¡¨
    '''
    # if lower:
    #     text=text.lower()

    text=text.replace("``",'"').replace("''",'"').replace('`',"'")
    text=re.sub(r"https?:\/\/\S+\b|www\.(\w+\.)+\S*",r'<url>',text,re.S)

    text=' - '.join(text.split('-'))
    # text=text.replace('- -','--')
    words=nltk.word_tokenize(text)
    if lemmatize:
        lemmatizer = nltk.stem.WordNetLemmatizer()  # è¯å¹²æå–
        if vocabs is not None:
            words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word,pos='n'),pos='v'),pos='a')
                     if word not in vocabs else word for word in words]
        else:
            words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word,pos='n'),pos='v'),pos='a')
                     for word in words]

    if not keep_punc:
        stop_puncs_0 = ['|', '{}', '()', '[]', '&', '*',
                        '/', '//', '#', '\\', '~', '""', 'â€–', 'Â§']
        stop_puncs_1 = ['ã€', '\'', '"', '.', ':', ',', '...', '{', '}', '(', ')', '[', ']',
                        ';', '?', '!', '-', '--']
        # stop_puncs_2 = ["``", "''",'`']
        stop_puncs = stop_puncs_0 + stop_puncs_1
        words=[word for word in words if word not in stop_puncs]
    if not keep_stopword:
        stop_words = nltk.corpus.stopwords.words('english')
        words=[word for word in words if word not in stop_words]
    if lower:
        if vocabs is not None:
            words=[word.lower() if word not in vocabs else word for word in words]
        else:
            words=[word.lower() for word in words]
    return words

def tokenize_glove(text,vocabs=None,keep_punc=True,keep_stopword=True,lemmatize=True,lower=False):
    '''
    é’ˆå¯¹gloveè¯åº“åšåˆ†è¯
    :param text:
    :param vocabs: gloveå­—å…¸,gloveå­—å…¸ä¸­è¯è¯­åŒºåˆ†å¤§å°å†™
    :param keep_punc:
    :param keep_stopword:
    :param lemmatize:
    :param lower:
    :return:
    '''
    # stop_puncs=['"','.',':',',','...','|','{','}','{}','(',')','()','[',']','[]','&','*','`',
    #             '/','//','#','\\','~','ã€',';','?','!','\'','-','--','""','â€–','Â§']

    # stemmer=nltk.stem.SnowballStemmer(language='english')
    # lemmatizer = nltk.stem.WordNetLemmatizer()  #è¯å¹²æå–
    # text = (' ' + text + ' ').lower()
    # if text.startwith('\''):
    #     text=text.lstrip('\'')
    #     text='\' '+text
    # eyes = "[8:=;]"
    # nose = "['`\-]?"
    text=text.replace("``",'"').replace("''",'"').replace('`',"'")
    text=re.sub(r"https?:\/\/\S+\b|www\.(\w+\.)+\S*",r'<url>',text,re.S)

    text=re.sub(r"#{[8:=;]}#{['`\-]?}[)d]+|[)d]+#{['`\-]?}#{[8:=;]}",'<smile>',text,re.S)
    text = re.sub(r"#{[8:=;]}#{['`\-]?}p+", '<lolface>', text,re.S)
    text = re.sub(r"#{[8:=;]}#{['`\-]?}\(+|\)+#{['`\-]?}#{[8:=;]}", '<sadface>', text, re.S)
    text = re.sub(r"#{[8:=;]}#{['`\-]?}[\/|l*]", '<neutralface>', text, re.S)
    text = re.sub(r"<3", '<heart>', text,re.S)
    text = re.sub(r"[-+]?[.\d]*[\d]+[:,.\d]*", '<number>', text, re.S)

    text = (' ' + text).replace(' \'',' \' ').strip()
    # text = (' ' + text + ' ').lower().lstrip(' \'').rstrip('\' ').strip()
    text=' - '.join(text.split('-'))
    # text=text.replace('- -','--')
    words=nltk.word_tokenize(text)
    if lemmatize:
        lemmatizer = nltk.stem.WordNetLemmatizer()  # è¯å¹²æå–

        if vocabs is not None:
            words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word,pos='n'),pos='v'),pos='a')
                     if word not in vocabs else word for word in words]

        else:
            words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word,pos='n'),pos='v'),pos='a')
                     for word in words]
    words = ' '.join(words).replace('< url >', '<url>').replace('< smile >', '<smile>') \
        .replace('< lolface >', '<lolface>').replace('< sadface >', '<sadface>') \
        .replace('< neutralface >', '<neutralface>').replace('< heart >', '<v>') \
        .replace('< number >', '<number>').split()
    # words = text.split()
    if not keep_punc:
        stop_puncs_0 = ['|', '{}', '()', '[]', '&', '*',
                        '/', '//', '#', '\\', '~', '""', 'â€–', 'Â§']
        stop_puncs_1 = ['ã€', '\'', '"', '.', ':', ',', '...', '{', '}', '(', ')', '[', ']',
                        ';', '?', '!', '-', '--']
        # stop_puncs_2 = ["``", "''",'`']
        stop_puncs = stop_puncs_0 + stop_puncs_1
        words=[word for word in words if word not in stop_puncs]
    if not keep_stopword:
        stop_words = nltk.corpus.stopwords.words('english')
        words=[word for word in words if word not in stop_words]
    if lower:
        if vocabs is not None:
            words=[word.lower() if word not in vocabs else word for word in words]
        else:
            words=[word.lower() for word in words]
    return words

from nltk import WordPunctTokenizer
import string
punc_str2="""â€–`Â§ï¼ï¼Ÿï½¡ï¼‚ï¼ƒï¼„ï¼…ï¼†ï¼‡ï¼ˆï¼‰ï¼Šï¼‹ï¼ï¼ï¼šï¼›ï¼œï¼ï¼ï¼ ï¼»ï¼¼ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½ï½ï½Ÿï½ ï½¢ï½£ï½¤ã€ã€ƒã€‹ã€Œã€ã€ã€ã€ã€‘ã€”ã€•ã€–ã€—ã€˜ã€™ã€šã€›ã€œã€ã€ã€Ÿã€°ã€¾ã€¿â€“â€”â€˜'â€›â€œâ€â€â€Ÿâ€¦â€§ï¹"""
punc_str=string.punctuation+punc_str2
def tokenize_python(code,keep_puc=False,punc_str=punc_str):
    code = code.replace('_', ' _ ')
    tokens = WordPunctTokenizer().tokenize(code)
    # tokens = nltk.word_tokenize(code)
    # for punc in punc_str:
    #     code=code.replace(punc,' '+punc+' ')
    # code= ' '.join(tokens)
    if not keep_puc:
        code=re.sub(r'[{}]'.format(punc_str),' ',' '.join(tokens),re.S)
        tokens=code.split()
    # if keep_puc:
    #     tokens=list(filter(lambda x: x not in pucs,tokens))
    return tokens
# def tokenize_text_en(text,
#                      vocabs=None,
#                      keep_mark=True,
#                      lemmatize=False,
#                      lower=False,
#                      dash_sep=False):
#     '''
#     ä½¿ç”¨nltkåˆ†è¯
#     :param text: å¾…åˆ†è¯çš„è‹±æ–‡æ–‡æœ¬
#     :param vocabs: æ˜¯å¦åœ¨è¯æ ¹æå–æ—¶æ·»åŠ å‚è€ƒè¯åº“ï¼Œåœ¨è¯åº“ä¸­çš„ä¸åšè¯æ ¹æå–
#     :param keep_mark: æ˜¯å¦ä¿ç•™æ ‡ç‚¹ç¬¦å·
#     :param lemmatize: æ˜¯å¦åšè¯æ ¹æå–
#     :param lower: æ˜¯å¦å…¨éƒ¨è½¬æ¢æˆå°å†™
#     :param dash_sep: æ˜¯å¦å°†dashè¿æ¥çš„è¯è¯­åˆ†ç¦»å¼€
#     :return:
#     '''
#     '''
#
#     :param text: å¾…åˆ†è¯çš„è‹±æ–‡æ–‡æœ¬
#     :param keep_mark: æ˜¯å¦ä¿ç•™æ ‡ç‚¹ç¬¦å·
#     :param keep_mark: æ˜¯å¦åšè¯å¹²æå–ï¼Œæ–‡æœ¬åŒ¹é…æ—¶å¾ˆæœ‰ç”¨
#     :return: åˆ†è¯åçš„è¯è¯­åˆ—è¡¨
#     '''
#     # stop_marks=['"','.',':',',','...','|','{','}','{}','(',')','()','[',']','[]','&','*','`',
#     #             '/','//','#','\\','~','ã€',';','?','!','\'','-','--','""','â€–','Â§']
#     stop_marks_0 = ['|', '{}', '()', '[]', '&', '*', '`',
#                     '/', '//', '#', '\\', '~', '""', 'â€–', 'Â§']
#     stop_marks_1 = ['ã€', '\'','"', '.', ':', ',', '...', '{', '}', '(', ')', '[', ']',
#                     ';', '?', '!', '-', '--']
#     # stemmer=nltk.stem.SnowballStemmer(language='english')
#     # lemmatizer = nltk.stem.WordNetLemmatizer()  #è¯å¹²æå–
#     # text = (' ' + text + ' ').lower()
#     # if text.startwith('\''):
#     #     text=text.lstrip('\'')
#     #     text='\' '+text
#     # eyes = "[8:=;]"
#     # nose = "['`\-]?"
#
#     text=re.sub(r"https?:\/\/\S+\b|www\.(\w+\.)+\S*",'<url>',text,re.S)
#     text=re.sub(r"#{[8:=;]}#{['`\-]?}[)d]+|[)d]+#{['`\-]?}#{[8:=;]}",'<smile>',text,re.S)
#     text = re.sub(r"#{[8:=;]}#{['`\-]?}p+", '<lolface>', text,re.S)
#     text = re.sub(r"#{[8:=;]}#{['`\-]?}\(+|\)+#{['`\-]?}#{[8:=;]}", '<sadface>', text, re.S)
#     text = re.sub(r"#{[8:=;]}#{['`\-]?}[\/|l*]", '<neutralface>', text, re.S)
#     text = re.sub(r"<3", '<heart>', text,re.S)
#     text = re.sub(r"[-+]?[.\d]*[\d]+[:,.\d]*", '<number>', text, re.S)
#
#     # text=re.sub("<3", '<HEART>', text)
#     if lower:
#         text=text.lower()
#     text = (' ' + text).replace(' \'',' \' ').strip()
#     # text = (' ' + text + ' ').lower().lstrip(' \'').rstrip('\' ').strip()
#
#     if dash_sep:
#         text=' - '.join(text.split('-'))
#
#     # text=text.replace('- -','--')
#     words=nltk.word_tokenize(text)
#
#     if lemmatize:   #æ˜¯å¦æ ¹æ®å…·ä½“æƒ…å†µåšè¯å¹²æå–
#         lemmatizer = nltk.stem.WordNetLemmatizer()  # è¯å¹²æå–
#
#     if isinstance(vocabs,list):
#         for i,word in enumerate(words):
#             if word not in vocabs:
#                 word=word[:1]+word[1:].lower()  #é™¤é¦–å­—æ¯å¤–éƒ½å°å†™
#                 if word not in vocabs:
#                     word=word.lower()   #å…¨éƒ¨å°å†™
#                     if word not in vocabs and lemmatizer:
#                         word=lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, pos='n'), pos='v'),pos='a')
#             words[i]=word
#     elif vocabs is None:
#         if lemmatize:
#             words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, pos='n'), pos='v'), pos='a')
#                      for word in words]
#
#     # if lemmatize:
#     #     lemmatizer = nltk.stem.WordNetLemmatizer()  # è¯å¹²æå–
#     #
#     #     if vocabs is not None:
#     #         words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word,pos='n'),pos='v'),pos='a')
#     #                  if word not in vocabs else word for word in words]
#     #
#     #     else:
#     #         words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word,pos='n'),pos='v'),pos='a')
#     #                  for word in words]
#
#     text=' '.join(words).replace("``",'"').replace("''",'"')
#
#     if lower:
#         text=text.replace('< url >','<url>').replace('< smile >','<smile>')\
#             .replace('< lolface >','<lolface>').replace('< sadface >','<sadface>')\
#             .replace('< neutralface >','<neutralface>').replace('< heart >','<v>')\
#             .replace('< number >','<number>')
#
#     words=text.split()
#     words = [word for word in words if word not in stop_marks_0]
#     if not keep_mark:
#         words=[word for word in words if word not in stop_marks_1]
#     return words


# def parse_chinese(text,model_path,visual=False):
#     '''
#     ä½¿ç”¨æ–¯å¦ç¦è§£æå·¥å…·å¯¹ä¸­æ–‡æ–‡æœ¬è¿›è¡Œè¯­æ³•è§£æ
#     https://www.jianshu.com/p/002157665bfd
#     @InProceedings{manning-EtAl:2014:P14-5,
#       author    = {Manning, Christopher D. and  Surdeanu, Mihai  and  Bauer, John  and  Finkel, Jenny  and  Bethard, Steven J. and  McClosky, David},
#       title     = {The {Stanford} {CoreNLP} Natural Language Processing Toolkit},
#       booktitle = {Association for Computational Linguistics (ACL) System Demonstrations},
#       year      = {2014},
#       pages     = {55--60},
#       url       = {http://www.aclweb.org/anthology/P/P14/P14-5010}
#     }
#     :param text:
#     :param model_path: path of stanford-corenlp-4.0.0-models-chinese.jar
#     :return:
#     '''
#     nlp=StanfordCoreNLP(model_path)




if __name__=='__main__':
    # s='she does not want to did-it inter-mediate is went fully <number> <LOLFACE>.'
    # print(tokenize_text_en(s))

    text='https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb '
    text=re.sub(r"https?:\/\/\S+\b|www\.(\w+\.)+\S*", '<URL>', text,re.S)
    print(text)

    text = 'ğŸ˜Š#{eyes}#{nose})d'
    text = re.sub(r"#{eyes}#{nose}[)d]+|[)d]+#{nose}#{eyes}",'<SMILE>',text,re.S)
    print(text)

    text='a3asdfasd32 234 adj'
    text = re.sub(r"[-+]?[.\d]*[\d]+[:,.\d]*", '<NUMBER>', text, re.S)
    print(text)

    code='''
    def tokenize_python(code,keep_puc=False,punc_str=punc_str):
    code = code.replace('_', ' _ ')
    tokens = WordPunctTokenizer().tokenize(code)'''
    print(tokenize_python(code))