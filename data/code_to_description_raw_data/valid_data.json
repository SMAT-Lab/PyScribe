[
    {
        "code": "\nimport seaborn as sns  \nsns.set(context='notebook')\ncosts = np.squeeze(d['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\nplt.show()",
        "text": "let  s also plot the cost function and the gradient  . ",
        "id": 1
    },
    {
        "code": "\ndef goat_door(prizedoors, guesses):\n    \n    \n    result = np.random.randint(0, 3, prizedoors.size)\n    while True:\n        bad = (result == prizedoors) | (result == guesses)\n        if not bad.any():\n            return result\n        result[bad] = np.random.randint(0, 3, bad.sum())",
        "text": "next , write a function , goat _ door  , to simulate randomly reveal one of the goat door that a contestant do n't pick  . ",
        "id": 2
    },
    {
        "code": "t = df['timeStamp'].iloc[0]\nt\nt.date()\ndf['Date']=df['timeStamp'].apply(lambda t: t.date())\ndf.head()",
        "text": "create a new column call date  that contain the date from the timestamp column  .  you ll need to use apply along with the  . date ( ) method  .  there be a difference between timestamp and date object   - ",
        "id": 3
    },
    {
        "code": "import csv\nfrom datetime import datetime\nfrom time import time\nff = \"./Data/train.csv\" \nwith open(ff, 'rt') as f:\n    reader = csv.reader(f)\n    train_data = list(reader)\nff = \"./Data/test.csv\" \nwith open(ff, 'rt') as f:\n    reader = csv.reader(f)\n    test_data = list(reader)\ntrain_data_full = pd.DataFrame(train_data[1:], columns = train_data[0])\ntest_data = pd.DataFrame(test_data[1:], columns = test_data[0])\nprint (\"Shape of train data\", train_data_full.shape)\nprint (\"Shape of test data\", test_data.shape)",
        "text": "iii  .  data import import data and check the shape of train and test set  . ",
        "id": 4
    },
    {
        "code": "print(50 * '_')\nprint('% 9s' % 'method' + '                   time' + '      homo')\ndef compare_method(estimator, name, data):\n    t0 = time()\n    estimator.fit(data)\n    print('% 25s   %.2fs     %.3f ' % (name, (time() - t0), metrics.homogeneity_score(y, estimator.labels_)))\ncompare_method(KMeans(init='k-means++', n_clusters=n_digits, n_init=10), name=\"k-means++\", data=X)\ncompare_method(KMeans(init='random', n_clusters=n_digits, n_init=10), name=\"random\", data=X)",
        "text": "other cluster method take a look at the cluster method and option for various method on the [ scikit   -   learn page ] ( <url> )   -  exercise ,   -  by modify the follow code , try to find the cluster method with the large homogeneity score for this dataset  . ",
        "id": 5
    },
    {
        "code": "\ndf.rename(columns={'Base Salary': 'Salary', \n                   'Guaranteed Compensation': 'Total', \n                   'First Name': 'First',\n                   'Last Name': 'Last'}, inplace=True)\ncol_order = ['Salary', 'Player', 'Club', 'POS', 'Year', 'GP', 'GS', 'MINS', \n             'G', 'A', 'SHTS_FP', 'SOG', 'GWG', 'HmG', 'RdG', 'Gp90', 'SCpct', \n             'GWA', 'HmA', 'RdA', 'Ap90', 'SOGpct', 'FC', 'FS', 'OFF', 'YC', \n             'RC', 'PKG_FP', 'PKA_FP', 'SHTS_GK', 'SV', 'GA', 'GAA', 'W', 'L', \n             'T', 'ShO', 'Wpct', 'SvPct', 'PKG_GK', 'PKA_GK']\ndf = df.reindex(columns=col_order)\ndf.sample(10, random_state=129)\ndf.pivot_table(df, index='Year', aggfunc=len).iloc[:, 0]",
        "text": "rename , reorder , and drop column",
        "id": 6
    },
    {
        "code": "num_fruit = 4\nprint('I have {0} fruits'.format(num_fruits))",
        "text": "error what happen when we make a typo and try to write something that doe n't exist ?",
        "id": 7
    },
    {
        "code": "def Probability(o):\n    return o / (o+1)",
        "text": "and this function convert from odds to probability  . ",
        "id": 8
    },
    {
        "code": "plot_importance(best_model, X_train, max_features=16)\nm_oob = best_model.oob_decision_function_",
        "text": "auc and confusion matrix suggest that the gradient boost model be good than random forest  . ",
        "id": 9
    },
    {
        "code": "\nindir_varyB = []\nfor x in np.arange(40,104,4):\n    indir_varyB_matrix = indirectTest(linda,feat_weights, iterations, features, distribution, [x,40,12,120])\n    indir_varyB_dirStd, indir_varyB_dirMean = getStdMean(indir_varyB_matrix)\n    X = [indir_varyB_dirStd, indir_varyB_dirMean]\n    indir_varyB.append(X)\nindir_varyB = np.array(indir_varyB)\nplot_multiple(indir_varyB,\"Average Probability judgements for indirect test over 100 iterations\\n (Varying B proportion while keeping B, CP & BF equal)\",\n             \"Percentage of B size with respect to the number of common people\", np.arange(40,104,4)*100//120)",
        "text": "keep f , bf , and common people the same , vary b",
        "id": 10
    },
    {
        "code": "\nstoplist = set('for a of the and to it'.split())\ntexts = [[word for word in document.lower().split() if word not in stoplist]\n        for document in documents]\ntexts\nfrom collections import defaultdict\nfrequency = defaultdict(int)\nfor text in texts:\n    for token in text:\n        frequency[token] += 1\ntexts = [[token for token in text if frequency[token] > 1]\n        for text in texts]\nfrom pprint import pprint\npprint(texts)\ntexts",
        "text": "tokenize the document , remove common word ( use a toy stoplist ) a well a word that only appear once in the corpus   - ",
        "id": 11
    },
    {
        "code": "plt.figure(figsize=(20,10))\nplt.plot([5,-15], [0,0], 'k--',\n         [0,0], [-5,5], 'k--',\n         -10, 0, 'r+', markersize=30, mew=5)\nplt.xlabel('Real')\nplt.ylabel('Img')\nplt.ylim([-5,5])\nplt.xlim([-15,5])\nplt.title('Roots of characteristic equation')\nplt.show()",
        "text": "the follow plot show the location of the root of characteristic equation in the imagionary plane  .  a can be see the only root ha negative part , which indicate a stable solution  .  this be evident from the time response of the system in the above figure  . ",
        "id": 12
    },
    {
        "code": "\ndf['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\ndf.head()\ntrain, test = df[df['is_train']==True], df[df['is_train']==False]\nprint('Number of observations in the training data:', len(train))\nprint('Number of observations in the test data:',len(test))",
        "text": "create train and test data",
        "id": 13
    },
    {
        "code": "positive_words = table.sort('coefficients [L2=0]', ascending = False)['word'][0:5]\nnegative_words = table.sort('coefficients [L2=0]', ascending = True)['word'][0:5]",
        "text": "use   -  the coefficient train with l2 penalty 0  -  , find the 5 most positive word ( with large positive coefficient )  .  save them to   -  positive _ words  -  similarly , find the 5 most negative word ( with large negative coefficient ) and save them to   -  negative _ words  - ",
        "id": 14
    },
    {
        "code": "d = {'person': 2, 'cat': 4, 'spider':8}\nfor animal in d:\n    legs = d[animal]\n    print('A %s has %d legs' % (animal, legs))\nd = {'person': 2, 'cat': 4, 'spider': 8}\nfor animal, legs in d.items():\n    print('A %s has %d legs' % (animal, legs))",
        "text": "loop iterate over the key in a dictionary it easy",
        "id": 15
    },
    {
        "code": "sal[sal['Year']==2013]['JobTitle'].value_counts()\nsum(sal[sal['Year']==2013]['JobTitle'].value_counts() == 1) \n# pretty tricky way to do this...",
        "text": "how many job title be represent by only one person in 2013 ? ( e . g  .  job title with only one occurence in 2013 ? )",
        "id": 16
    },
    {
        "code": "df['ADDRESS'] = df.BUILDING + \" \" + df.STREET + \" \" + df.BORO\ndef yelp_url_gen(row):\n    \n    \n    title1 = row['DBA'].replace(\" \", \"+\")\n    address1 = row['ADDRESS'].replace(\" \", \"+\")\n    return \"https://www.yelp.com/search?find_desc={title1}&find_loc={address1}\".format(title1=title1, address1=address1)\ndf_clean = df.dropna(axis=0, how='any')\ndf_clean['URL'] = df_clean.apply(yelp_url_gen, axis = 1)",
        "text": "yelp url be formulaic , by title and address  .  generate yelp url for scrap with scrapy  . ",
        "id": 17
    },
    {
        "code": "for col in df_2014.columns.values:\n    df_2014.rename(columns={str(col):str(col).replace('_','')}, inplace=True)\ndf_2014_1=df_2014[list(common_feature)+['issued', 'loanstatus']]\ndf_2014_1.shape\ndf_2014_1.to_csv('loan_2014.csv',index=False)\ndf=pd.read_csv('loan_2014.csv')\ndf.head()",
        "text": "convert column name in the dataframe and save with common feature ,  issue  , and  loanstatus",
        "id": 18
    },
    {
        "code": "\ndf_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header=None)\ndf_wine.columns =['Class label', 'Alcohol',\n                  'Malic acid', 'Ash',\n                  'Alcalinity of ash', 'Magnesium',\n                  'Total phenols', 'Flavanoids',\n                  'Nonflavanoid phenols',\n                  'Proanthocyanins',\n                  'Color intensity', 'Hue',\n                  'OD280/OD315 of diluted wines',\n                  'Proline']\ndf_wine.head()\nfrom sklearn.cross_validation import train_test_split\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)",
        "text": "partition a dataset in train and test set",
        "id": 19
    },
    {
        "code": "print(mask)\nindices = np.where(mask)\nindices\nx[indices] # this indexing is equivalent to the fancy indexing x[mask]",
        "text": "function for extract data from array and create array   -  where  -  the index mask can be convert to position index use the where function",
        "id": 20
    },
    {
        "code": "df['Survived'].sum()\ndf['Survived'].cumsum()\ndf['Name'].count()\ncount_nan = len(df) - df.count()\ncount_nan\ndf['Age'].min()\ndf['Age'].max()\ndf['Fare'].min()\ndf['Fare'].max()\ndf['Age'].max() - df['Age'].min()\ndf['Fare'].max() - df['Fare'].min()\ncounts = df['Age'].value_counts()\ncounts\ncounts = df['Fare'].value_counts()\ncounts",
        "text": "basic statistic sum , cumulative sum , count the number of non   -   na value , count the number of na value , minimum value of  age  and  passenger fare  , minimum value of  age  and  passenger fare  , range of  age  and  passenger fare '' and frequency table of ' age '' and ' passenger fare ''  . ",
        "id": 21
    },
    {
        "code": "filename = 'data/China_Children_Population.xlsx'\ntable = read_excel(filename, header=0, index_col=0, decimal='M')\ntable",
        "text": "the data directory contain a download copy of <url> /",
        "id": 22
    },
    {
        "code": "a1.items()\n\",  \".join( \"%s = %d\" % (name,val) for name,val in a1.items())",
        "text": "items ( )   -  be return a list contain both the list but each element in the dictionary be inside a tuple  .  this be same a the result that wa obtain when zip function wa use   -   except that the order ha be shuffled  by the dictionary  . ",
        "id": 23
    },
    {
        "code": "\ntop_sales = df2.copy()\ntop_sales.groupby('Zip Code')[['Sale (Dollars)', 'Volume Sold (Liters)']].\\\nsum().reset_index().sort_values(by='Sale (Dollars)', ascending=False).head(10)\ndef draw_histograms(data, col):\n    sns.distplot(data[col]);\n    plt.title(col);\n    plt.xlabel(col);\n    plt.ylabel('Frequency');\n    plt.xticks(rotation=45);\n    plt.show();\n    print\ndemo_data.columns.values\ndemo_hist_cols = ['Per Capita Inc', 'Pop Below Poverty Level', '% P16+ in labor force']\nfor i in demo_hist_cols:\n    draw_histograms(demo_data, i)",
        "text": "we can see that there be outlier in all case  . ",
        "id": 24
    },
    {
        "code": "model = create_model()\nadam = keras.optimizers.Adam(lr=0.0001, decay=10e-6)\nmodel.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\nmodel.fit(X_train, y_train, batch_size = 64, epochs = 10)\nscores = model.evaluate(X_test, y_test)\nprint(\"Accuracy: \", scores[1]*100, \"%\")\nmodel.save('conv_adam_lr0.0001_batch64_epoch10.h5')",
        "text": "model 8 batch size , 64 epoch , 10 optimizer , adam learn rate , 0 . 0001 decay , 10e   -   6 loss function , categorical cross   -   entropy model accuracy , 59 . 55 %",
        "id": 25
    },
    {
        "code": "import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()",
        "text": "h2o automl regression demo this be a [ jupyter ] ( <url> / ) notebook  .  when you execute code within the notebook , the result appear beneath the code  .  to execute a code chunk , place your cursor on the cell and press *shift+enter  -  start h2o import the   -  h2o  -  python module and h2oautoml  class and initialize a local h2o cluster  . ",
        "id": 26
    },
    {
        "code": "A = np.asarray(bestScores_4)\nplt.plot(np.arange(1,len(A)+1),1-A)\nplt.xlabel('Number of hidden layer')\nplt.ylabel('Prediction accuracy')\nplt.title('Total number of neurons fixed to 1200')\nplt.show",
        "text": "plot the result ( reproduce figure 2 )",
        "id": 27
    },
    {
        "code": "sorted(dcentralities_plots.items(), key = lambda x: x[1], reverse = True)[:10]\nsorted(dcentralities_plots.items(), key = lambda x: x[1], reverse = True)[-10:]",
        "text": "top and bottom ten word in term of degree ,",
        "id": 28
    },
    {
        "code": "model = 'self_other'\nexperiment_dir = opj('/output', model)\noutput_dir = 'datasink'\nworking_dir = 'workingdir'\nsubject_list = ['sub-222', 'sub-256', 'sub-270']\ntask_list = ['emp']\nfwhm = [4, 8]\nwith open('/data/task-emp_bold.json', 'rt') as fp:\n    task_info = json.load(fp)\nTR = task_info['RepetitionTime']\nprint(TR)\niso_size = 4",
        "text": "experiment parameter it 's always a good idea to specify all parameter that might change between experiment at the begin of your script  .  we will use one functional image for fingerfootlips task for ten subject  . ",
        "id": 29
    },
    {
        "code": "messages = pd.read_sql_query(\"\"\"\nSELECT \n  Score, \n  Summary, \n  HelpfulnessNumerator as VotesHelpful, \n  HelpfulnessDenominator as VotesTotal\nFROM Reviews \nWHERE Score != 3\"\"\", con)",
        "text": "let 's select only what 's of interest to u ,",
        "id": 30
    },
    {
        "code": "cv_params = {'max_depth': [3,5,7], 'min_child_weight': [1,3,5]}\nind_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n             'objective': 'binary:logistic'}\noptimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n                            cv_params, \n                             scoring = 'accuracy', cv = 5, n_jobs = -1) \noptimized_GBM.fit(X_train, Y_train)\noptimized_GBM.grid_scores_",
        "text": "parameter tune tune on the maximum depth of the tree first , the min _ child _ weight , which be very similar to min _ samples _ split in sklearn  s version of gradient boost tree  .  we set the objective to  binary , logistic  since this be a binary classification problem  . ",
        "id": 31
    },
    {
        "code": "eq1 = eq.substitute_function(a, af).simplify_full()\neq1\neq2 = (eq1 * f(x)^3).simplify_full()\neq2",
        "text": "we substitute for $ a ( \\tau , \\chi ) $ the value find when solve the $ \\tau\\chi $ component ,",
        "id": 32
    },
    {
        "code": "P_heart = ...\nP_ace_of_spades_given_first_card_was_heart = ...\nP_first_card_heart_and_second_card_is_ace_of_spades = ...\nP_ace_of_spades_and_heart_any_order = ...\nprint(P_ace_of_spades_and_heart_any_order)",
        "text": "suppose you take out two card from a standard pack of card one after another , without replace the first card  .  what be probability that one of the card be the ace of spade , and the other card be a heart ?",
        "id": 33
    },
    {
        "code": "Xpred = test[['Overall Qual','Total Bsmt SF','Baths','Garage Cars','Garage Area','Year Built','Gr Liv Area','TotRms AbvGrd','Fireplaces',\n       'Open Porch SF','Lot Frontage','Lot Area','Overall Cond']]\nss = StandardScaler()  \nss.fit(X_train)   \nXs_train = ss.transform(X_train)  \nXss_test = ss.transform(Xpred)",
        "text": "part 2 d . 3 , preprocessing and scale for test data",
        "id": 34
    },
    {
        "code": "class Square():\n    def __init__(self, length):\n        self.length = length\n    def area(self):\n        return self.length ** 2\nclass Shape(Square):\n    def areaa(self, length = 0):\n        return length ** 2\na = Square(12)\nprint(a.area())\nb = Shape(13)\nprint(b.area())\nprint(b.areaa())",
        "text": "define a class name shape and it subclass square  .  the square class ha an init function which take a length a argument  .  both class have a area function which can print the area of the shape where shape 's area be 0 by default  . ",
        "id": 35
    },
    {
        "code": "array = np.random.random((10,10))\nmax_value = array.max()\nmin_value = array.min()",
        "text": "create a 10x10 array with random value and find the minimum and maximum value",
        "id": 36
    },
    {
        "code": "cats[cats[\"inches\"]> 12]\ndf[(df['animal'] == 'cat') & (df['inches'] > 12)]\ndogs[dogs[\"inches\"]> 12]\ndf[(df['animal'] == 'dog') & (df['inches'] > 12)]",
        "text": "display all of the animal that be cat and above 12 inch long   -  first do it use the cats  variable , then also do it use your df  dataframe  .  >   -  tip ,   -  for multiple condition , you use df [ ( one condition )   -   ( another condition ) ]",
        "id": 37
    },
    {
        "code": "import xgboost as xgb\ngrid_values = {'max_depth' : [4,5,6], 'learning_rate' : [0.0008, 0.00085, 0.0009], 'n_estimators' : [900, 950, 1000]}\ngrid_clf_acc = GridSearchCV(xgb.XGBClassifier(), param_grid = grid_values, cv=10, scoring = 'accuracy')\ngrid_clf_acc.fit(X_train, y_train)\nprint('Mean score matrix: ', grid_clf_acc.cv_results_['mean_test_score'])\nprint('Grid best parameter (max. accuracy): ', grid_clf_acc.best_params_)\nprint('Grid best score (accuracy): ', grid_clf_acc.best_score_)",
        "text": "xgboost classifier with cross validation ( cv=10 ) and hyperparameter optimization",
        "id": 38
    },
    {
        "code": "from sklearn.cross_validation import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33)\nmodel.fit(X_train, Y_train)\nmodel.score(X_test, Y_test)",
        "text": "validate the knn classifier",
        "id": 39
    },
    {
        "code": "import numpy as np\nx = np.arange(0,11,1)\ny=x**2\nprint(y)",
        "text": "the arange function will make it easy for u to create a set of x value and then evaluate function at those value  .  for example , suppose we want to make a list of x value between 0 and 10 and then calculate f ( x ) =x^2 at those value  .  we could type ,",
        "id": 40
    },
    {
        "code": "df_no_salary_outlier.loc[:,'from_ratio'] = df_no_salary_outlier.from_this_person_to_poi / df_no_salary_outlier.from_messages\nggplot(aes(x='poi', y='from_ratio'), data=df_no_salary_outlier) + geom_point()\nggplot(aes(x='from_ratio'), data=df_no_salary_outlier) + geom_boxplot()",
        "text": "create new variable a ratio of message to poi and total message send  . ",
        "id": 41
    },
    {
        "code": "stds = np.std(split, axis=2)\nprint(stds.shape)\nprint(np.amax(stds))\nmax_idx = np.unravel_index(np.argmax(stds), stds.shape)\nprint(stds[max_idx])",
        "text": "find standard deviation for each router in each location*",
        "id": 42
    },
    {
        "code": "distinct_matches = schools.groupby('match_id')['School_ID'].nunique()\ndistinct_matches.sort_values(ascending=False).head()",
        "text": "check if budget row have be duplicate let 's make sure there be n't multiple school match to a single budget entry",
        "id": 43
    },
    {
        "code": "train = pd.read_csv(os.path.join(data_dir, 'Train', 'train.csv'))\ntest = pd.read_csv(os.path.join(data_dir, 'Test.csv'))\ntrain.head()",
        "text": "read the datasets  .  these be in  . csv format , and have a filename along with the appropriate label",
        "id": 44
    },
    {
        "code": "choices = ['http:', 'ftp:']\nurl = 'http://www.python.org'\nurl.startswith(choices)\nurl.startswith(tuple(choices))",
        "text": "oddly , this be one part of python where a tuple be actually require a input  .  if you happen to have the choice specify in a list or set , just make sure you convert them use tuple ( ) first  .  for example ,",
        "id": 45
    },
    {
        "code": "loss, _ = net.loss(X, y, reg=0.1)\ncorrect_loss = 1.30378789133\nprint('Difference between your loss {} and correct loss {}:'.format(\n        loss, correct_loss))\nprint(np.sum(np.abs(loss - correct_loss)))",
        "text": "forward pas , compute loss in the same function , implement the second part that compute the data and regularization loss  .  for the data loss , use the average cross   -   entropy softmax loss  . ",
        "id": 46
    },
    {
        "code": "from pynq.lib.logictools import Waveform\nup_counter = {'signal': [\n    ['stimulus',\n        {'name': 'bit0', 'pin': 'D0', 'wave': 'lh' * 8},\n        {'name': 'bit1', 'pin': 'D1', 'wave': 'l.h.' * 4},\n        {'name': 'bit2', 'pin': 'D2', 'wave': 'l...h...' * 2},\n        {'name': 'bit3', 'pin': 'D3', 'wave': 'l.......h.......'}],\n        {},\n    ['analysis',\n        {'name': 'bit0_loopback', 'pin': 'D0'},\n        {'name': 'bit1_loopback', 'pin': 'D1'},\n        {'name': 'bit2_loopback', 'pin': 'D2'},\n        {'name': 'bit3_loopback', 'pin': 'D3'}]], \n    'foot': {'tock': 1},\n    'head': {'text': 'up_counter'}}\nwaveform = Waveform(up_counter)\nwaveform.display()",
        "text": "create wavejson waveform the pattern to be generate be specify in the wavejson format the pattern be apply to the arduino interface , pin   -  d0  -  ,   -  d1  -  ,   -  d2  -  and   -  d3  -  be set to generate a 4   -   bite count  .  the waveform class be use to display the specify waveform  . ",
        "id": 47
    },
    {
        "code": "from keras import layers\nmodel = keras.models.Sequential()\nmodel.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\nmodel.add(layers.Dense(len(chars), activation='softmax'))\noptimizer = keras.optimizers.RMSprop(lr=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)",
        "text": "let 's build the network  .  this network be a single lstm layer follow by a dense classifier and softmax over all possible character  .  but note that recurrent neural network aren  t the only way to do sequence data generation , 1d convnets also have prove extremely successful at this task in recent time  . ",
        "id": 48
    },
    {
        "code": "df_no_salary_outlier.loc[:,'to_ratio'] = df_no_salary_outlier.from_poi_to_this_person / df_no_salary_outlier.to_messages\nggplot(aes(x='poi', y='to_ratio'), data=df_no_salary_outlier) + geom_point()\nggplot(aes(x='to_ratio'), data=df_no_salary_outlier) + geom_boxplot()",
        "text": "create new variable a ratio of message from poi to total message receive  . ",
        "id": 49
    },
    {
        "code": "epochs = 10\ndata_size = int(mnist.train_labels.size()[0])\nfor i in range(epochs):\n    for j, (images, _) in enumerate(data_loader):\n        images = images.view(images.size(0), -1) \n        images = Variable(images).type(FloatTensor)\n        autoencoder.zero_grad()\n        reconstructions = autoencoder(images)\n        loss = torch.dist(images, reconstructions) / reconstructions.size(0)\n        loss.backward()\n        optimizer.step()\n        \n    print('Epoch %i/%i loss %.4f' % (i + 1, epochs, loss.data[0]))",
        "text": "finally , we train the autoencoder  . ",
        "id": 50
    },
    {
        "code": "elton = people[people['name'] == 'Elton John']\nelton[['tfidf']].stack('tfidf',new_column_name=['word','tfidf']).sort('tfidf',ascending=False)\nelton[['word_count']].stack('word_count',new_column_name=['word','count']).sort('count',ascending=False)",
        "text": "now , take a particular famous person , elton john   .  what be the 3 word in his article with high word count ? what be the 3 word in his article with high tf   -   idf ? these result illustrate why tf   -   idf be useful for find important word  . ",
        "id": 51
    },
    {
        "code": "rate = 0.00095\nlogits = LeNet4Con(x, keep_prob,image_channles)\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\nloss_operation = tf.reduce_mean(cross_entropy)\noptimizer = tf.train.AdamOptimizer(learning_rate = rate)\ntraining_operation = optimizer.minimize(loss_operation)",
        "text": "train pipeline create a train pipeline that us the model to classify data  . ",
        "id": 52
    },
    {
        "code": "class InferenceConfig(coco.CocoConfig):\n    \n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\nconfig = InferenceConfig()\nconfig.print()",
        "text": "configuration we ll be use a model train on the m   -   coco dataset  .  the configuration of this model be in the  cococonfig   class in  coco . py    .  for inferencing , modify the configuration a bite to fit the task  .  to do so , sub   -   class the  cococonfig   class and override the attribute you need to change  . ",
        "id": 53
    },
    {
        "code": "\nx = np.array([0, 100])\nfor i in range(100):\n    plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i], linewidth=0.5, alpha=0.2, color='firebrick')\nplt.plot(illiteracy, fertility, linestyle='none', marker='.')\nplt.xlabel('illiteracy')\nplt.ylabel('fertility')\nplt.margins(0.02);",
        "text": "a nice way to visualize the variability we might expect in a linear regression be to plot the line you would get from each bootstrap replicate of the slope and intercept  .  we can do this for the first 100 of your bootstrap replicate of the slope and intercept ( store a bs _ slope _ reps and bs _ intercept _ reps )  . ",
        "id": 54
    },
    {
        "code": "Lower_Leftys_Loughlin = rootlocal + 'E5382-MonitoringData/below lefty spring_150103090632_S4811.CSV'\nLower_Leftys_John_Files = rootlocal + 'E5382-MonitoringData/Information Received from Protestants/LOWER LEFTYS SPRING NOV26 2015-01-01 10.47.45.wsl.csv'\nLower_Leftys_Manual = rootlocal + 'E5382-MonitoringData/Information Received from UGS/Manual_Readings_Compilation.xlsx'\nLower_Leftys_John_Files2 = rootlocal + 'E5382-MonitoringData/Information Received from Protestants/LOWERLEFTYS2015_Append_2015-04-03_15-05-57-229.csv'\nLower_Leftys_John_Files3 = rootlocal + 'E5382-MonitoringData/Information Received from Protestants/LOWERLEFTYS2015_Append_2015-10-04_.csv'",
        "text": "path to low lefty weir data .    [ loughlin data import ] (   -   low ) , [ loughlin bp import ] (   -   leftysbp1 ) , [ file data import ] (   -   fileslower ) , [ manual read ] (   -   lowerman )",
        "id": 55
    },
    {
        "code": "poly = PolynomialFeatures(degree=2)\nx_new = poly.fit_transform(x)\nx_old = x\nprint(\"Original Data Shape = \", x.shape)\nprint(\"Polynomial Degree 2 Data Shape = \", x_new.shape)",
        "text": "create polynomial regression of degree 2",
        "id": 56
    },
    {
        "code": "plt.scatter(X_test,y_test,color = 'red')\nplt.plot(X_train,regressor.predict(X_train),color = 'blue') \nplt.title('salary vs Experience(test set)')\nplt.xlabel(testdata.columns[0])\nplt.ylabel(testdata.columns[1])\nplt.show()",
        "text": "visualise the test set result",
        "id": 57
    },
    {
        "code": "def X_y_split(df):\n    X = df.drop(['END_LAT', 'END_LONG'], axis = 1).reset_index(drop=True)\n    y = df.loc[:,['END_LAT', 'END_LONG']].reset_index(drop=True)\n    return X, y\nX_train, y_train = X_y_split(train)\nX_val, y_val = X_y_split(val)",
        "text": "split our train and val in to x ( predictor variable ) and y ( outcome variable ) respectively",
        "id": 58
    },
    {
        "code": "subs2_clusters = subs2[np.ix_(subs2_df.index, subs2_df.index)]\nsubs2_clusters_labels = [str(x) for x in subs2_df.hashval]\n_ = sourmash_lib.fig.plot_composite_matrix(subs2_clusters, subs2_clusters_labels)",
        "text": "example , select only the cluster out of the original matrix",
        "id": 59
    },
    {
        "code": "sns.set_style('whitegrid')\nad_data['Age'].hist(bins = 30, color = 'purple')\nplt.xlabel('Age')",
        "text": "exploratory data analysis now , use seaborn to explore the data  .  try recreate the plot show below   -  create a histogram of the age  - ",
        "id": 60
    },
    {
        "code": "import pickle\nwith open(\"../data/randomforest_params_syphilis.pickle\", \"wb\") as myfile:\n    pickle.dump(clf, myfile)\nwith open(\"../data/Ymean_syphilis.pickle\", \"wb\") as myfile:\n    pickle.dump(Ymean, myfile)\nwith open(\"../data/Ystd_syphilis.pickle\", \"wb\") as myfile:\n    pickle.dump(Ystd, myfile)\ndeployed_model = pickle.load(open('../data/randomforest_params_syphilis.pickle', \"rb\" ))\nprint('Variance score: %.5f\\t(%.5f)' % (deployed_model.score(X_test, Y_test), deployed_model.score(X_full, Y_full)))",
        "text": "save model parameter for use in web app ,",
        "id": 61
    },
    {
        "code": "min_rating = None\nraise NotImplementedError()\nassert isinstance(min_rating, np.float64), \"Try again, make sure you are taking the min of just 1 column\"\nassert abs(min_rating - 0.5) < .01, \"Try again, the minimum should be 0.5\"",
        "text": "exercise 1 , find the minimum rat first of all let 's start by compute the minimum rat  .  in the next cell , define the min _ rating  variable to be the minimum rat across all of the dataframe ,",
        "id": 62
    },
    {
        "code": "def FormatThousands(x):\n    return float(x)/1000.0\ncity_table['income_in_k'] =city_table['city_avg_incomes'].apply(FormatThousands)",
        "text": "let u do something similar for the income , but format the number in term of thousand  . ",
        "id": 63
    },
    {
        "code": "sat['Verbal'].median()\nsat_median_Verbal =sat[sat['Verbal'] > sat['Verbal'].median()]\nsat_median_Verbal",
        "text": "find the list of state that have a median verbal  score great than the median of verbal  score across the entire dataset how doe this compare to the list of state great than the mean of verbal  score ? why ?",
        "id": 64
    },
    {
        "code": "df['preTestScore'].groupby([df['regiment'], df['company']]).mean().unstack()",
        "text": "mean pretestscores group by regiment and company without heirarchical index",
        "id": 65
    },
    {
        "code": "\nts_log_moving_avg_diff = ts_log - moving_avg\nts_log_moving_avg_diff.head(13)",
        "text": "the red line show the roll mean  .  let subtract this from the original series  .  note that since we be take average of last 12 value , roll mean be not define for first 11 value  .  this can be observe a ,",
        "id": 66
    },
    {
        "code": "from bcycle_lib.all_utils import load_bcycle_data\nprint('Loading stations and trips....', end='')\nstations_df, trips_df = load_bcycle_data('../input', 'all_stations_clean.csv', 'all_trips_clean.csv', verbose=False)\nprint('done!')\nprint('Bike trips loaded from {} to {}'.format(trips_df.index[0], trips_df.index[-1]))\nstations_df.info()",
        "text": "load station and trip data the notebooks/bcycle _ all _ data _ eda  notebook clean up the raw csv file from bcycle , and split it into a station and trip dataframe  .  because of this , the clean csv file read in below should n't need too much process  . ",
        "id": 67
    },
    {
        "code": "a = input(\"enter a\")\nb = input(\"enter b\")\nif (a > b):\n    print(a)\nelse:\n    print(b)",
        "text": "write a program to print large of two number  .  take two number from user use input ( ) function  - ",
        "id": 68
    },
    {
        "code": "def isPrime(n):\n    if n < 2: return \"Neither prime, nor composite\"\n    for i in range(2, int(n**0.5) + 1):\n        if n % i == 0:\n            return False\n    return True\nsum = 0\nfor i in range(2, 2000000):\n    if isPrime(i):\n        sum += i\nprint (sum)",
        "text": "summation of prime the sum of the prime below 10 be 2 + 3 + 5 + 7 = 17 .  find the sum of all the prime below two million  . ",
        "id": 69
    },
    {
        "code": "prop_df = pd.DataFrame((survey_df['FUTURE.CONTRIBUTION.INTEREST'].value_counts()))\nprop_df.columns=[\"count\"]\nprop_df\nprop_df = pd.DataFrame((survey_df['FUTURE.CONTRIBUTION.INTEREST'].value_counts(normalize=True).round(4).round(4)*100))\nprop_df.columns=[\"percent\"]\nprop_df\nax = pd.DataFrame(survey_df['FUTURE.CONTRIBUTION.INTEREST'].value_counts()).plot(kind='barh')\nplt.title(\"How interested are you in contributing\\nto open source projects in the future?\")\nt = ax.set_xlabel(\"Count of responses\")",
        "text": "how interest be you in contribute to open source project in the future ? future . contribution . interest",
        "id": 70
    },
    {
        "code": "global_var = 'i am global'\nfrom scope_utils import print_formatted_variable, print_error_msg\ndef show_global_variables_changes(function_to_call, name='Global Variable'):\n    global global_var\n    print_formatted_variable(global_var, name + ' value before change')    \n    function_to_call()\n    print_formatted_variable(global_var, name + ' value after change OUTSIDE function')\ndef function_trying_to_change_a_global():\n    try:\n        global_var += 'who are you?'\n    except Exception as e:\n        print_error_msg('ERROR You cannot change the global variable: {}'.format(e))\nshow_global_variables_changes(function_trying_to_change_a_global)\ndef function_trying_to_change_a_global_2():\n    \n    global_var = 'who are you?'\n    \n    print_formatted_variable(global_var, 'Global Variable value after change INSIDE function')\n\nshow_global_variables_changes(function_trying_to_change_a_global_2)",
        "text": "change global variable value inside other function globals be like closure where you can not change the value inside a function a that would make the variable local  .  to be able to change the global variable name you have to say that it use   -  global  - ",
        "id": 71
    },
    {
        "code": "def f05_score_hard(preds, labels):\n    tp = np.sum((labels==preds) & (labels==1))\n    tn = np.sum((labels==preds) & (labels==0))\n    fp = np.sum(preds==1)-tp\n    fn = np.sum(preds==0)-tn\n    p = tp*1.0/(tp+fp)\n    r = tp*1.0/(tp+fn)\n    score = 1.25*p*r/(0.25*p+r)\n    return score\nnewtrain.columns",
        "text": "f0 . 5 score calculator   -   hard prediction",
        "id": 72
    },
    {
        "code": "\ncrimes_wea.to_csv('data/crime/crimes_weather.csv')\ncrime_wea=pd.read_csv('data/crime/crimes_weather.csv')\ncrime_wea.loc[:, ['Arrest', 'Domestic']]=crime_wea.loc[:, ['Arrest', 'Domestic']].astype('bool')\ncrime_wea.info(null_counts=True)\ncrime_wea['Date']=pd.to_datetime(crime_wea['Date'])\ncrime_wea.index=crime_wea['Date']\ncrime_wea.head(10)",
        "text": "we can then save the new data frame a a csv file for future analysis  .  in the follow section , i will use other data set and this new data with weather   information to do some exploratory analysis",
        "id": 73
    },
    {
        "code": "from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nscores = cross_validation.cross_val_score(clf, X, y, cv=5)\nprint(scores)\nprint(scores.mean())",
        "text": "decision tree with cross   -   validation",
        "id": 74
    },
    {
        "code": "draw_four(viewer3)\nplt.savefig('chap08-4.pdf')",
        "text": "here 's what they look like ,",
        "id": 75
    },
    {
        "code": "cars.sum()\ncars.sum(axis=1)\ncars.median()\ncars.mean()\ncars.max()\nmpg = cars.mpg\nmpg.idxmax()",
        "text": "look at summary statistic that decribe a variable 's numeric value",
        "id": 76
    },
    {
        "code": "\npd.DataFrame(np.arange(1, 6))\ndf = pd.DataFrame(np.array([[10, 11], [20, 21]]))\ndf\ndf.columns\ndf = pd.DataFrame(np.array([[70, 71], [90, 91]]),\n                  columns=['Missoula', 'Philadelphia'])\ndf\nlen(df)\ndf.shape",
        "text": "create a dataframe use numpy function result",
        "id": 77
    },
    {
        "code": "print(airport_count.keys())\nprint(airport_count.values())\nCounter = 0\nmax_country = {}\nfor key, value in airport_count.items():\n    if value > Counter:\n        Counter = value\n        max_country['country'] = key\n        max_country['count'] = value\nprint(max_country)\nmax(airport_count.values())",
        "text": "find the country with the most entry",
        "id": 78
    },
    {
        "code": "\ndf_train = pd.get_dummies(df_train)\ndf_train;\ndf_train.values",
        "text": "last but not the least , dummy variable   - ",
        "id": 79
    },
    {
        "code": "def create_lstm_cell(lstm_cell_units, keep_probability, lstm_layers, batch_size, X):\n    cell = tf.nn.rnn_cell.LSTMCell(lstm_cell_units, state_is_tuple=True)\n    cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=keep_probability)\n    cell = tf.nn.rnn_cell.MultiRNNCell([cell] * lstm_layers, state_is_tuple=True)\n    cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_probability)\n    initial_state = cell.zero_state(batch_size, tf.float32)\n    cell_inputs = tf.expand_dims(X, 2)\n    return cell, cell_inputs, initial_state",
        "text": "model preparation        -  create lstm cell this include dropout and multiple layer  . ",
        "id": 80
    },
    {
        "code": "election88['vote_bush'] = np.around((election88.electionresult*election88.vote_total).values)",
        "text": "in the vote data , compute the number of people who do and do n't vote for bush by state  . ",
        "id": 81
    },
    {
        "code": "title_mapping = {'Mr':1, 'Miss':2, 'Mrs':3, 'Master':4, 'Rare':5}\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_data.head()",
        "text": "we can convert the categorical title to ordinal  . ",
        "id": 82
    },
    {
        "code": "def X_y_split(df):\n    X = df.drop(['DECODED'], axis = 1).reset_index(drop=True)\n    y = df.loc[:,['DECODED']].reset_index(drop=True)\n    return X, y\nX_train, y_train = X_y_split(train)\nX_val, y_val = X_y_split(val)",
        "text": "split our train and val in to x ( predictor variable ) and y ( outcome variable )",
        "id": 83
    },
    {
        "code": "from layers import softmax_loss\nloss, _ = softmax_loss(scores, y_train, W)\nprint(\"loss = %.2f\" % loss)",
        "text": "softmax loss function , vectorized implementation",
        "id": 84
    },
    {
        "code": "capacity_curves_file = \"../../../../../rmtk_data/capacity_curves_point.csv\"\nsdof_hysteresis = \"Default\"\ncapacity_curves = utils.read_capacity_curves(capacity_curves_file)\ncapacity_curves = utils.check_SDOF_curves(capacity_curves)\nutils.plot_capacity_curves(capacity_curves)\nhysteresis = read_parameters(sdof_hysteresis)",
        "text": "load capacity curve in order to use this methodology , it be necessary to provide one ( or a group ) of capacity curve , define accord to the format describe in the [ rmtk manual ] (   -  rmtk   -   docs . pdf )  .  please provide the location of the file contain the capacity curve use the parameter capacity _ curves _ file   . ",
        "id": 85
    },
    {
        "code": "from keras.preprocessing.text import Tokenizer\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\ntokenizer = Tokenizer(num_words=1000)\ntokenizer.fit_on_texts(samples)\nsequences = tokenizer.texts_to_sequences(samples)\none_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\none_hot_results.shape",
        "text": "use kera for word   -   level one   -   hot encode",
        "id": 86
    },
    {
        "code": "\ndef view_samples(epoch, samples):\n    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n    for ax, img in zip(axes.flatten(), samples[epoch]):\n        img = img.detach()\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\nwith open('train_samples.pkl', 'rb') as f:\n    samples = pkl.load(f)",
        "text": "generator sample from train here we can view sample of image from the generator  .  first we ll look at the image we save during train  . ",
        "id": 87
    },
    {
        "code": "sports = {'Archery': 'Bhutan',\n          'Golf': 'Scotland',\n          'Sumo': 'Japan',\n          'Taekwondo': 'South Korea'}\ns = pd.Series(sports, index=['Golf', 'Sumo', 'Hockey'])\ns",
        "text": "when list of value in the index object be not align with the key in your dictionary for create the series , it will ignore it from your dictionary , all key , which be not in you index and panda will add none or nan value for any index value you provide which be not in your dictionary key list  . ",
        "id": 88
    },
    {
        "code": "from sklearn.ensemble import RandomForestRegressor\nforest = RandomForestRegressor()\nforest = forest.fit(X_train, y_train)\ny_test_hat = forest.predict(X_test)\nCheckAccuracy(y_test, y_test_hat)",
        "text": "compare to a random forest of decision tree",
        "id": 89
    },
    {
        "code": "train_file = 'training_corpus.txt'\ntest_file = 'test_corpus.txt'\ntoken_len = trainCorpus(train_file,test_file,bi_dict,tri_dict,quad_dict,vocab_dict,prob_dict)\ntrain_file = 'corpusfile.txt'\ntoken_len = loadCorpus(train_file, bi_dict, tri_dict, quad_dict, vocab_dict)\ncomputeKnesserNeyProb(vocab_dict, bi_dict, tri_dict, quad_dict, prob_dict )\nsortProbWordDict(prob_dict)\ninput_sen = takeInput()\nstart_time2 = time.time()\nprediction = doPrediction(input_sen,prob_dict)\nprint('Word Prediction:',prediction)\nprint(\"---Time for Prediction Operation: %s seconds ---\" % (time.time() - start_time2))\nwriteProbDicts(prob_dict)",
        "text": "for test the language model calculate % accuracy and perplexity   note , if this be run then no need to run the cell follow it",
        "id": 90
    },
    {
        "code": "d = {'person': 2, 'cat': 4, 'spider': 8}\nfor animal in d:\n    legs = d[animal]\n    print('A %s has %d legs' % (animal, legs)) \nfor animal, legs in d.items():\n    print('A %s has %d legs' % (animal, legs)) # Prints \"A person has 2 legs\", \"A cat has 4 legs\", \"A spider has 8 legs\"",
        "text": "loop , it be easy to iterate over the key in a dictionary ,",
        "id": 91
    },
    {
        "code": "pred_summary = X.copy()\npred_summary[\"cluster\"] = cluster_pred\npred_summary.head()",
        "text": "assign each observation to a cluster",
        "id": 92
    },
    {
        "code": "import gym\nmake_env = lambda: gym.make(\"LunarLander-v2\")\nenv=make_env()\nenv.reset()\nstate_shape = env.observation_space.shape\nn_actions = env.action_space.n\nplt.imshow(env.render(\"rgb_array\"))\ndel env",
        "text": "experiment setup * here we simply load the game and check that it work",
        "id": 93
    },
    {
        "code": "\nplt.figure(figsize=(10,5))\nplt.ylim(df_hw.Weight.min()-5, df_hw.Weight.max()+5)\nplt.xlim(df_hw.Height.min()-5, df_hw.Height.max()+5)\nsns.regplot(df_f.Height, df_f.Weight, scatter_kws={'alpha':0.3,'s':500})\nsns.regplot(df_m.Height, df_m.Weight,scatter_kws={'alpha':0.3,'s':500})",
        "text": "plot height and weight density distribution make a kernal density estimate plot  .  [ more about kde  .  ] (  <url>  ) note , your data should be normally distribute  .  if your data doe not look normal , transform it with a function like np . log ( )   . ",
        "id": 94
    },
    {
        "code": "def delayedLines(xmlroot):\n    SC_lines = Return_SC_Lines(xmlroot)\n    l=[]\n    for i in SC_lines:\n        if i[1]!='GOOD SERVICE':\n            l.append(i[0])\n    return l\ndelayed=delayedLines(root)\ndelayed",
        "text": "function to return list of line with delay or plan work",
        "id": 95
    },
    {
        "code": "from sklearn.metrics import r2_score\nprint(r2_score(Y_pred_rf,Y_test))\nprint(np.sqrt(mean_squared_error(Y_pred_rf, Y_test)))\nplt.plot(Y_pred_rf,Y_test,\"o\",markersize=2)\nplt.plot(range(0,int(max(Y_pred_rf))),range(0,int(max(Y_pred_rf))),'k--')\nplt.show()\nplt.figure(figsize=(15,12))\nplt.plot(Y_pred_rf)\nplt.plot(Y_test)",
        "text": "score model calculate r^2 , rmse and produce residual plot and forecast vs .  actual",
        "id": 96
    },
    {
        "code": "def batch_generator(X, y, batch_size=128, shuffle=True):\n    if shuffle:\n        indices = np.arange(len(X))\n        np.random.shuffle(indices)\n    for start_idx in range(0, len(X)-batch_size+1, batch_size):\n        if shuffle:\n            excerpt = indices[start_idx:start_idx+batch_size]\n        else:\n            excerpt = slice(start_idx, start_idx+batch_size)\n        yield X[excerpt].astype(np.float32), y[excerpt].astype(np.float32)",
        "text": "train the neural network we want to train our cnn model with mini   -   batch stochastic gradient descent  .  so , we need a way to generate mini   -   batch of data  .  an effective strategy for train be to shuffle the dataset between each epoch  .  so , let 's define a function to generate shuffle mini   -   batch  . ",
        "id": 97
    },
    {
        "code": "t1 = time.time()\ntrain_data['comment_text'] = train_data['comment_text'].apply(clean_text)\nprint(\"Finished cleaning the train set.\", \"Time needed:\", time.time()-t1,\"sec\")\nt2 = time.time()\ntest_data['comment_text'] = test_data['comment_text'].apply(clean_text)\nprint(\"Finished cleaning the test set.\", \"Time needed:\", time.time()-t2,\"sec\")",
        "text": "clean train and test data  - ",
        "id": 98
    },
    {
        "code": "b = np.array([(1,2,3), (4,5,6)])\nb",
        "text": "two   -   dimensional array array transform sequence of sequence into two   -   dimensional array , sequence of sequence of sequence into three   -   dimensional array , and so on  . ",
        "id": 99
    },
    {
        "code": "\nimport heapq as Q\ndef mergeKLists(lists):\n    q = []\n    for i in xrange(len(lists)):\n        if lists[i] is not None:\n            Q.heappush(q, (lists[i].val, lists[i]))\n    dummy = ListNode(0)\n    cur = dummy\n    while q:\n        val, node = Q.heappop(q)\n        cur.next = node\n        cur = cur.next\n        if node.next:\n            Q.heappush(q, (node.next.val, node.next))\n    return dummy.next",
        "text": "no . 76 merge k sort list merge k sort link list and return it a one sort list  .  analyze and describe it complexity  .  example give list ,  python [ 2   -   > 4   -   > null , null ,   -   1   -   > null ] , return   -   1   -   > 2   -   > 4   -   > null . ",
        "id": 100
    },
    {
        "code": "from sklearn.cross_validation import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(xmat, ymat, test_size=0.33)\nknn.fit(X_train, np.ravel(Y_train))\nknn.score(X_test, np.ravel(Y_test))",
        "text": "validate the knn classifier",
        "id": 101
    },
    {
        "code": "from sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\nimport time\nt0 = time.time()\nrnd_clf.fit(X_train, y_train)\nt1 = time.time()\nprint(\"Training took {:.2f}s\".format(t1 - t0))\nfrom sklearn.metrics import accuracy_score\ny_pred = rnd_clf.predict(X_test)\naccuracy_score(y_test, y_pred)",
        "text": "exercise , train a random forest classifier on the dataset and time how long it take , then evaluate the result model on the test set   - ",
        "id": 102
    },
    {
        "code": "labels = ['a', 'b', 'c']\nmy_list = [10, 20, 30]\narr = np.array([100, 200, 300])\nd = {'a': 1, 'b': 2, 'c': 3}",
        "text": "create a series you can convert a list , numpy array , or dictionary to a series ,",
        "id": 103
    },
    {
        "code": "import pandas as pd\nufo = pd.read_csv('http://bit.ly/uforeports')\nufo.head()\nufo.columns\nufo.rename(columns={'Colors Reported': 'Color_Reported', 'Shape Reported': 'Shape_Repored'}, inplace=True)\nufo.columns\nufo_cols = ['city', 'colors reported', 'shape reported', 'state', 'time']\nufo.columns = ufo_cols\nufo.head()\nufo = pd.read_csv('http://bit.ly/uforeports', names=ufo_cols, header=0)\nufo.head()\nufo.columns = ufo.columns.str.replace(' ', '_')\nufo.columns",
        "text": "how do i rename column in a panda dataframe ?",
        "id": 104
    },
    {
        "code": "np.random.seed(10)\nn_samples = 30\ndef true_fun(X):\n    return np.cos(1.5 * np.pi * X)\nX = np.sort(np.random.rand(n_samples))\nnoise_size = 0.1\ny = true_fun(X) + np.random.randn(n_samples) * noise_size\nnp.random.rand(n_samples)\nX.shape\nplt.scatter(X, y)",
        "text": "let 's make a random dataset where x be uniformly distribute between 0 and 1 , and y be a cosine function plus noise ,",
        "id": 105
    },
    {
        "code": "l = [1,2,3,4,5]\nl1 = [\"a\", \"b\", \"c\", \"d\"]\nl2 = [\"d\", 4, 3, 1.2, [1,2]]\nprint(l)\nprint(l1)\nprint(l2)",
        "text": "list this be use to put a set of item in the list  .  let 's instantiate a list  . ",
        "id": 106
    },
    {
        "code": "n_different_users(spoilers.choose_color_assignment, n=10000).groupby('color').count()",
        "text": "the proportion of user _ ids that see red and blue be roughly 50   -   50",
        "id": 107
    },
    {
        "code": "\norders = pd.read_table('http://bit.ly/chiporders')\norders.head()\norders.item_name.str.upper().head()\norders.item_name.str.contains('Chicken').head()\norders[orders.item_name.str.contains('Chicken')].head()\norders.choice_description.str.replace('[', '').str.replace(']', '').head()",
        "text": "how to use string method in panda ?",
        "id": 108
    },
    {
        "code": "m = np.random.rand(3,3)\nm\nm.max()\nm.max(axis=0)\nm.max(axis=1)",
        "text": "specify the axis on high   -   dimensional data when function such a min , max , etc  .  be apply to a multidimensional array , it be sometimes useful to apply the calculation to the entire array , and sometimes only on a row or column basis  .  use the axis argument we can specify how these function should behave ,",
        "id": 109
    },
    {
        "code": "\nimport pandas as pd\ns = pd.Series([3,5,5,9,6])\ns\ns.head()\nufo = pd.read_table('http://bit.ly/uforeports', sep = ',')\nufo.head()\nufo['State']\nufo['Location'] = ufo.City + ',' + ufo.State\nufo.head()\nufo.shape\nufo.dtypes\nufo.columns",
        "text": "top 5 machine learn library   -   udemy",
        "id": 110
    },
    {
        "code": "import cv2\nimport numpy as np\nimage = cv2.imread('images/elephant.jpg')\nblur = cv2.blur(image, (3,3))\ncv2.imshow('Averaging', blur)\ncv2.waitKey(0)\nGaussian = cv2.GaussianBlur(image, (7,7), 0)\ncv2.imshow('Gaussian Blurring', Gaussian)\ncv2.waitKey(0)\nmedian = cv2.medianBlur(image, 5)\ncv2.imshow('Median Blurring', median)\ncv2.waitKey(0)\nbilateral = cv2.bilateralFilter(image, 9, 75, 75)\ncv2.imshow('Bilateral Blurring', bilateral)\ncv2.waitKey(0)\ncv2.destroyAllWindows()",
        "text": "other commonly use blur method in opencv",
        "id": 111
    },
    {
        "code": "QS = QS_ranking.groupby(['country_QS']).agg({'ratio_international_QS': [np.mean, np.std]}).reset_index()\nQS = QS.sort_values(by = ('ratio_international_QS', 'mean'),ascending= False).head(20)\n_, ax = plt.subplots(figsize=(15, 7))\nsns.barplot(ax=ax, y=\"country_QS\",x=QS.ratio_international_QS['mean'],xerr = QS.ratio_international_QS['std'], data=QS, orient=\"h\")",
        "text": "best country in term of ratio of international student to total student we group the university by country and aggregate the mean ratio and standard deviation of international student to the total number of student and plot the result number in descend order  .  the display black error bar indicate the standard deviation of the ratio for all university in a give country  . ",
        "id": 112
    },
    {
        "code": "m_orig = cm.load_movie_chain(fname[:1])\ndownsample_ratio = 0.2\noffset_mov = -np.min(m_orig[:100])  \nm_orig.resize(1, 1, downsample_ratio).play(\ngain=10, offset=offset_mov, fr=30, magnification=2)",
        "text": "play the movie ( optional )  .  this will require load the movie in memory which in general be not need by the pipeline  .  display the movie us the opencv library  .  press  q  to close the video panel  . ",
        "id": 113
    },
    {
        "code": "myarray[1:-1]",
        "text": "to get all element except the first and the last ,",
        "id": 114
    },
    {
        "code": "data[[\"Sales\"]].resample(\"D\").mean().rolling(window = 15).mean().plot()\nplt.show()",
        "text": "plot the 15 day roll mean of customer in the store",
        "id": 115
    },
    {
        "code": "def f05_score_soft(preds, labels):\n    tp = np.sum((labels==(preds>0.5)) & (labels==1))\n    tn = np.sum((labels==(preds<0.5)) & (labels==0))\n    fp = np.sum((preds>0.5))-tp\n    fn = np.sum(preds<0.5)-tn\n    p = tp/(tp+fp)\n    r = tp/(tp+fn)\n    score = 1.25*p*r/(0.25*p+r)\n    return score",
        "text": "f0 . 5 score calculator   -   soft prediction",
        "id": 116
    },
    {
        "code": "Srec_on_rec = pd.read_pickle('461211_Srec.pkl')\nfig,ax = plt.subplots()\nSrec_on_rec.plot(x='Time',y='DTD', ax=ax, legend=None)\nax.set_title('DTD of receiver synctag')",
        "text": "dtd and spline of receiver synctag",
        "id": 117
    },
    {
        "code": "\nnp.random.seed(123456)\ndf = pd.DataFrame(np.random.randn(8, 3), \n                  index=pd.date_range('1/1/2000', periods=8),\n                  columns=['A', 'B', 'C'])\nstore = pd.HDFStore('data/store.h5')\nstore['df'] = df \nstore\nstore = pd.HDFStore(\"data/store.h5\")\ndf = store['df']\ndf[:5]\ndf.iloc[0].A = 1 \nstore['df'] = df\npd.HDFStore(\"data/store.h5\")['df'][:5] # it's now in there",
        "text": "read and write hdf5 format file",
        "id": 118
    },
    {
        "code": "conditional_loss = vlb_binomial(\n    x, cond_x_decoded_mean, cond_t_mean, cond_t_log_var\n)\ncvae = Model(\n    [x, label], \n    cond_x_decoded_mean\n)\ncvae.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss=lambda x, y: conditional_loss)",
        "text": "define the loss and the model",
        "id": 119
    },
    {
        "code": "lst = [1,2,3]\nlst[0] = 2\nt = tuple([1,2,3])",
        "text": "list be mutable unlike string , list be mutable  .  when the bracket operator appear on the leave side of an assignment , it identify the element of the list that will be assign  . ",
        "id": 120
    },
    {
        "code": "l_w_len = []  \nfor w in words:  \n    l_w_len.append(len(w)) \nl_w_len.sort()  \nmed_of_len = l_w_len[len(l_w_len)//2] \nprint('Median of all word length is : {0}'.format(med_of_len))\nmean_of_len = sum(l_w_len)/len(l_w_len) \nprint('Mean of all word length is : {0}'.format(mean_of_len))\nmod_of_len = max(set(l_w_len), key=l_w_len.count)\nprint('Mode of all word length is : {0}'.format(mod_of_len))",
        "text": "find out what be the average , median and mode for word length ( character length ) ( 10 point )  . ",
        "id": 121
    },
    {
        "code": "par(mfrow=c(1,2))\nbarplot(margin.table(bed_vs_bath,1))\nbarplot(margin.table(bed_vs_bath,2))\nlibrary(pastecs)\nstat.desc(housing_prices)",
        "text": "let plot barplots use the table command  .  essentially bar plot work like a table command nternally  . ",
        "id": 122
    },
    {
        "code": "s0 = State( asia, name=\"asia\" )\ns1 = State( tuberculosis, name=\"tuberculosis\" )\ns2 = State( smoking, name=\"smoker\" )\ns3 = State( lung, name=\"cancer\" )\ns4 = State( bronchitis, name=\"bronchitis\" )\ns5 = State( tuberculosis_or_cancer, name=\"TvC\" )\ns6 = State( xray, name=\"xray\" )\ns7 = State( dyspnea, name='dyspnea' )",
        "text": "now let 's create the state for our bayesian network  . ",
        "id": 123
    },
    {
        "code": "seed = 15\nnp.random.seed(seed)\nLda = gensim.models.ldamodel.LdaModel\nntopics = 10\nldamodel = Lda(corpus, num_topics=ntopics, id2word = dictionary, passes=50)",
        "text": "compute the model set the seed , indicate the number of topic and run the model",
        "id": 124
    },
    {
        "code": "reset_graph()\nn_inputs = 28 * 28  \nn_hidden1 = 300\nn_outputs = 10\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\ny = tf.placeholder(tf.int64, shape=(None), name=\"y\")\nwith tf.name_scope(\"dnn\"):\n    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")",
        "text": "avoid overfitting through regularization    -  \\ell _ 1 $ and $ \\ell _ 2 $ regularization let 's implement $ \\ell _ 1 $ regularization manually  .  first , we create the model , a usual ( with just one hide layer this time , for simplicity ) ,",
        "id": 125
    },
    {
        "code": "\ndef list_of_periods(filename): \n    TimePeriods = []\n    periods_dog = open(filename, \"r\", encoding = \"ISO-8859-15\") \n    list_dog = periods_dog.readlines()[2:]\n    for line in list_dog:\n        name = line.split(\",\")[5]      \n        early = line.split(\",\")[12]    \n        print(str(name) + \", \" + str(early))\n    return TimePeriods\nlist_of_periods(\"my-final-formatted-dog.csv\")",
        "text": "make a list of time period ( early interval ) where canis specie be find  . ",
        "id": 126
    },
    {
        "code": "crime.head(1)\ndistricts = np.unique(crime['PdDistrict'])\nprint(districts)\ncrime_per_district = [crime[crime['PdDistrict'] == dis].shape[0] for dis in districts]\nnew_df = pd.DataFrame({'districts':districts, 'Crimes':crime_per_district})\nprint(new_df)",
        "text": "make a dataframe that include the district and crime count per district  .  which district ha the most crime ?   -  hint , you can use the  . sort _ values ( )  function to sort your dataframe by column   - ",
        "id": 127
    },
    {
        "code": "def update(kernel_size, min_thresh, max_thresh):\n    exampleImg_sobelMag = mag_thresh(exampleImg_unwarp, kernel_size, (min_thresh, max_thresh))\n    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n    f.subplots_adjust(hspace = .2, wspace=.05)\n    ax1.imshow(exampleImg_unwarp)\n    ax1.set_title('Unwarped Image', fontsize=30)\n    ax2.imshow(exampleImg_sobelMag, cmap='gray')\n    ax2.set_title('Sobel Magnitude', fontsize=30)\ninteract(update, kernel_size=(1,31,2), \n                 min_thresh=(0,255), \n                 max_thresh=(0,255))\nprint('...')",
        "text": "visualize sobel magnitude threshold on example image",
        "id": 128
    },
    {
        "code": "a = {'x': 1, 'z': 3}\nb = {'y': 2, 'z': 4}\nmerged = dict(b)\nmerged.update(a)\nmerged['x']\nmerged['y']\nmerged['z']",
        "text": "a an alternative to chainmap , you might consider merge dictionary together use the update ( ) method  .  for example ,",
        "id": 129
    },
    {
        "code": "os.chdir('C:/Users/Bangda/Desktop/pandas/practice')\nusers = pd.read_table('u.user.txt', sep = '|', index_col = 'user_id')\nusers.head()",
        "text": "import data and assign it to a variable call user and use the user _ id  a index",
        "id": 130
    },
    {
        "code": "from sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nX_dtm = vect.fit_transform(X)\ntrain_dtm = vect.fit_transform(X_train.text)\ntest_dtm = vect.transform(X_test.text)\n#y = yelp_best_worst.stars",
        "text": "task 4 , use countvectorizer to create document   -   term matrix from x _ train and x _ test",
        "id": 131
    },
    {
        "code": "\nfor season_num, season in enumerate(seasons):\n    fig = plt.figure(figsize=(12,11))\n    ax = fig.add_subplot(111)\n    ax.set_title(\"Season {} ({}-{})\".format(season['season_num'], season['year'], season['year']+1))\n    ax.scatter(season['data']['loc_x'], season['data']['loc_y'])\n    draw_court(ax, outer_lines=True)\n    plt.ylim(-100,500)\n    plt.xlim(300,-300)",
        "text": "where do kobe take his shoot from ?",
        "id": 132
    },
    {
        "code": "for dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S':0, 'C':1, 'Q':2} ).astype(int)\ntrain_data.head()",
        "text": "convert categorical feature to numeric we can now convert the embark feature by create a new numeric port feature  . ",
        "id": 133
    },
    {
        "code": "def f(x):\n    return 1.5*x\ndef intersection(F1,F2,x0):\n    return optimize.fsolve(lambda x:F1(x) - F2(x),x0)\nintersection1 = intersection(objective, f,0)\nprint(\"intersection 1:\" , intersection1 , f(intersection1))\nintersection2 = intersection(objective, f,10)\nprint(\"intersection 2:\"  , intersection2 , f(intersection2))",
        "text": "find the point of intersection between the function $ objective $ above and the function $ f ( x ) =1 . 5 x   - ",
        "id": 134
    },
    {
        "code": "df = pd.read_csv('/Users/kiefer/github-repos/DSI-SF-2/datasets/breast_cancer_wisconsin/breast_cancer.csv', na_values='?')\ndf['Bare_Nuclei'].hist()\ndf.ix[df['Bare_Nuclei'].isnull()]['Class'].value_counts()\ndf.dropna(inplace=True)\ndf.isnull().sum()\ndf['Class'].unique()\nX = df.drop(['Sample_code_number', 'Class'], axis = 1)\ny = df['Class'].map(lambda x: 1 if x == 4 else 0)\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nXn = ss.fit_transform(X)",
        "text": "breast cancer    -  load the data   -   be there any miss value ? impute or clean if so   -  select a classification target and predictor  . ",
        "id": 135
    },
    {
        "code": "sales_data.volume_sold.head()\nsales_data.volume_sold.tail()\nsales_data.volume_sold.mean()",
        "text": "a column can be select by  . name and it , too , ha a head ( ) , and tail ( ) , and can do statistical operation  . ",
        "id": 136
    },
    {
        "code": "ten_lowest_quality_article_country = hight_quality_per_country.sort_values()[:10]\ndf_10_lowest_quality_country = pd.DataFrame({\"country\" : ten_lowest_quality_article_country.index,\n                                                                    \"percentage\" :ten_lowest_quality_article_country.values})\ndf_10_lowest_quality_country",
        "text": "（4) 10 low   -   rank country in term of number of ga and fa   -   quality article a a proportion of all article about politician from that country",
        "id": 137
    },
    {
        "code": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=1000, random_state=0)\nlr.fit(X_train_std, y_train)\nplot_decision_regions(X_combined_std, y_combined, classifier=lr, test_idx=range(105,150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.show()\nlr.predict_proba(X_test_std[0, :].reshape(1,-1))",
        "text": "train a logistic regression model with scikit   -   learn",
        "id": 138
    },
    {
        "code": "\nfrom sklearn.cross_validation import cross_val_score\ndecisionTree = DecisionTreeClassifier()\naccuracy_list = cross_val_score(decisionTree, X, y, cv=10, scoring='accuracy')\naccuracy_gb = accuracy_list.mean()\nprint(\"Accuracy Using Cross Validation With Decision Tree\",accuracy_gb)",
        "text": "cross validation use decision tree",
        "id": 139
    },
    {
        "code": "\nimport threading\ndef worker(num):\n    print('Worker: %s' % num)\nthreads = []\nfor i in range(5):\n    t = threading.Thread(target=worker, args=(i,))\n    threads.append(t)\n    t.start()",
        "text": "it be useful to be able to spawn a thread and pas it argument to tell it what work to do  .  any type of object can be pass a argument to the thread  .  this example pass a number , which the thread then print  .  the integer argument be now include in the message print by each thread  . ",
        "id": 140
    },
    {
        "code": "X = df.iloc[:,yXCorr.index[:3]]\nmodel.fit(X,y)\nyHat = model.predict(X)\nmodel.score(X,y)",
        "text": "a you can see above , our model score doe n't decrease by much , and we be only use three feature  .  to drive home the point that the correlation matter , let 's repeat our test with the three least correlate variable  . ",
        "id": 141
    },
    {
        "code": "\nimport skimage.io as io\nfig = plt.figure(figsize=(10, 7))\nfor i in range(20):\n    ax = fig.add_subplot(1, 20, i + 1, xticks=[], yticks=[])\n    ax.imshow(np.reshape(X[i,:], (8, 8), 'A'))\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import cross_validation\n#to see how robust our results are, do it (cross_val_score) with 10-fold cross-validation",
        "text": "install [ scikit   -   image ] ( <url> / ) by call conda install scikit   -   image  at the command line  . ",
        "id": 142
    },
    {
        "code": "def load_play_data(filename):\n    records = []\n    with open(filename, 'rt') as csvfile:\n        csvreader = csv.reader(csvfile, delimiter=',')\n        for row in csvreader:\n            if len(row) == 5:  \n                records.append({\n                    \"Outlook\": row[0],\n                    \"Temp\": float(row[1]),\n                    \"Humidity\": float(row[2]),\n                    \"Windy\": row[3],\n                    \"Target\": row[4]\n                })\n    return records\nplay_data = load_play_data(\"data/dt_play.data.csv\")",
        "text": "the data set be , a usual , store in a comma   -   separate text file  .  we read it and store it a a list of record , where each record be represent use a dict  . ",
        "id": 143
    },
    {
        "code": "sns.heatmap(train.isnull(),cbar=False, cmap='magma',yticklabels=False)",
        "text": "lets check the heatmap again  - ",
        "id": 144
    },
    {
        "code": "fetch_housing_data()\nimport pandas as pd\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)",
        "text": "load the data use panda this function return a panda dataframe object contain all the data  . ",
        "id": 145
    },
    {
        "code": "le_sex = pp.LabelEncoder()\ny = le_sex.fit_transform(tc['sex'].astype(str))\ny\nle_child = pp.LabelEncoder()\ny1 = le_child.fit_transform(tc['child'].astype(str))\ny1\nX = tc.drop(['survived'],axis = 1)",
        "text": "preprocessing the sex and child column",
        "id": 146
    },
    {
        "code": "plt.figure(figsize=(8, 4))\nplt.scatter(pvols, prets,\n            c=prets / pvols, marker='o')\n            \nplt.scatter(tvols, trets,\n            c=trets / tvols, marker='x')\n            \nplt.plot(statistics(opt_sharpe['x'])[1], statistics(opt_sharpe['x'])[0],\n         'r*', markersize=15.0)\n            \nplt.plot(statistics(opt_variance['x'])[1], statistics(opt_variance['x'])[0],\n         'y*', markersize=15.0)\n            \nplt.grid(True)\nplt.xlabel('expected volatility')\nplt.ylabel('expected return')\nplt.colorbar(label='Sharpe ratio')",
        "text": "cross indicate the optimal portfolio give a certain target return * the dot be portfolio * figure show two large star , 1  .  ) minimum volatility portfolio ( the leftmost portfolio ) 2  .  ) maximum sharpe ratio ( top   -   middle portfolio )",
        "id": 147
    },
    {
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport os\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()",
        "text": "layer nest operation we start by load the necessary library and reset the computational graph  . ",
        "id": 148
    },
    {
        "code": "import cv2\nimport numpy as np\nimage = cv2.imread('images/input.jpg')\nM = np.ones(image.shape, dtype = \"uint8\") * 175 \nadded = cv2.add(image, M)\ncv2.imshow(\"Added\", added)\nsubtracted = cv2.subtract(image, M)\ncv2.imshow(\"Subtracted\", subtracted)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\nM = np.ones(image.shape, dtype = \"uint8\") * 75 \nM",
        "text": "arithmetic operation these be simple operation that allow u to directly add or subract to the color intensity  .  calculate the per   -   element operation of two array  .  the overall effect be increase or decrease brightness  . ",
        "id": 149
    },
    {
        "code": "def map_plotter(x_dict, y_dict):\n    lattitude = [] \n    longitude = [] \n    \n    x_list = list(x_dict.keys()) \n    for item in x_list:\n        longitude.append(float(x_dict[item])) \n    \n    y_list = list(y_dict.keys()) \n    for item in y_list:\n        lattitude.append(float(y_dict[item]))\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from mpl_toolkits.basemap import Basemap\n    plt.figure(figsize = (20,10))\n    map = Basemap()\n    map.drawcoastlines()\n    x,y = map(longitude, lattitude)\n    map.scatter(x,y,marker=\"D\", color=\"r\")\n    return plt.show()\n\n\n\n\n#see section: \"Graphing by fossil age\" for demonstration of this function",
        "text": "map _ plotter   -  must supply two dictionary with match keys  -  the function take in two dictionary with match key and plot both dictionary  value a a scatterplot on top of a map , and return it",
        "id": 150
    },
    {
        "code": "g = sns.FacetGrid(titanic, col=\"Survived\", row=\"Pclass\")\ng.map(sns.kdeplot, \"Age\", shade=True)\nsns.despine(left=True, bottom=True)\nplt.show()",
        "text": "create conditional plot use two condition",
        "id": 151
    },
    {
        "code": "fig, ax = plt.subplots(figsize=(8,4))\nax.plot(np.arange(iters), cost, 'r')\nax.set_xlabel('Iterations')\nax.set_ylabel('Cost')\nax.set_ylim(4.0)\nax.set_title('Error vs. Training Epoch')\nax.grid(True)",
        "text": "look pretty good  .  remember that the gradient decent function also output a vector with the cost at each train iteration , we can plot it a well  .  since the cost always decrease   -   this be an example of a convex optimization problem  . ",
        "id": 152
    },
    {
        "code": "bg_df['tokenized_text'] = bg_df['paragraphs'].apply(lambda x: nltk.word_tokenize(x))",
        "text": "now we have all the text in a dataframe we can look at a few thing  .  first let 's tokenize the text with the same tokenizer a we use before  .  we will just save the token a a list for now , no need to convert to text  . ",
        "id": 153
    },
    {
        "code": "loss, acc = model.evaluate(test_X, test_Y, batch_size=batch_size, verbose=1)\nprint('')\nprint('Got %.2f%% accuracy' % (acc * 100.))",
        "text": "test time now it 's time to actually test the network  .  get above   -  65 %   - ",
        "id": 154
    },
    {
        "code": "3-2\n5*8\n1/2\n1./2\n2**6",
        "text": "elementary arithmetic operation python be capable of work like a calculator with some caveat  . ",
        "id": 155
    },
    {
        "code": "\nvect = CountVectorizer(ngram_range=(1, 2))\nX_train_dtm = vect.fit_transform(X_train)\nX_train_dtm.shape\nprint(vect.get_feature_names()[-50:])",
        "text": "ngram _ range ,   -  tuple ( min _ n , max _ n ) , default= ( 1 , 1 )   -   the low and upper boundary of the range of n   -   value for different n   -   gram to be extract   -  all value of n such that min _ n   -  n   -  max _ n will be use  . ",
        "id": 156
    },
    {
        "code": "lines = sc.parallelize([\"hello world\", \"hi\"])\nwords = lines.flatMap(lambda line: line.split(\" \"))\nwords.first()\nwords.take(3)",
        "text": "sometimes we want to produce multiple output element for each input element  .  the operation to do this be call flatmap ( )  . ",
        "id": 157
    },
    {
        "code": "pAudio.record(3)\npAudio.save(\"Recording_1.pdm\")",
        "text": "record and play record a 3   -   second sample and save it into a file  . ",
        "id": 158
    },
    {
        "code": "x = np.random.randint(0,21,10)\nx\nx[x.argmax()] = 0\nx",
        "text": "create random vector of size 10 and replace the maximum value with 0",
        "id": 159
    },
    {
        "code": "Stress_Indicator_2012 = pd.read_excel('Data\\DATA Storebælt\\Stress\\DD2012.xlsx', header=None)\nStress_Indicator_2012 = Stress_Indicator_2012.T\nStress_Indicator_2012 = Stress_Indicator_2012.append(['NaN'])\nStress_Indicator_2012.columns = ['SG1', 'SG2','SG3','SG4','SG5','SG6','SG7','SG8','SG9','SG10','SG11','SG12','SG13','SG14','SG15']\nStress_Indicator_2012 = Stress_Indicator_2012.drop(['SG10','SG11','SG12','SG13','SG14','SG15'], axis=1)\nStress_Indicator_2012.index = pd.date_range('2012-01-01', '2012-12-31', freq='d')\nStress_Indicator_2012.index.name='Timestamp'\n#Stress_Indicator_2012.head()",
        "text": "stress performance indicator   -   2012",
        "id": 160
    },
    {
        "code": "\ndists_one = classifier.compute_distances_one_loop(X_test)\ndifference = np.linalg.norm(dists - dists_one, ord='fro')\nprint('Difference was: %f' % (difference, ))\nif difference < 0.001:\n    print('Good! The distance matrices are the same')\nelse:\n    print('Uh-oh! The distance matrices are different')",
        "text": "you should expect to see a slightly good performance than with  k = 1   . ",
        "id": 161
    },
    {
        "code": "def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000): \n        with pd.option_context(\"display.max_columns\", 1000): \n            display(df)\ndisplay_all(df_raw.tail().transpose())\ndisplay_all(df_raw.describe(include='all').transpose())",
        "text": "in any sort of data science work , it 's   -  important to look at your data  -  , to make sure you understand the format , how it 's store , what type of value it hold , etc  .  even if you ve read description about your data , the actual data may not be what you expect  . ",
        "id": 162
    },
    {
        "code": "plot_decision_boundary(XT, ZT, W=W, b=b)",
        "text": "decision boundary on the test set",
        "id": 163
    },
    {
        "code": "def removeZeros(header, data):\n    rawList = data.tolist()\n    resultant_list = []\n    i = header.index('Open')\n    for row in rawList:\n        if (row[i] is True):\n            resultant_list.append(row)\n    return header, np.array(resultant_list)\nheaderRawTrain, dataRawTrain = removeZeros(headerRawTrainZero, dataRawTrainZero)\nheaderRawTest, dataRawTest = removeZeros(headerRawTestZero, dataRawTestZero)\nheaderRawTrainLog, dataRawTrainLog = removeZeros(headerRawTrainZeroLog, dataRawTrainZeroLog)\nheaderRawTestLog, dataRawTestLog = removeZeros(headerRawTestZeroLog, dataRawTestZeroLog)",
        "text": "data load and zero ( not open ) record remove",
        "id": 164
    },
    {
        "code": "n = 600851475143\ni = 2\nwhile i^2 < n:\n    while n % i == 0:\n        n = n / i\n    i = i + 1\nprint(n)",
        "text": "the prime factor of 13195 be 5 , 7 , 13 and 29 .  what be the large prime factor of the number 600851475143 ?",
        "id": 165
    },
    {
        "code": "RSS = np.sum((Y_train - train_predict) ** 2)\nprint(RSS)\nESS = np.sum(train_predict - np.mean(Y_train)) ** 2\nprint(ESS)\nR_squared = ESS/(ESS+RSS)\nprint(RSS)\nmse_train = np.mean((Y_train - lm.predict(X_train))**2)\nmse_test = np.mean((Y_test - lm.predict(X_test))**2)\nprint('mean squared error for the training set: {}'.format(mse_train))\nprint('mean squared error for the test set: {}'.format(mse_test))",
        "text": "mse   -   msr > use train data",
        "id": 166
    },
    {
        "code": "ad_data['Age'].hist(bins=30)\nplt.xlabel('Age')",
        "text": "exploratory data analysis let 's use seaborn to explore the data   -  create a histogram of the age  - ",
        "id": 167
    },
    {
        "code": "graphlab.canvas.set_target('ipynb')\ncat = image_train[18:19]\ncat['image'].show()\nknn_model.query(cat)\ndef get_images_from_ids(query_result):\n    return image_train.filter_by(query_result['reference_label'],'id')\ncat_neighbors = get_images_from_ids(knn_model.query(cat))\ncat_neighbors['image'].show()\ncar = image_train[8:9]\ncar['image'].show()\nget_images_from_ids(knn_model.query(car))['image'].show()",
        "text": "use image retrieval model with deep feature to find similar image",
        "id": 168
    },
    {
        "code": "obj = pd.Series(['a', 'b', 'c', 'd'])\nobj",
        "text": "< a name=  series  >        -  series a series be a one   -   dimensional array   -   like object contain an array of data ( of any numpy data type ) and an associate array of data label , call it index  .  by default , the index just consist of ordinary array index , i . e  .  consecutive integer start from zero  . ",
        "id": 169
    },
    {
        "code": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(model_df_with_features.drop(labels=[\"user_id\",\"artist_id\",\"artist_name\", \"no_plays\",\"country\"],axis=1),\n                                                   model_df_with_features[\"no_plays\"],\n                                                    test_size=0.3, random_state = 123)\nX_train.shape\nfrom sklearn.linear_model import LinearRegression\nreg_model = LinearRegression()\ntrain_model = reg_model.fit(X_train,y_train)\npred_test = train_model.predict(X_test)\npred_test[0:12]\nfrom sklearn.metrics import r2_score\nr2_score(y_test, pred_test)",
        "text": "train a regression model < a name=  model _ train  >",
        "id": 170
    },
    {
        "code": "pl.plot(timeArray, distArray, color='b', ls='-')\npl.xlabel('time (s)')           \npl.ylabel('distance (m)')       # ylabel is the ordinate",
        "text": "to plot distarray vs .  timearray with a blue solid line ,",
        "id": 171
    },
    {
        "code": "q232_errors = ...\n...\n_ = lab08.grade('q232')",
        "text": "use errors  , compute the error for the line with slope 16000  on the close _ novas  dataset  .  then make a scatter plot of the error   -  hint , * to make a scatter plot of the error , plot the error for each supernova in the dataset  .  put the speed on the horizontal axis and the error on the vertical axis  . ",
        "id": 172
    },
    {
        "code": "\ndfoas_merge=dfoas_merge.dropna(how='any')\ndfoas_merge.shape\ndfoas_merge.dtypes\ndfoas_merge.head(6)\ndfoas_merge.info()\nprint (dfoas_merge.CDR[(dfoas_merge.CDR == 0.0)].count(),\",\", dfoas_merge.CDR[(dfoas_merge.CDR == 1.0)].count())",
        "text": "drop row in the merge dataset with nan in any column  .  we see that there be 809   -   570=239 row with nan value ( miss data ) in at least one column  . ",
        "id": 173
    },
    {
        "code": "\nsp500.Price < 100\nsp500[sp500.Price < 100]\nr = sp500[(sp500.Price < 10) & \n          (sp500.Price > 0)] [['Price']]\nr",
        "text": "select row by boolean selection",
        "id": 174
    },
    {
        "code": "\ncrimes_wea.to_csv('data/crime/crimes_weather.csv')\ncrime_wea=pd.read_csv('data/crime/crimes_weather.csv')\ncrime_wea.loc[:, ['Arrest', 'Domestic']]=crime_wea.loc[:, ['Arrest', 'Domestic']].astype('bool')\ncrime_wea.info(null_counts=True)\ncrime_wea['Date']=pd.to_datetime(crime_wea['Date'])\ncrime_wea.index=crime_wea['Date']\ncrime_wea.head(10)\ncrime_wea['ID'].resample('A').count().plot()\nplt.xlabel('Year')\nplt.ylabel('Crime counts')\nplt.title('Crimes per year')\nplt.show()",
        "text": "we can then save the new data frame a a csv file for future analysis  .  in the follow section , i will use other data set and this new data with weather   information to do some exploratory analysis",
        "id": 175
    },
    {
        "code": "linkNYC['lonlat']=list(zip(linkNYC.longitude,linkNYC.latitude))\nlinkNYC['geometry']=linkNYC[['lonlat']].applymap(lambda x:shapely.geometry.Point(x))\nlinkNYC.head()\nlinkNYC = gpd.GeoDataFrame(linkNYC)\nlinkNYC.crs = from_epsg(4326)",
        "text": "combine long lat into a column like you do in the lab to greate a  geometry  column for the dataframe , then convert the dataframe into a geodataframe  _ linknyc _  and set native coordinate frame to lat/lon a you do in the lab  . ",
        "id": 176
    },
    {
        "code": "\naverage_daily_customers = data[['Customers']].resample('D').mean()\naverage_daily_customers['Customers'].rolling(freq='D', window=15, center=True).mean().plot()",
        "text": "plot the 15 day roll mean of customer",
        "id": 177
    },
    {
        "code": "\nscatter3D(y_3, azim = 140, elev = 32, xlim=(-6,6),ylim=(-6,6),zlim=(-6,6))\ncmmds_3, cmmds_3_evals = cmdscale(y_3,dim=2)\ncmmds_3 = pd.DataFrame(cmmds_3)\ncmmds_3.columns = ['first','second']\nfig = plt.figure(figsize=(9,5))\nax = fig.add_subplot(1,1,1)\nax.scatter(x = 'first',y = 'second',data = x_uniform, alpha = 0.5)\nax.scatter(x = 'first',y = 'second',data = cmmds_3,c = 'purple', alpha = 0.5)\nax.set_title('Original and recovered latent variables in 2D space')",
        "text": "linear embed of uniform random data",
        "id": 178
    },
    {
        "code": "yearly = data['2000':].resample('A').mean()\n(yearly > 40).sum()\nyearly.plot()\nplt.axhline(40, linestyle='--', color='k')",
        "text": "< div class=  alert alert   -   success  >   question   , and be there exceedance of the yearly limit value of 40 µg/m3 since 2000 ?",
        "id": 179
    },
    {
        "code": "\ny_2017 = df_2017['Happiness.Score']\nX_2017 = df_2017.drop(['Happiness.Score','Happiness.Rank','Country','Whisker.high','Whisker.low'],axis=1)\npredict_2017=lm.predict(X_2017)",
        "text": "eliminate feature in 2017 dataset to fit generic model   -  apply prediction on dataset",
        "id": 180
    },
    {
        "code": "long_animals = df['length'] > 40\ndf[long_animals]",
        "text": "display all of the animal that be great than 40 cm  . ",
        "id": 181
    },
    {
        "code": "group_type = new_df.groupby('type')\ncity_type = [\"Rural\", \"Suburban\", \"Urban\"]\nfare = [4255.09, 20335.69, 40078.34]\ncolor = [\"gold\", \"lightskyblue\", \"lightcoral\"]\nexplode = (0, 0, 0.1)\nplt.pie(fare, explode=explode, labels=city_type, colors=color, autopct='%1.1f%%',shadow=True, startangle=100)\nplt.title(\"% of Total Fares by City Type\")\nplt.axis('equal')\nplt.show()",
        "text": "of total fare by city type",
        "id": 182
    },
    {
        "code": "cp_weather.explain(True)",
        "text": "inspect execution plan let u have a look at the execution plan of the checkpointed dataframe",
        "id": 183
    },
    {
        "code": "x_train, x_test, y_train, y_test = train_test_split(data_dummies, subscribed, random_state=42)\nx_train.shape",
        "text": "train test split of our train data ,",
        "id": 184
    },
    {
        "code": "titanic.loc[(titanic.who == \"child\") & (titanic.age > 18), \"who\"] = \"woman\"",
        "text": "in this case , we have to use the value of < font color=  blue  > sex   to figure out the value of < font color=  blue  > who    .  now change the value of < font color=  blue  > who   for this record  . ",
        "id": 185
    },
    {
        "code": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(model_df_with_features.drop(labels=[\"user_id\",\"target\",\"country\"],axis=1),\n                                                   model_df_with_features[\"target\"],\n                                                    test_size=0.3, random_state = 123)\nX_train.shape\ny_train.value_counts()\nfrom sklearn.linear_model import LogisticRegression\nlr_model = LogisticRegression()\ntrain_model = lr_model.fit(X_train,y_train)",
        "text": "train a binary classification model < a name=  model _ train  >",
        "id": 186
    },
    {
        "code": "with tf.Session() as sess:\n    saver.restore(sess, './transnet_densenet')\n    test_accuracy = evaluate(X_test, y_test)\n    print(\"Test Accuracy = {:.4f}\".format(test_accuracy))",
        "text": "evaluate the performance of the model on the test set  . ",
        "id": 187
    },
    {
        "code": "import re\ntest = \"one 1 two 2 three 3 four 4 five 5\"\nre.findall(r\"\\w+ \\d\", test)\nfor item in re.findall(r\"\\w+ \\d\", test):\n    x = item.split(\" \")\n    print(x[0])\n    print(x[1])\ntest = \"one 1 two 2 three 3 four 4 five 5\"\nre.findall(r\"(\\w+) (\\d)\", test)\nall_subjects = open(\"enronsubjects.txt\").read()",
        "text": "back to regular expression for a sec    -  group with multiple match in the same string",
        "id": 188
    },
    {
        "code": "s2prd = \"%s.SAFE/%s.xml\" % (s2path, s2meta)\nreader = ProductIO.getProductReader(\"SENTINEL-2-MSI-60M-UTM31N\")\nproduct = reader.readProductNodes(s2prd, None)\nwidth = product.getSceneRasterWidth()\nheight = product.getSceneRasterHeight()\nname = product.getName()\ndescription = product.getDescription()\nband_names = product.getBandNames()\nprint(\"Product: %s, %d x %d pixels\" % (name, width, height))\nprint(product.getSceneGeoCoding())",
        "text": "open the product and get some information about it",
        "id": 189
    },
    {
        "code": "from sklearn import datasets,svm\ndigits=datasets.load_digits()\nX_digits=digits.data\ny_digits=digits.target\nsvc=svm.SVC(C=1, kernel=\"linear\")\nsvc.fit(X_digits[:-100],y_digits[:-100]).score(X_digits[-100:],y_digits[-100:])",
        "text": "score and cross   -   validate score",
        "id": 190
    },
    {
        "code": "\nrandForest = RandomForestRegressor(featuresCol = 'indexedFeatures', labelCol = 'label', \n                                   featureSubsetStrategy=\"auto\",impurity='variance', maxBins=100)\npipeline = Pipeline(stages=[regFormula, featureIndexer, randForest])\nparamGrid = ParamGridBuilder() \\\n    .addGrid(randForest.numTrees, [10, 25, 50]) \\\n    .addGrid(randForest.maxDepth, [3, 5, 7]) \\\n    .build()\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=RegressionEvaluator(metricName=\"rmse\"),\n                          numFolds=3)\ncvModel = crossval.fit(trainData)\npredictions = cvModel.transform(testData)\nevaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\nr2 = evaluator.evaluate(predictions)\nprint(\"R-squared on test data = %g\" % r2)",
        "text": "hyper   -   parameter tune , train a random forest model use hyper   -   parameter tune and cross   -   validation notice that a expect , the parameter tune and cross   -   validation improve the model performance ( r   -   sqr ) significantly on test data  . ",
        "id": 191
    },
    {
        "code": "from scipy.misc import imread, imsave, imresize\nimg = imread('images/cat.jpeg')\nprint(img.dtype, img.shape)\nprint(img)\nimg_tinted = img * [1, 0.95, 0.9]\nimg_tinted = imresize(img_tinted, (300, 300))\nimsave('images/cat_tinted.jpeg', img_tinted)\nfrom PIL import Image\npil_im = Image.open('images/cat.jpeg')\npil_im\npil_im2 = Image.open('images/cat_tinted.jpeg')\npil_im2",
        "text": "scipy provide some basic function to work with image  .  for example , it ha function to read image from disk into numpy array , to write numpy array to disk a image , and to resize image  .  here be a simple example that showcase these function ,",
        "id": 192
    },
    {
        "code": "distances = get_distances(features_test[2], features_train)\nnp.argmin(distances)",
        "text": "take the query house to be third house of the test set ( features _ test [ 2 ] )  .  what be the index of the house in the train set that be close to this query house ?",
        "id": 193
    },
    {
        "code": "pp.plot()\nfrom points.centrography import hull, mbr, mean_center, weighted_mean_center, manhattan_median, std_distance,euclidean_median,ellipse",
        "text": "we can use pointpattern class method   -  plot  -  to visualize   -  pp  - ",
        "id": 194
    },
    {
        "code": "X = np.arange(8)\nprint(X)\nY = X + 0.5\nprint(Y)\nprint(np.subtract(X,Y))\nprint(np.subtract.outer(X,Y))\nC = 1.0 / np.subtract.outer(X, Y)\nprint(C)\nprint(np.linalg.det(C))",
        "text": "give two array , x and y , construct the cauchy matrix c ( cij =1/ ( xi   -   yj ) )",
        "id": 195
    },
    {
        "code": "bat_df[(bat_df['yearID']==1997) & (bat_df['PA']>=400) & (bat_df['lgID']=='NL')].sort_values('OBP', ascending=False).head(1)",
        "text": "same question but for 1997 and only in the nl ( check league id ) ?",
        "id": 196
    },
    {
        "code": "a=[1,2,3,4]\nb=[17,12,11,10]\nc=[-1,-4,5,9]\nlist(map(lambda x,y:x+y, a,b))\nlist(map(lambda x,y,z:x+y+z, a,b,c))\nlist(map(lambda x,y,z:2.5*x+2*y-z, a,b,c))",
        "text": "map ( ) can be apply to more than one list  .  the list have to have the same length  .  map ( ) will apply it lambda function to the element of the argument list , i . e  .  it first apply to the element with the 0th index , then to the element with the 1st index until the n   -   th index be reach ,",
        "id": 197
    },
    {
        "code": "import pandas as pd\ndatapath = \"datasets/lifesat/\"\noecd_bli = pd.read_csv(datapath+\"oecd_bli_2015.csv\",\n                       thousands = \",\")\nprint(oecd_bli.shape)\noecd_bli.head()\noecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"] == \"TOT\"]\noecd_bli.shape\n# >> Reduce the number of rows from 3292 to 888.",
        "text": "load and prepare life satisfaction data ,",
        "id": 198
    },
    {
        "code": "\nfor i, layer in enumerate(base_model.layers):\n    print(i, layer.name)\nfor layer in model.layers[:249]:\n    layer.trainable = False\nfor layer in model.layers[249:]:\n    layer.trainable = True\nopt = optimizers.SGD(lr=0.0001, momentum=0.9)\nmodel.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory2 = model.fit_generator(datagen.flow(X_train, y_train),\n            steps_per_epoch = train_size // batch_size,\n            epochs = 10,\n            validation_data = (X_valid, y_valid),\n            callbacks=[tb_callback_ln])",
        "text": "fine tune of the last layer   -   we will freeze the bottom n layer and train the remain top layer  . ",
        "id": 199
    },
    {
        "code": "arr = np.arange(9).reshape((3, 3))\narr[[1, 0, 2]]",
        "text": "< span style=  color , red  > 17 . how to swap two row in a 2d numpy array ?   q .  swap row 1 and 2 in the array   -  arr  - ",
        "id": 200
    },
    {
        "code": "\nprices_noheader = prices[1:]\nprices_noheader\nfor price in prices_noheader:\n    print(int(float(price)), ' ')\nfor price in prices_noheader:\n    if not price == '':\n        print(int(float(price)))\n    else:\n        print('None')\ndef extract_int_price(price):\n    if not price == '':\n        return int(float(price))\n    else:\n        return None\nfor price in prices_noheader:\n    print(extract_int_price(price))\n\nint_prices = []\nfor price in prices_noheader:\n    int_prices.append(extract_int_price(price))\nprint(int_prices)\n\nint_prices = [ extract_int_price(price) for price in prices_noheader ]\nprint(int_prices)",
        "text": "this list ha a couple of problem  .  first , it include the header  .  second , it 's all string even though price be numeric data  .  third , it contain some empty string  .  we ll have to clean it up  . ",
        "id": 201
    },
    {
        "code": "data = DataFrame([[1.,6.5,3.], [1.,NA,NA],\n                [NA,NA,NA], [NA,6.5,3.]])\ncleaned = data.dropna()\ndata\ncleaned",
        "text": "with dataframe object , these be a bite more complex  .  may want to drop row or column which be all na or just those contain an na  .  dropna by default drop any row contain a miss value ,",
        "id": 202
    },
    {
        "code": "data1 = np.vstack([np.random.multivariate_normal([1,1],[[.5,0],[0,.5]],150),\n                 np.random.multivariate_normal([1,3],[[.5,0],[0,.5]],150),\n                 np.random.multivariate_normal([3,1],[[.5,0],[0,.5]],150),\n                 np.random.multivariate_normal([3,3],[[.5,0],[0,.5]],150)])\nplt.scatter(data1[:,0], data1[:,1])\ndo_kmeans_4(km, data1)",
        "text": "repeat the above with 150 point per cluster",
        "id": 203
    },
    {
        "code": "\nsp500[:5]\nindex_moved_to_col = sp500.reset_index()\nindex_moved_to_col[:5]\nindex_moved_to_col.set_index('Sector')[:5]\nreindexed = sp500.reindex(index=['MMM', 'ABBV', 'FOO'])\nreindexed\nsp500.reindex(columns=['Price', \n                       'Book Value', \n                       'NewCol'])[:5]",
        "text": "move data to and from the index",
        "id": 204
    },
    {
        "code": "from sklearn import grid_search, cross_validation\nk = range(2, 100)\nparams = {\"n_neighbors\": k }\nkf = cross_validation.KFold(len(irisdf), n_folds = 5)\ngs = grid_search.GridSearchCV(\n    estimator  = neighbors.KNeighborsClassifier(),\n    param_grid = params,\n    cv         = kf)\ngs.fit(iris.data, iris.target)\ngs.grid_scores_\n# <Code Here> # plot",
        "text": "solution to solve k this be only one approach to the problem , but add in the distance  parameter ( instead of uniform ) would only be additive , note that the code would need some edit to handle it properly if do in the grid search , alternatively , make the change directly in the estimator  . ",
        "id": 205
    },
    {
        "code": "def harmonic_time_independent(x,n,L):\n    \n    return amplitude",
        "text": "define a function for the time   -   independent portion of an eigenstate input should be , $ x $ , $ n $ , and $ l   - ",
        "id": 206
    },
    {
        "code": "np.set_printoptions(threshold=6)\na = np.arange(15)\nnp.set_printoptions(threshold=np.nan)\na",
        "text": "print the full numpy array without truncate",
        "id": 207
    },
    {
        "code": "for lisr in lisrgc:\n    gtdrndic.update({'title': lisr.title})\n    lisauth.append(str(lisr.author))\n    for osliz in os.listdir(fullhom + metzdays):\n        with open(str(lisr.author) + '.meta', \"w\") as f:\n            rstrin = lisr.title.encode('ascii', 'ignore').decode('ascii')\n            \n            \n            \n            \n            \n            \n            \n            f.write(rstrin)\n#matdict",
        "text": "need to save json object  .  dict be create but it isnt save  .  loop through lisrgc twice , should only require the one loop  .  cycle through lisr and append to dict/concert to json , and also cycle through lisr . author meta folder save the json that wa create  . ",
        "id": 208
    },
    {
        "code": "model1 = Sequential()\nmodel1.add(Flatten(input_shape=INPUT_SHAPE))\nmodel1.add(Dense(100, activation='relu'))\nmodel1.add(Dense(30))\nprint(model1.summary())\nmodel1_hist_adam, model1_adam = fit_model(model1, data, '1dense100neuronlayer_adam', retrain = True, optimizer='adam')\nmodel1_hist_nadam, model1_nadam = fit_model(model1, data, '1dense100neuronlayer_nadam', retrain = True, optimizer='nadam')\nplot_loss([model1_hist_adam],['dense1_adam'])\nplot_loss([model1_hist_nadam],['dense1_nadam'])\nplot_loss([model1_hist_adam,model1_hist_nadam],['dense1_adam','dense1_nadam'])",
        "text": "model    -  1 dense hide layer with 100 unit",
        "id": 209
    },
    {
        "code": "\np = figure(x_axis_label='fertility (children per woman)', y_axis_label='female_literacy (% population)', tools='box_select')\nsource=ColumnDataSource(df)\np.circle(x='fertility', y='female literacy', source=source, selection_color='red', nonselection_alpha=0.2)\nshow(p)",
        "text": "now , we re go to add the box _ select tool to a figure and change the select and non   -   select circle glyph property so that select glyph be red and non   -   select glyph be transparent blue  .  be sure to experiment with the box select tool  . ",
        "id": 210
    },
    {
        "code": "plt.pie(cuisine_counts.id, autopct='%d%%', labels=(cuisine_counts.cuisine))\nplt.axis('equal')\nplt.title('Cuisines offered by foodwheel')\nplt.show()",
        "text": "let 's use this information to create a pie chart  .  make sure that your pie chart include , label for each cuisine ( i . e ,  american  ,  chinese  , etc  .  ) percent label use autopct a title use plt . axis to make the pie chart a perfect circle plt . show ( ) to display your chart",
        "id": 211
    },
    {
        "code": "l = [1,2,3,4]\nl1 = [\"a\", \"b\", \"c\", \"d\"]\nl2 = [\"d\", 4, 3, 1.2, [1,2]]\nprint(l)\nprint(l1)\nprint(l2)\nl.append(5)\nprint(l)\nprint(len(l))",
        "text": "list this be use to put a set of item in the list  .  let 's instantiate a list  . ",
        "id": 212
    },
    {
        "code": "\ndef resize_img(fpath, base):\n    img = Image.open(fpath)\n    rimg = img.resize((224,224))\n    img.close()\n    return rimg\nDATA_PATH='../data2/train/'\nfnames = list(glob.iglob(DATA_PATH+'*/*.JPEG'))\nN = 5000",
        "text": "resize image   -  url>",
        "id": 213
    },
    {
        "code": "\nW1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())\nb1 = tf.Variable(tf.random_normal([256]))\nW2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())\nb2 = tf.Variable(tf.random_normal([256]))\nW3 = tf.get_variable(\"W3\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())\nb3 = tf.Variable(tf.random_normal([10]))",
        "text": "create weight and bias > weight   -   bias for nn layer ( [ how to do xavier initialization on tensorflow ] ( <url> ) )   > > w be initialize by use xavier initializer  .    > > b be initialize to random variable with normal random distribution  . ",
        "id": 214
    },
    {
        "code": "start_year = 2002\nend_year = 2015\nsounding_times = get_sounding_times(start_year,1,1,0,1,\n                                    end_year,1,1,23,1)\npres_levels = [1013, 950, 925, 900, 850, 800, 750, 700, \n               650, 600, 550, 500, 400, 300, 200, 100] \nprint(len(sounding_times))",
        "text": "here be where we input the time and pressure level to get sound from  . ",
        "id": 215
    },
    {
        "code": "\nreq = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\nsrc = req.text\nsoup = BeautifulSoup(src, \"lxml\")\nmembers = []\nrows = soup.select('tr tr tr')\nfor row in rows:\n    detailCells = row.select('td.detail')\n    \n    if len(detailCells) is not 5: \n        continue\n        \n    rowData = [cell.text for cell in detailCells]\n    \n    name = rowData[0]\n    district = int(rowData[3])\n    party = rowData[4]\n    \n    tup = (name,district,party)\n    \n    members.append(tup)\nlen(members)",
        "text": "loop it all together let 's use a for loop to get em all  . ",
        "id": 216
    },
    {
        "code": "min_samples_leafs_max=10\nresultsauc = []\nresultsacc = []\nfor subsample in subsamples:\n    accuracy,logit_roc_auc,features=getAccuracyGBC(X_train,y_train,X_validation,y_validation,subsample = subsample,min_samples_leaf = min_samples_leafs_max,max_depth = max_depts_max,num_trees = trees_max,loss = losses_max,learning_rate = learning_rate_max)\n    print(subsample)\n    roc = logit_roc_auc\n    print (\"C-stat:\")\n    print(roc)\n    resultsauc.append(roc)\n    print(\"accuracy\")\n    print(accuracy)\n    resultsacc.append(accuracy)\n    print(\"\")\npd.Series(resultsauc, subsamples).plot();\npd.Series(resultsacc, subsamples).plot();",
        "text": "both accuracy and auc reach maximum at min sample leafs=10",
        "id": 217
    },
    {
        "code": "\nvocab_dict = defaultdict(int)          \nbi_dict = defaultdict(int)             \ntri_dict = defaultdict(int)            \nquad_dict = defaultdict(int)           \nquad_prob_dict = OrderedDict()              \ntri_prob_dict = OrderedDict()\nbi_prob_dict = OrderedDict()\nprint(\"---Preprocessing Time for Corpus loading: %s seconds ---\" % (time.time() - start_time))",
        "text": "for debug purpose only       uncomment the above two cell and ignore run the cell below if not debug",
        "id": 218
    },
    {
        "code": "\nchange_cols = ['id', 'id2', 'geography', 'change_1016_total', 'change_1016_natural', 'change_1016_births', 'change_1016_deaths', 'change_1016_migration', 'change_1016_international', 'change_1016_domestic', 'change_1516_total', 'change_1516_natural', 'change_1516_births', 'change_1516_deaths', 'change_1516_migration', 'change_1516_international', 'change_1516_domestic' ]\nchange.columns = change_cols\nchange.head(1)",
        "text": "some of those column name be way too long  .  let 's shorten them  . ",
        "id": 219
    },
    {
        "code": "animals = ['duck', 'rat', 'boar', 'slug', 'mammoth', 'gazelle']\nanimals = ['duck', 'rat', 'boar', 'slug', 'mammoth', 'gazelle']\nfor animal in animals:\n    print(animal.upper())",
        "text": "iterate through the follow list of animal , and print each one in all cap  . ",
        "id": 220
    },
    {
        "code": "columns = ['Data Source'] + [str(year) for year in range(1990, 2011)]\npop = df[columns].dropna()\npop.head()",
        "text": "select the column we need",
        "id": 221
    },
    {
        "code": "pickle.dump(xgbFinal, open(\"xgbFinal.pickle.dat\", \"wb\"))\ntrain_preds = pd.DataFrame(xgbFinal_train_preds)\ntest_preds = pd.DataFrame(xgbFinal_test_preds)\ntrain_preds.columns = ['RESPONSE']\ntest_preds.column = ['RESPONSE']\ntrain.to_csv('XGBoost Train.csv', sep=',')\ntrain_preds.to_csv('XGBoost Train Preds.csv', sep=',')\ntest.to_csv('XGBoost Test.csv', sep=',')\ntest_preds.to_csv('XGBoost Test Preds.csv', sep=',')\nslack_message(\"Files saved!\", 'channel')",
        "text": "save xgb model file and write  . csv file to work directory save xgb model file for future reference  .  similar function to load previously save file be comment out below  .  then , write all file to the work directory",
        "id": 222
    },
    {
        "code": "a = np.arange(3).reshape(1, 3)\nb = np.arange(3).reshape(3, 1)\nprint(\"a:\\n{}\".format(a))\nprint(\"b:\\n{}\".format(b))\nit = np.nditer([a,b,None])\nfor x,y,z in it: z[...] = x + y\nprint(it.operands[2])",
        "text": "consider two array with shape ( 1,3 ) and ( 3,1 ) , how to compute their sum use an iterator ?",
        "id": 223
    },
    {
        "code": "import pandas as pd\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)",
        "text": "load the data into panda",
        "id": 224
    },
    {
        "code": "train_set.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\")\ntrain_set.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",alpha = 0.1)\ntrain_set.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n    s=train_set[\"population\"]/100, label=\"population\", figsize=(10,7),\n    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n    sharex=False)\nplt.legend()\ntrain_set.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n    s=train_set[\"population\"]/100, label=\"population\",\n    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n    sharex=False)\nplt.legend()",
        "text": "discover and visualize the data to gain insight",
        "id": 225
    },
    {
        "code": "hh = [1.0/2.0, 1.0/10.0, 1.0/50.0]\nfor h in hh:\n    t,y = euler(t0,T,0,h)\n    ye = yexact(t)\n    plt.semilogy(t,np.abs(y-ye))\n    plt.legend(hh)\n    plt.xlabel('t')\n    plt.ylabel('log(error)')",
        "text": "study the effect of decrease step size  .  the error be plot in log scale  . ",
        "id": 226
    },
    {
        "code": "lens = np.array(list(map(len, trn)))\n(lens.max(), lens.min(), lens.mean())",
        "text": "look at distribution of length of sentence  . ",
        "id": 227
    },
    {
        "code": "hits , tracks , vertices = get_tracks_hits_vertices( ana, entry=43 , debug=True )\nfig=draw_vertex( hits , tracks , vertices.at(0) , planes=[0,1,2]  , tracks_leg_plane=0,leg_fontsize=25\n            , do_add_RdQaroundVertex=True            \n            , box_colors=['purple','red','black'], box_dimesions=[(20,40),(80,160),(150,300)]\n            , do_legend=True , legend_locs=['upper left','upper right','upper right'])\nfig.savefig(figures_path+'mupVertex_43_vertex_0.pdf')",
        "text": "a typical $ \\mu p $ vertex with additional undetected particle",
        "id": 228
    },
    {
        "code": "def plot_k_vs_inertia(clt_list):\n    for i, v in enumerate(clt_list):\n        plt.plot(v.n_clusters, np.sqrt(v.inertia_), '-o')\n    plt.xlabel(\"number of clusters (K)\")\n    plt.ylabel(\"inertia (distance)\")\nplot_k_vs_inertia(clt_model_list)",
        "text": "find elbow point which k be the best to use  .  one straightforward method be elbow method  .  ( <url> ) )  .  check the inertia ( sum of square distance of sample to their close cluster center  .  ) vs .  k can give an idea about elbow point  .  the function below plot the square root of inertia v k  . ",
        "id": 229
    },
    {
        "code": "\nw = complex(0.28, 0.96)\nn = 10\nmy_list = generate_sequence(w, n)\nplot_complex_list(my_list, style=\"points\")",
        "text": "now , we can simply execute the follow line ,",
        "id": 230
    },
    {
        "code": "class MyQueue(object):\n    def __init__(self):\n        \n        self.head = ListNode(0)\n        self.tail = self.head\n    \n    def enqueue(self, item):\n        \n        node = ListNode(item)\n        self.tail.next = node\n        self.tail = node\n    def dequeue(self):\n        \n        if self.head.next is None:\n            return None\n            \n        val = self.head.next.val\n        self.head = self.head.next\n        return val",
        "text": "no . 144 implement queue by link list implement a queue by link list  .  support the follow basic method , enqueue ( item )  .  put a new item in the queue  .  dequeue ( )  .  move the first item out of the queue , return it  . ",
        "id": 231
    },
    {
        "code": "left_curverad = np.absolute(((1 + (2 * left_coeffs[0] * 500 + left_coeffs[1])**2) ** 1.5) /(2 * left_coeffs[0]))\nright_curverad = np.absolute(((1 + (2 * right_coeffs[0] * 500 + right_coeffs[1]) ** 2) ** 1.5) /(2 * right_coeffs[0]))\ncurvature = (left_curverad + right_curverad) / 2\ncentre = center(719, left_coeffs, right_coeffs)\nmin_curvature = min(left_curverad, right_curverad)\nprint(\"Left radius: \", left_curverad, \"pixels\")\nprint(\"Right radius: \", right_curverad, \"pixels\")",
        "text": "determine curvature of the lane and vehicle position with respect to center  . ",
        "id": 232
    },
    {
        "code": "df2[\"exp condition\"]\ndf2=deepcopy(df)\ndf2[\"Condition\"]='None'\ndf2.loc[df2['exp condition'].str.contains('fixed'), 'Condition'] = 'Fixed'\ndf2.loc[df2['exp condition'].str.contains('live'), 'Condition'] = 'Live'\ndf2[\"Blocked\"]='Blocked'\ndf2.loc[df['filename'].str.contains('unblocked'),\"Blocked\"]='Unblocked'",
        "text": "< a id=makingsense  >      -  make sense let 's add some human   -   understandable tag to the dataframe , so that we can plot what we want more easily   -  a tag for the incubation time   -   a tag for the treatment condition ( bic/dmso )",
        "id": 233
    },
    {
        "code": "series_3 = frame['d']\nseries_3\nframe.sub(series_3, axis=0)",
        "text": "if you want to instead broadcast over the column , match on the row , you have to use an arithmetic method  . ",
        "id": 234
    },
    {
        "code": "jeopardy[\"Air Date\"] = pd.to_datetime(jeopardy[\"Air Date\"])\njeopardy\njeopardy.dtypes",
        "text": "the air date column should also be a datetime , not a string , to enable u to work with it more easily  . ",
        "id": 235
    },
    {
        "code": "max_OBP_1986 = bat_data[bat_data['yearID'] == 1986].pivot_table(values=['AB','BB','H','HBP','SF','PA'], index=['nameFirst','nameLast','playerID'],aggfunc='sum').reset_index()\nmax_OBP_1986['OBP'] = (max_OBP_1986['H']+max_OBP_1986['BB']+max_OBP_1986['HBP'])/(max_OBP_1986['AB']+max_OBP_1986['BB']+max_OBP_1986['HBP']+max_OBP_1986['SF'])\nmax_OBP_1986[max_OBP_1986['PA'] >= 400].sort_values(by='OBP',ascending=False).reset_index(drop=True).head(1)",
        "text": "who have the high obp in 1986 with at least 400 pa ? ( dataframe )",
        "id": 236
    },
    {
        "code": "\nresult = sm.ols(formula='Price ~ Rooms + Car + Distance', data=melHousing).fit()\nresult.summary()\nmodelName = 'Property Price(Y) ~ Number of Rooms(X1) + Number of Cars fit into the Garage(X2) + Distance from CBD(X3)'\nanalysisResult = analysisResult.append(_capture_Result(modelName, result), ignore_index=True)\nanalysisResult\nresult = sm.ols(formula='Car ~ Distance + Rooms', data=melHousing).fit()\nresult.summary()",
        "text": "regression analysis , property price ( y ) v number of room ( x1 ) + number of car fit into the garage ( x2 ) + distance from cbd ( x3 )",
        "id": 237
    },
    {
        "code": "X_train,X_test,Y_train,Y_test = train_test_split(nba[x_columns],nba[y_column],train_size=.7,random_state=100)",
        "text": "split data for train   -   test",
        "id": 238
    },
    {
        "code": "print(len(df_AzReviews[0]['reviewText'][0]))\nprint(len(df_AzReviews[1]['reviewText'][1]))",
        "text": "check the length ( number of character ) in entire reviewtext  field of the few record of review dataframe",
        "id": 239
    },
    {
        "code": "\nnum_train = 50\nsmall_data = {\n  'X_train': data['X_train'][:num_train],\n  'y_train': data['y_train'][:num_train],\n  'X_val': data['X_val'],\n  'y_val': data['y_val'],\n}\nlearning_rate = 1e-3\nweight_scale = 8e-2\nmodel = FullyConnectedNet([100, 100, 100, 100],\n                weight_scale=weight_scale, dtype=np.float64)\nsolver = Solver(model, small_data,\n                print_every=10, num_epochs=20, batch_size=25,\n                update_rule='sgd',\n                optim_config={\n                  'learning_rate': learning_rate,\n                }\n         )\nsolver.train()",
        "text": "now try to use a five   -   layer network with 100 unit on each layer to overfit 50 train example  .  again you will have to adjust the learn rate and weight initialization , but you should be able to achieve 100 % train accuracy within 20 epoch  . ",
        "id": 240
    },
    {
        "code": "import numpy as np\neip_x = np.array([1, 2])   \nprint(eip_x.dtype)         \neip_x = np.array([1.0, 2.0])   \nprint(eip_x.dtype)             \neip_x = np.array([1, 2], dtype=np.int64)   \nprint(eip_x.dtype)                         # Prints \"int64\"",
        "text": "datatypes every numpy array be a grid of element of the same type  .  numpy provide a large set of numeric datatypes that you can use to construct array  .  numpy try to guess a datatype when you create an array , but function that construct array usually also include an optional argument to explicitly specify the datatype  .  here be an example ,",
        "id": 241
    },
    {
        "code": "model_3 = ConvNet(weight_scale=0.001, hidden_dim=[500], reg=0.001, filter_size=[3, 3], \n                  num_filters=[32, 64], use_batchnorm=True)\nsolver_3 = Solver(model_3, data,\n                num_epochs=1, batch_size=50,\n                update_rule='adam',\n                optim_config={\n                  'learning_rate': 1e-3,\n                },\n                verbose=True, print_every=70)\nsolver_3.train()",
        "text": "two conv layer , each follow by a pool layer",
        "id": 242
    },
    {
        "code": "def add(a, b):\n    return a + b\nadd(1, 3)\nadd(\"hello\", \"world\")\nadd([1, 2, 3], [4, 5, 6])\nadd(str(1), \"hello\")",
        "text": "dynamic type the   -  type  -  of an object be n't resolve until the program in run  . ",
        "id": 243
    },
    {
        "code": "df = pd.read_csv('sample_submission.csv')\npath = 'for_test'\nbreed = '0'\nif os.path.exists(path):\n    shutil.rmtree(path)\nfor fname in df['id']:\n    path2 = '%s/%s' % (path, breed)\n    if not os.path.exists(path2):\n        os.makedirs(path2)\n    os.symlink('../../test/%s.jpg' % fname, '%s/%s.jpg' % (path2, fname))",
        "text": "preprocessing the test data set",
        "id": 244
    },
    {
        "code": "s = 'HelloWorld'\na = slice(5,50,2)\na.indices(10)\nfor i in range(*a.indices(len(s))):\n    print(s[i])",
        "text": "in addition , you can map a slice onto a sequence of a specific size by use it *indices ( size ) * method  .  this return a tuple * ( start , stop , step ) * where all value have be suitably limit to fit within bind ( a to avoid *indexerror* exception when index )  . ",
        "id": 245
    },
    {
        "code": "data = ImageClassifierData.from_paths(PATH, tfms=tfms)\nlearn = ConvLearner.pretrained(arch, data, precompute=True)\nlearn.fit(1e-2, 1)\nlearn.precompute=False\nlearn.fit(1e-2, 3, cycle_len=1)\nlearn.sched.plot_lr()\nlearn.fit(1e-2, 3, cycle_len=2)\nlearn.sched.plot_lr()\nlearn.save('224_lastlayer')\nlearn.load('224_lastlayer')",
        "text": "now , let 's create a new data object that include these augmentation transform  . ",
        "id": 246
    },
    {
        "code": "R = numpy.arange(-4,4+1e-9,0.1)\nX,Y = numpy.meshgrid(R,R)\nprint(X.shape,Y.shape)",
        "text": "a an example , we would like to plot the l2   -   norm function $ f ( x , y ) = \\sqrt { x^2 + y^2 } $ on the subspace $ x , y \\in [   -   4,4 ]   -  first , we create a meshgrid with appropriate size ,",
        "id": 247
    },
    {
        "code": "import math\ndef magnitude(x, y=2):\n    return math.sqrt(x**2 + y**2)\nmagnitude(1)",
        "text": "exercise 04 . 2 write a single function that take the component of a vector of length 2 or 3 and return the magnitude  .  use default argument to handle vector of length 2 or 3 with the same code  .  test your function for correctness against hand calculation for a selection of value  . ",
        "id": 248
    },
    {
        "code": "ridge = Ridge(fit_intercept=True, alpha=3.02492462312)\nridge.fit(x_train, y_train)    \npred = ridge.predict(x_test)\nMAE = mean_squared_error(y_test, pred)\nprint('Mean absolute error on test data: %0.8f' % MAE,'using alpha = 3.02492462312')\ncalc_params(x_train, y_train, \"alpha\", alpha, 5, lasso, \"Lasso Regression\")",
        "text": "so we can see that the alpha value of 3 . 02492462312 give the low mae on train data and we can use that to run the model on set aside test data  . ",
        "id": 249
    },
    {
        "code": "necessarySubsets = Implies(ContainsMatch(cSocks), chose2orMoreOfEither)",
        "text": "conversely , if there be a match among the choose sock , there must have be two or more of either black or white sock choose ,",
        "id": 250
    },
    {
        "code": "\np = figure(x_axis_label='Year', y_axis_label='Time', tools='box_select')\np.circle('Year','Time',source=source,\n         selection_color='red',\n         nonselection_alpha=0.1)\nshow(p)",
        "text": "selection and non   -   selection glyph in this exercise , you re go to add the box _ select tool to a figure and change the select and non   -   select circle glyph property so that select glyph be red and non   -   select glyph be transparent blue  . ",
        "id": 251
    },
    {
        "code": "other_counts = df_with_clusters[['left', 'cluster', 'work_accident', 'promotion_last_5years']].groupby(['left', 'cluster']).sum()\nperc_leavers_by_other = other_counts.transpose()[True] / other_counts.groupby('cluster').sum().transpose() * 100\nsns.heatmap(perc_leavers_by_other, annot=True, fmt='.1f', vmin=0, vmax=100)",
        "text": "within each cluster , the percentage of leaver per department band be relatively stable with a little variation  . ",
        "id": 252
    },
    {
        "code": "\naccept = [line for line in ____ if ____.startswith(str(para1))][1:]\nexamine = [______ for ______ in ______ if ______.startswith(______)][1:]\nreject = [________________________________________________________][1:]",
        "text": "parse the text now create 3 new list , accept  , examine  , reject  .   complete the list comprehension to filter through recs  and assign each recommendation to it correspond section   -  hint  -  , how do you know if a line belong to a section ? it start with the main paragraph number for that section  .  so use the   -  startswith ( )   -  method  . ",
        "id": 253
    },
    {
        "code": "from sklearn.model_selection import train_test_split\ndef train_test_rmse(feature_cols):\n    X = bikes[feature_cols]\n    y = bikes.total\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n    linreg = LinearRegression()\n    linreg.fit(X_train, y_train)\n    y_pred = linreg.predict(X_test)\n    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))\nprint (train_test_rmse(['temp', 'season', 'weather', 'humidity']))\nprint (train_test_rmse(['temp', 'season', 'weather']))\nprint (train_test_rmse(['temp', 'season', 'humidity']))\nprint (train_test_rmse(['casual', 'registered']))",
        "text": "compare model with train/test split and rmse",
        "id": 254
    },
    {
        "code": "fig, ax = plt.subplots(figsize=(12,8))\nfig = sm.graphics.plot_partregress(\"prestige\", \"income\", [\"income\", \"education\"], data=prestige, ax=ax)",
        "text": "partial regression plot this plot be show how much effect income ha on prestige when the effect of education and income be remove   -  url>",
        "id": 255
    },
    {
        "code": "x = raw_data.filter(feature_columns).apply(LabelEncoder().fit_transform).as_matrix()\ny = raw_data.filter(label_columns).as_matrix()\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.70, random_state = 100)",
        "text": "split data in train and test",
        "id": 256
    },
    {
        "code": "me['first_name']\nme.values()\nme.keys()",
        "text": "access value from a dictionary",
        "id": 257
    },
    {
        "code": "plt.title(\"Directors with Highest Average Revenue\")\ndf[df['director'].isin(director_list)].groupby('director')['revenue'].mean().sort_values(ascending=False).head(10).plot(kind='bar', colormap='autumn_r')\nplt.show()",
        "text": "director with high average revenue",
        "id": 258
    },
    {
        "code": "conn = sqlite3.connect(DATABASE)\ncur = conn.cursor()",
        "text": "create connection and a cursor  . ",
        "id": 259
    },
    {
        "code": "def harmonic_time_dependent(t,c,n,L):\n    \n    return amplitude",
        "text": "define a function for the time   -   dependent portion of an eigenstate  .  input should be , $ t $ , $ c $ , $ n $ , and $ l   - ",
        "id": 260
    },
    {
        "code": "PEF = float(purchases[30])/float(totals[30])\nprint(\"P(purchase | 30s): %F\" % PEF)",
        "text": "first let 's compute $ p ( e|f ) $ , where e be  purchase  and f be  you re in your 30 's   .  the probability of someone in their 30 's buy something be just the percentage of how many 30   -   year   -   old buy something ,",
        "id": 261
    },
    {
        "code": "\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndel df['Category']\ndf['State Bottle Cost'] = df['State Bottle Cost'].apply(lambda x: str(x).replace('$','')).astype(float)\ndf['State Bottle Retail'] = df['State Bottle Retail'].apply(lambda x: str(x).replace('$','')).astype(float)\ndf['Sale (Dollars)'] = df['Sale (Dollars)'].apply(lambda x: str(x).replace('$','')).astype(float)\ndf['City'] = df['City'].apply(lambda x: x.upper())",
        "text": "convert date field to datetime object",
        "id": 262
    },
    {
        "code": "df.loc[:,'hw1']",
        "text": "access one column by specify index label",
        "id": 263
    },
    {
        "code": "fig, ax = plt.subplots(figsize=(8,4))\npredictions = network.run(test_features).T\nax.plot(predictions[:,0], label='Prediction')\nax.plot(test_targets.values, label='Data')\nax.set_xlim(right=len(predictions))\nax.legend()\npredictions[:,0].shape",
        "text": "check out the prediction here , use the test data to view how well the network be model the data  .  if something be completely wrong here , make sure each step in your network be implement correctly  . ",
        "id": 264
    },
    {
        "code": "def get_dense_layers():\n    return [\n        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n        Flatten(),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        \n        Dense(1000, activation='relu')\n        ]\ndense_model = Sequential(get_dense_layers())\nfor l1, l2 in zip(dense_layers, dense_model.layers):\n    l2.set_weights(l1.get_weights())\ndense_model.add(Dense(763, activation='softmax'))",
        "text": "this be our usual vgg network just cover the dense layer ,",
        "id": 265
    },
    {
        "code": "import datetime\ndates = [datetime.datetime(year=int(row[1]), month=int(row[2]), day=1) for row in data]\ndate_counts = {}\nfor date in dates:\n    if date in date_counts:\n        date_counts[date] += 1\n    else:\n        date_counts[date] = 1\ndate_counts",
        "text": "find gun death by month and year",
        "id": 266
    },
    {
        "code": "def svrg(model, w0, idx_samples, n_iter, step, callback, verbose=True):\n    w = w0.copy()\n    w_old = w.copy()\n    n_samples = model.n_samples\n    callback(w)\n    for idx in range(n_iter):        \n        \n        \n        if idx % n_samples == 0:\n            callback(w)\n    return\nstep = 1 / model.lip_max()\ncallback_svrg = inspector(model, n_iter=n_iter)\nw_svrg = svrg(model, w0, idx_samples, n_iter=model.n_samples * n_iter,\n              step=step, callback=callback_svrg)",
        "text": "< a id=svrg  >      -  3 . 7 .  stochastic variance reduce gradient    -  question   -   finish the function svrg  below that implement the stochastic variance reduce gradient algorithm   -   test it use the next cell",
        "id": 267
    },
    {
        "code": "df.iloc[df.index.get_loc(stamp, method='bfill')]",
        "text": "find the row near to timestamp that be also after timestamp",
        "id": 268
    },
    {
        "code": "b = np.arange(12).reshape(3, 4)\nprint(b)\nb.sum(axis = 0)\nb.sum(axis = 1)\nb.cumsum(axis = 1)",
        "text": "by default , these operation apply to the array a though it be a list of number , regardless of it shape  .  however , by specify the  axis   parameter you can apply an operation along the specify axis of an array  . ",
        "id": 269
    },
    {
        "code": "X_df = df.iloc[:,:-1]\nX_df.head()\nX = X_df.as_matrix()\ny_df = df[\"quality\"].values",
        "text": "separate the dataset into   -  feature matrix x  -  and   -  respoinse vector y  - ",
        "id": 270
    },
    {
        "code": "\nfrom __future__ import print_function\nimport datacube\nimport xarray as xr\nfrom datacube.storage import masking\nimport json\nimport pandas as pd\nimport shapely\nfrom shapely.geometry import shape\nimport numpy as np \nimport fiona\nimport shapely.geometry\nimport rasterio.features\nimport rasterio\nfrom datacube.utils import geometry\nfrom datacube.helpers import ga_pq_fuser\nfrom datacube.storage.masking import mask_invalid_data\n\n\nfrom datacube.storage.storage import write_dataset_to_netcdf\n\nimport sys\nimport os.path\n\n\nimport matplotlib.pyplot as plt\n\n\nimport warnings\n\ndef eprint(*args, **kwargs):\n    print(*args, file=sys.stderr, **kwargs)",
        "text": "import the python module we need",
        "id": 271
    },
    {
        "code": "\ntext = str(rejected['Loan Title'])\nwc = WordCloud(font_path='/Library/Fonts/Verdana.ttf',\n               max_font_size=40).generate(text)\nplt.figure()\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show();\n#wc = WordCloud(background_color=\"white\", max_words=100)",
        "text": "loan title word cloud by rank",
        "id": 272
    },
    {
        "code": "import pandas as pd\nrawtram = './raw/Tram Boardings and Alightings 2011 - data.XLS'\ndf = pd.read_excel(rawtram,sheetname='Data', header=0,converters={'Route_Number':str,'Tram_Tracker_ID':str, 'Metlink_Stop_ID':str, 'VicgridX':str, 'VicgridY':str})\ndf",
        "text": "download raw tram board data , save a local copy in   -  raw directory download tram board and alight xl file manually  .  the web page ha a  i consent to term and condition / i be not a robot  button that prevent automate download ( or at least make it hard than i expect )  .  save file to   -  raw  directory",
        "id": 273
    },
    {
        "code": "column_name_unformat = []\ncolumn_names = []\ncolumn_names.append('date') \nfor i in range(9, 24):\n    col_title_unformat = soup.select(\"th\")[i]\n    col_title = \"%s\"%col_title_unformat\n    column_name_unformat.append(col_title)\n    \nfor i in column_name_unformat :\n    col_title = i.split('<th>')[1].split(\"</th>\")[0].replace(\"<br/>\", \" \")\n    column_names.append(col_title)\n    \n   \ncolumn_names",
        "text": "make a list of column name list name , column _ names",
        "id": 274
    },
    {
        "code": "row = train_data.sample(1)\ntitle_str = 'Iceberg' if row['is_iceberg'].values == 1 else 'Ship'\nband_1 = np.reshape(row['band_1'].values.tolist(), (75, 75))\nband_2 = np.reshape(row['band_2'].values.tolist(), (75, 75))\nfig = plt.figure(0, figsize=(10, 10))\nax = fig.add_subplot(1, 2, 1)\nax.set_title(title_str + ' - Band 2')\nax.imshow(band_2)\nplt.show()",
        "text": "here be an example image from the dataset with band 2 show separately",
        "id": 275
    },
    {
        "code": "def normalize(problem, points):\n  \n  meta = problem.objectives\n  all_objs = []\n  for point in points:\n    objs = []\n    for i, o in enumerate(problem.evaluate(point)):\n      low, high = meta[i].low, meta[i].high\n      \n      if high==low:continue;\n      o = (o-low)/(high-low)\n      objs.append(o)\n    all_objs.append(objs)\n  return all_objs",
        "text": "to compute most measure , data ( i . e objective ) be normalize  .  normalization be scale the data between 0 and 1 .  why do we normalize ? todo2 , answer the above question we normalize to make objective easy to compare  . ",
        "id": 276
    },
    {
        "code": "\nsp500[:5]\nsp500.loc['ZTS']\nindex_moved_to_col = sp500.reset_index()\nindex_moved_to_col[:5]\nindex_moved_to_col.set_index('Sector')[:5]\nreindexed = sp500.reindex(index=['MMM', 'ABBV', 'FOO'])\nreindexed\nsp500.reindex(columns=['Price', \n                       'Book Value', \n                       'NewCol'])[:5]",
        "text": "move data to and from the index",
        "id": 277
    },
    {
        "code": "pt.axis(\"equal\")\npt.contour(xmesh, ymesh, fmesh)",
        "text": "and then a a  contour plot  ,",
        "id": 278
    },
    {
        "code": "\ncontent = Variable(image_preprocess(content_dir), requires_grad=False).cuda()\nstyle = Variable(image_preprocess(style_dir), requires_grad=False).cuda()\ngenerated = Variable(content.data.clone(),requires_grad=True)\nimshow(image_postprocess(image_preprocess(content_dir)))\nimshow(image_postprocess(image_preprocess(style_dir)))",
        "text": "train    -  1 ) prepare image",
        "id": 279
    },
    {
        "code": "seed = 25\nnp.random.seed(seed)\nLda = gensim.models.ldamodel.LdaModel\nntopics = 10\nldamodel = Lda(corpus, num_topics=ntopics, id2word = dictionary, passes=50)",
        "text": "compute the model set the seed , indicate the number of topic and run the model",
        "id": 280
    },
    {
        "code": "temp2 = df_total[df_total['fips'] == 6037]\ntemp2.groupby('regionidzip').count()",
        "text": "all have the same county  .  get all entry in that county ,",
        "id": 281
    },
    {
        "code": "cats = ['sci.space', 'talk.politics.mideast', 'comp.sys.mac.hardware']\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=cats, remove=('headers', 'footers', 'quotes'), shuffle = False)\ntrain_data = newsgroups_train.data\ny_train = newsgroups_train.target\nnewsgroups_valid = fetch_20newsgroups(subset='test', categories=cats, remove=('headers', 'footers', 'quotes'), shuffle = False)\nvalid_data = newsgroups_valid.data\ny_valid = newsgroups_valid.target",
        "text": "decision boundry for multi   -   class logistic regression ,",
        "id": 282
    },
    {
        "code": "\ndef double_the_values(**kwargs):\n    doubled_ret = {}\n    for k,v in kwargs.items():\n        doubled_ret[k] = 2 * v\n    print(doubled_ret)    \n    return doubled_ret\nprint(double_the_values(a=1, b=2, c=3))\nmy_values = dict(a=1, b=2, c=3)\nprint(double_the_values(**my_values))",
        "text": "variable length keyword parameter ( \\*\\*kwargs ) work similar to the variable length positional parameter but with   -  unpack dictionaries  -  use double star sign ( \\*\\* ) or with   -  keyword argument like a=2 , b=3  -  \\*\\*kwargs be the common name use for the variable length keyword argument",
        "id": 283
    },
    {
        "code": "dfc=df.fillna(0)\ndfc=dfc.groupby(['playerID','nameFirst','nameLast','yearID','birthYear','age'],as_index=False).sum()\ndfc['PA']=dfc['AB']+dfc['BB']+dfc['IBB']+dfc['SH']+dfc['SF']\ndfc['OBP']=(dfc['H']+dfc['BB']+dfc['IBB']+dfc['SH']+dfc['SF'])/dfc['AB']\ndf10 = dfc[(dfc['yearID']==1977)&(dfc['PA']>=500)]\ndf10.nlargest(5,'OBP')",
        "text": "top 5 high obp ( on base percentage ) with at least 500 pa in 1977 ? ( dataframe )",
        "id": 284
    },
    {
        "code": "True or False",
        "text": "try out the follow logical operator ,   -     -  ,   -  , > , > = ,   -  , <    -   not , and , or  and logical value ,   -   true , false",
        "id": 285
    },
    {
        "code": "data = pd.DataFrame(raw_data['X'], columns=['X1', 'X2'])\ndata['y'] = raw_data['y']\npositive = data[data['y'].isin([1])]\nnegative = data[data['y'].isin([0])]\nfig, ax = plt.subplots(figsize=(12,8))\nax.scatter(positive['X1'], positive['X2'], s=50, marker='x', label='Positive')\nax.scatter(negative['X1'], negative['X2'], s=50, marker='o', label='Negative')\nax.legend()",
        "text": "we ll visualize it a a scatter plot where the class label be denote by a symbol ( + for positive , o for negative )  . ",
        "id": 286
    },
    {
        "code": "\ndf[\"inches\"] = df[\"length\"]\ndf[\"inches\"] = df[\"length\"] * 0.393701\ndf",
        "text": "length  be the animal 's length in centimeter  .  create a new column call inches  that be the length in inch   - ",
        "id": 287
    },
    {
        "code": "TVTimeLineordered = {}\nTVTimeLineordered[\"2010-03-02\"]= 50\nTVTimeLineordered[\"2011-04-19\"]= 50\nTVTimeLineordered[\"2012-04-10\"]= 50\nTVTimeLineordered[\"2013-04-02\"]= 50\nTVTimeLineordered[\"2014-04-08\"]= 50\nTVTimeLineordered[\"2014-04-08\"]= 50\nTVTimeLineordered[\"2016-04-21\"]= 50\nprint(TVTimeLineordered)\nodTVTimeLineordered = collections.OrderedDict(sorted(TVTimeLineordered.items()))\nprint(odTVTimeLineordered)",
        "text": "create data set for orederd date",
        "id": 288
    },
    {
        "code": "women_target_df = women_men[women_men['gender'] == 0]['dec'].copy()\npca = PCA(n_components = 3)\npca.fit(X = women_new_input_df, y = women_target_df)\ntransformed_new_input_df = pca.fit_transform(X = women_new_input_df, y = women_target_df)\ntransformed_pca = pd.DataFrame(transformed_new_input_df, columns = ['x_s', 'y_s', 'z_s'])",
        "text": "pca , projection to 3 dimension ( female )",
        "id": 289
    },
    {
        "code": "loadTitanicData = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"Titanic.csv\")\nTitanicData = loadTitanicData.drop(\"PassengerId\").drop(\"Name\").drop(\"Ticket\").drop(\"Cabin\").dropna(how=\"any\", subset=(\"Age\", \"Embarked\"))",
        "text": "read data in a a dataframe    -  source data be in csv format and include a header  .  we will ask spark to infer the schema/data type   -  drop unwanted column and row with null or invalid data  . ",
        "id": 290
    },
    {
        "code": "param_grid = {'penalty':['l1', 'l2'], \n              'C': [0.001, 0.01, 0.1, 1, 10, 100]}\nprint('Before feature selection:')\nLogRegModel = tuningPara_crossValid(LogisticRegression(), param_grid, X_train, X_test, y_train, y_test)",
        "text": "logistic regression , parameter tune   -   cross validation",
        "id": 291
    },
    {
        "code": "carbon_emissions = pd.read_csv('Total Carbon Dioxide Emissions-StateRankings.csv')\ncarbon_emissions.head()\njoined_emission = states.merge(carbon_emissions, left_on='STUSPS', right_on='State')\njoined_emission.head(2)\njoined_emission.plot(figsize= (20,20),column='Total Carbon Dioxide Emissions, million metric tons', cmap='Oranges', linewidth=0.25, edgecolor='white', legend=True, scheme='Quantiles')",
        "text": "make a map of carbon dioxide emission by state , with low emission be light orange   -   and high emission be orange  . ",
        "id": 292
    },
    {
        "code": "from sklearn import preprocessing\nxs = preprocessing.scale(df[\"NOX\"])\nys = preprocessing.scale(df[\"TAX\"])\nplt.scatter(xs, ys, color='r')\nplt.xlabel(\"NOX standardized\")\nplt.ylabel(\"TAX standardized\")\nplt.show()",
        "text": "a you can see , we do not change the shape of the data , just it scale  .  you can also use scikit   -   learn to standardize your data  . ",
        "id": 293
    },
    {
        "code": "\nl.pop(0)\nl\npopped_item = l.pop()\npopped_item",
        "text": "use   -  pop  -  to pop off an item from the list  .  by default pop take off the last index , but also can specify which index to pop off",
        "id": 294
    },
    {
        "code": "y, x = np.arange(10), np.arange(1, 11)\na * 5              \na + 5              \na + b              \na / b              \nnp.exp(a)          \nnp.power(a,b)      \nnp.sin(a)          \nnp.cos(a)          \nnp.arctan2(y, x)   \nnp.arcsin(x)       \nnp.radians(a)      \nnp.degrees(a)      \nnp.var(a)          \nnp.std(a, axis=0)  # standard deviation",
        "text": "element   -   wise operation and math function",
        "id": 295
    },
    {
        "code": "b = np.zeros((2,3,4))\nprint(b.shape)\nprint(b)",
        "text": "create an array of shape ( 2 , 3 , 4 ) of zero  . ",
        "id": 296
    },
    {
        "code": "\nfrom __future__ import print_function\nimport datacube\nimport xarray as xr\nfrom datacube.storage import masking\nimport json\nimport pandas as pd\nimport shapely\nfrom shapely.geometry import shape\nimport numpy as np \nimport fiona\nimport shapely.geometry\nimport rasterio.features\nimport rasterio\nfrom datacube.utils import geometry\nfrom datacube.helpers import ga_pq_fuser\nfrom datacube.storage.masking import mask_invalid_data\n\n\nfrom datacube.storage.storage import write_dataset_to_netcdf\n\nimport sys\nimport os.path\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\n\nimport warnings\n\n\nfrom PIL import Image\n\ndef eprint(*args, **kwargs):\n    print(*args, file=sys.stderr, **kwargs)",
        "text": "import the python module we need",
        "id": 297
    },
    {
        "code": "node3 = tf.add(node1, node2)\nprint(node3)\nprint(sess.run(node3))",
        "text": "build more complicate computation by combine tensor node with operation ( operation be also node )  . ",
        "id": 298
    },
    {
        "code": "N = 20\ng1 = mx.barabasi_albert_graph(N,2,seed=231)\ng2 = mx.barabasi_albert_graph(N,3,seed=231)",
        "text": "create two erd  o   -   renyi network with n node for each layer",
        "id": 299
    },
    {
        "code": "phi = 1.5\namps2 = amps * np.exp(1j * phi)\nys2 = synthesize2(amps2, freqs, ts)\nn = 500\nthinkplot.plot(ts[:n], ys.real[:n], label=r'$\\phi_0 = 0$')\nthinkplot.plot(ts[:n], ys2.real[:n], label=r'$\\phi_0 = 1.5$')\nthinkplot.config(ylim=[-1.15, 1.05], loc='lower right')",
        "text": "to see the effect of a complex amplitude , we can rotate the amplitude by 1 . 5 radian ,",
        "id": 300
    },
    {
        "code": "fig = plt.figure(figsize=(20,6), dpi=1600) \nalpha = alpha_scatterplot = 0.2 \nax1 = plt.subplot2grid((2,3),(0,0))\ntrain_df.Survived.value_counts().plot(kind='bar')\nplt.title(\"Distribution of Survival, (1 = Survived)\")\nsns.pairplot(train_df.drop(\"PassengerId\", axis=1), hue=\"Survived\", size=3)",
        "text": "let plot few graph of feature v survival rate    -  1 .  distribution of survival",
        "id": 301
    },
    {
        "code": "cluster_pred = kmeans_model.labels_\npred_summary = X.copy()\npred_summary[\"cluster\"] = cluster_pred  \npred_summary.head()",
        "text": "assign each observation to a cluster",
        "id": 302
    },
    {
        "code": "stated_reason['total_debt_consolidation'] = sum(stated_reason[0] + stated_reason[7])\nprint('The relevant total of debt consolidation as a loan title is {:.2%}'\n      .format(stated_reason['total_debt_consolidation']))\nprint('credit card: {0}'.format(stated_reason[2]))\nprint('Credit card refinancing: {0}'.format(stated_reason[13]))",
        "text": "tied the two meaningful debt consolidation number together  .  any remain similar loan title be in trace amount at best and shall be set aside   - ",
        "id": 303
    },
    {
        "code": "\ntrain['Sex_num'] = train.Sex.map({'female':0, 'male':1})\ntrain.loc[0:4, ['Sex', 'Sex_num']]",
        "text": "goal ,   -  map the exist value of a series to a different set of value   -  method ,   -  [   -  map  -  ] ( <url> ) ( series method )",
        "id": 304
    },
    {
        "code": "img = mpimg.imread('camera_cal/calibration1.jpg')\nundistorted = undistort(img)\nf, (ax1, ax2) = plt.subplots(1, 2,figsize=(24,9))\nf.tight_layout()\nax1.imshow(img)\nax1.set_title('Original Image', fontsize=20)\nax2.imshow(undistorted)\nax2.set_title('Undistorted Image', fontsize=20)\nplt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)",
        "text": "apply a distortion correction to raw image  . ",
        "id": 305
    },
    {
        "code": "print('Before feature selection:')\nparam_grid = {'n_estimators': [2, 3, 4, 5],\n              'max_depth':[2, 3, 4, 5]}\nRanForModel = tuningPara_crossValid(RandomForestClassifier(max_features=4, n_jobs=-1), param_grid, X_train, X_test, y_train, y_test)",
        "text": "random forest , parameter tune   -   cross validation",
        "id": 306
    },
    {
        "code": "model_4 = ConvNet(weight_scale=0.001, hidden_dim=[500], reg=0.001, filter_size=[3, 3, 3, 3], \n                  num_filters=[32, 32, 64, 64], pool_interval=2, use_batchnorm=True)\nsolver_4 = Solver(model_4, data,\n                num_epochs=1, batch_size=50,\n                update_rule='adam',\n                optim_config={\n                  'learning_rate': 1e-3,\n                },\n                verbose=True, print_every=70)\nsolver_4.train()",
        "text": "four conv layer , every two follow by a pool layer",
        "id": 307
    },
    {
        "code": "\nrange(0,100,10)\nl = []\ni = 0\nwhile i < 10:\n    i = i + 1\n    l.append(i*10)\nl\nl = []\nfor i in [1,2,3,4,5,6,7,8,9,10]:\n    (l).append(i*10)\nl\nmap(lambda x: x*10,range(10))",
        "text": "how many way can you generate the follow list ,",
        "id": 308
    },
    {
        "code": "\nfirst_name = input(\"What is your first name? \")\nlast_name = input(\"What is your last name? \")\nprint('Hello', first_name,last_name)\nprint ('Or should I say', last_name,\",\",first_name)",
        "text": "problem analysis input , first name , last name output , first name , last name and last name , first name algorithm ( step in program ) , enter first name , enter last name , print hello first name , last name , print or should i say last name , first name",
        "id": 309
    },
    {
        "code": "\nfrom sklearn.cluster import KMeans\ntic = time()\nkmeans = KMeans(6)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\ny_true = y\nfrom scipy.stats import mode\nlabels = np.zeros_like(y_true)\nfor i in range(6):\n    mask = (y_kmeans == i)\n    labels[mask] = mode(y_true[mask])[0]\nfrom sklearn.metrics import accuracy_score\nprint(\"accuracy score:\", accuracy_score(y_true, labels))\ntoc = time()\nprint('The total time is %s seconds ' % (toc-tic))",
        "text": "exercise 3 . 3 , set the number of cluster to 6 and apply kmeans cluster to the data  .  compute the accuracy score between the true label and the one estimate by the kmeans algorithm  . ",
        "id": 310
    },
    {
        "code": "graph_v = v.plot(chart=cart, mapping=Phi, chart_domain=spher, number_values=11, \n                 scale=0.2, label_axes=False)\nshow(graph_v, viewer=viewer3D)",
        "text": "a 3d view of the vector field $ v $ be obtain via the embed $ \\phi $ ,",
        "id": 311
    },
    {
        "code": "lr2.predict_proba(X_test_std[0,:].reshape(1, -1))\nlr2.predict_proba(X_test_std[21,:].reshape(1,-1))",
        "text": "predict the probability of a pattern in the test set  . ",
        "id": 312
    },
    {
        "code": "def get3Grams(payload_obj):\n    payload = str(payload_obj)\n    ngrams = []\n    for i in range(0,len(payload)-3):\n        ngrams.append(payload[i:i+3])\n    return ngrams\ntfidf_vectorizer_3grams = TfidfVectorizer(tokenizer=get3Grams)\ncount_vectorizer_3grams = CountVectorizer(min_df=1, tokenizer=get3Grams)",
        "text": "gram feature create a countvectorizer and tf   -   idfvectorizer that us 3   -   gram",
        "id": 313
    },
    {
        "code": "x = np.arange(stats.binom.ppf(0.001, n, p),\n              stats.binom.ppf(0.999, n, p))\nprint(x)",
        "text": "discrete x value that cover most of the binomial cdf use the percentile point function",
        "id": 314
    },
    {
        "code": "mydict ={k:v for (k,v) in zip(range(5), range(5))}\nmydict",
        "text": "the key do not have to be string  .  from python 2 . 7 you can use dictionary comprehension a well",
        "id": 315
    },
    {
        "code": "def plot_weights():\n    w = session.run(weights)\n    \n    \n    w_min = np.min(w)\n    w_max = np.max(w)\n    \n    fig, axes = plt.subplots(3, 4)\n    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n    for i, ax in enumerate(axes.flat):\n        \n        if i<10:\n            \n            \n            image = w[:, i].reshape(img_shape)\n            \n            ax.set_xlabel(\"Weights: {0}\".format(i))\n\n            \n            ax.imshow(image, vmin=w_min, vmax=w_max, cmap='seismic')\n\n        \n        ax.set_xticks([])\n        ax.set_yticks([])",
        "text": "helper function to plot model weight",
        "id": 316
    },
    {
        "code": "\nlayer_1 = tf.add(tf.matmul(x, weights['hidden_layer_1']),biases['hidden_layer_1'])\nlayer_1 = tf.nn.relu(layer_1)\nlayer_2 = tf.add(tf.matmul(layer_1, weights['hidden_layer_2']),biases['hidden_layer_2'])\nlayer_2 = tf.nn.relu(layer_2)\nops = tf.add(tf.matmul(layer_2, weights['out']), biases['out'])",
        "text": "hide and output layer definition ( use tensorflow mathematical function )",
        "id": 317
    },
    {
        "code": "\nresult = em.select_matcher([dt, rf, svm, nb, lg, ln], table=feature_vectors_dev, \n        exclude_attrs=['_id', 'ltable.id', 'rtable.id', 'gold'],\n        k=10,\n        target_attr='gold', \n        metric_to_select_matcher='precision',\n        random_state=0)\nresult['cv_stats']\nresult['drill_down_cv_stats']['precision']",
        "text": "select the best matcher use cross   -   validation now , we select the best matcher use k   -   fold cross   -   validation  .  for the purpose of this guide , we use five fold cross validation and use precision  metric to select the best matcher  . ",
        "id": 318
    },
    {
        "code": "fig, ax = plt.subplots(figsize=(12,8))\nfig = sm.graphics.plot_partregress(\"prestige\", \"income\", [\"education\"], data=prestige, ax=ax)",
        "text": "this plot be show how much effect income ha on prestige when the effect of education be remove  . ",
        "id": 319
    },
    {
        "code": "\ndf_article_internal = pd.DataFrame.copy(df_article)\ntotal_words_original_article = df_article['sentence'].map(len).sum()\ntotal_words_internal_article = df_article_internal['sentence'].map(len).sum()\nprint('total_words_original_article : ', total_words_original_article)\nprint('total_words_internal_article : ', total_words_internal_article)",
        "text": "extract result base on user parameter , max number of word % of original number of word , etc",
        "id": 320
    },
    {
        "code": "import threading\ndef worker(num):\n    print('Worker: %s' % num)\nthreads = []\nfor i in range(5):\n    t = threading.Thread(target=worker, args=(i,))\n    threads.append(t)\n    t.start()",
        "text": "it be useful to be able to spawn a thread and pas it argument to tell it what work to do  .  any type of object can be pass a argument to the therad  .  this example pass a number , which the thread then print  . ",
        "id": 321
    },
    {
        "code": "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 5))\nsns.distplot(restaurant_df.stars,kde=False,color = 'g',ax =ax,bins=20);\nax.axvline(restaurant_df.stars.mean(), 0, 1, color='r', label='Mean')\nax.legend();\nax.set_ylabel('Count',size=20)\nax.set_xlabel('Stars',size=20)\nax.set_title('Distribution(count) of Restaurant rating',size=20);",
        "text": "distribution count of restaurant rat we can see below more restaurant get 4 rat than other rat",
        "id": 322
    },
    {
        "code": "classifier_NB = MultinomialNB().fit(train_x, train_y)\ny_predict_nb = classifier_NB.predict(test_x)\ny_predict_nb_prob = classifier_NB.predict_proba(test_x)\nconfusion_matrix(test_y, y_predict_nb)\nprint(\"Accuracy : \", accuracy_score(test_y, y_predict_nb))\nprint(classification_report(test_y, y_predict_nb, target_names=classes))",
        "text": "naive bay classifier ( one v one )",
        "id": 323
    },
    {
        "code": "a = np.arange(50)\na = a[::-1]\na",
        "text": "reverse a vector ( first element become last ) (    -  )",
        "id": 324
    },
    {
        "code": "cited_patents_level.head()\ncited_design = cited_patents_level.groupby('patent_number')['cited_patent_number'].apply(lambda x: np.count_nonzero(x.str.contains('D\\d{6}'))).reset_index().rename(index=str, columns={'cited_patent_number':'num_design_cited'})\ncited_design.head()\nmaster = pd.merge(master, cited_design, how='left', on='patent_number')\nmaster.head()",
        "text": "number of cite design patent",
        "id": 325
    },
    {
        "code": "\ndata = np.loadtxt('../data/data1.txt', delimiter = ',')\nX = data[:, 0, np.newaxis]  \nY = data[:, 1, np.newaxis]  \nfigureId = 1\ndef plotData(X, Y):\n    global figureId\n    plt.figure(figureId)\n    plt.figure()\n    plt.grid()\n    plt.xlabel('Population of City in 10,000s')\n    plt.ylabel('Profit in $10,000s')\n    plt.plot(X, Y, 'r+')\n    plt.draw()\n    figureId += 1\nplotData(X, Y)\nplt.show()",
        "text": "load and visualize data",
        "id": 326
    },
    {
        "code": "train[train['Embarked'].isnull()]\ntrain['Embarked'].fillna('S')\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\nprint (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())",
        "text": "fare    -  fare also ha some miss value and we will replace it with the median  .  then we categorize it into 4 range  . ",
        "id": 327
    },
    {
        "code": "\ndisplay_all(df.head().transpose())\ncols = df.select_dtypes(exclude=[np.number])\nlist(cols)\ndummy_df = pd.get_dummies(df, columns = ['basin',\n                                         'region',\n                                         'public_meeting',\n                                         'scheme_management',\n                                         'permit',\n                                         'extraction_type',\n                                         'extraction_type_group',\n                                         'extraction_type_class',\n                                         'management',\n                                         'management_group',\n                                         'payment',\n                                         'payment_type',\n                                         'water_quality',\n                                         'quality_group',\n                                         'quantity',\n                                         'quantity_group',\n                                         'source',\n                                         'source_type',\n                                         'source_class',\n                                         'waterpoint_type',\n                                         'waterpoint_type_group',\n                                         'date_recordedIs_month_end',\n                                         'date_recordedIs_month_start',\n                                         'date_recordedIs_quarter_end',\n                                         'date_recordedIs_quarter_start',\n                                         'date_recordedIs_year_end',\n                                         'date_recordedIs_year_start'])\n\ndummy_df.columns\ndummy_df.shape\ndummy_df.to_csv('./Datasets/clean_test.csv', index=False)",
        "text": "all nan and miss value be now go  . ",
        "id": 328
    },
    {
        "code": "sentence = 'This is a sentence; please slice it.'\nprint(sentence[0:4])\nprint(sentence[5:7])\nprint(sentence[8])\nprint(sentence[10:18])\nprint(sentence[20:26])\nprint(sentence[27:32])\nprint(sentence[33:35])",
        "text": "use slice to extract each word from",
        "id": 329
    },
    {
        "code": "\nname = 'sigmoid_init0'\nmodel = Sequential()\nmodel.add(Dense(500, batch_input_shape=(None, 784), init='zero'))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(50,init='zero'))\nmodel.add(Activation('sigmoid'))\nmodel.add(Dense(10, activation='softmax',init='zero'))\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['accuracy'])\nmodel.summary()",
        "text": "suggestion for the experiment let the experiment run for 100 epoch  .  you might need to restart the kernel so that name of the layer be the same * with init zero * with sigmoid activation * with relu activation * with dropout ( p=0 . 3 ) * with batch   -   normalization and dropout",
        "id": 330
    },
    {
        "code": "\nmask = geometry_mask([geom], Studysite_rain.geobox, all_touched=True,invert=True)\nStudysite_rain_masked = Studysite_rain.where(mask)\nplt.figure(figsize=(4,3))\nfig = Studysite_rain_masked.rainfall.isel(time = [55]).plot()\nfig.set_cmap('viridis_r')\nplt.show()\nmonth_sum = Studysite_rain_masked.resample('MS', dim='time', how='sum', keep_attrs=True)\nprint(month_sum)",
        "text": "resample to monthly value , use the mean the totally yearly rainfall ( or sum over time ) would seem to make sense , but our first and last year be incomplete , and be thus not able to be compare to all the others  .  it 's nice to not lose the data from them by use mean , rather than exclude them  . ",
        "id": 331
    },
    {
        "code": "new_user_ratings_ids = map(lambda r: r[1], new_user_ratings)\nnew_user_unrated_movies_RDD = (full_movies.filter(lambda x: x[0] not in new_user_ratings_ids)\n                               .map(lambda x: (new_user_Id, x[0])))\nnew_user_recommendations_RDD = new_model.predictAll(new_user_unrated_movies_RDD)\nnew_user_recommendations_RDD.take(3)",
        "text": "get recommendation now , it 's time to get some recommendation  .  we will get an rdd with all the movie the new user ha n't rat yet  . ",
        "id": 332
    },
    {
        "code": "rec = oo[oo.Edition >=1984]\nrec.head()\nrec.NOC.value_counts().head(3)",
        "text": "which three country have win the most medal in recent year ( from 1984 to 2008 ) ?",
        "id": 333
    },
    {
        "code": "layer_conv1, weights_conv1 = \\\n    new_conv_layer(input=x_image,\n                   num_input_channels=num_channels,\n                   filter_size=filter_size1,\n                   num_filters=num_filters1,\n                   use_pooling=True)\nlayer_conv1",
        "text": "convolutional layer 1",
        "id": 334
    },
    {
        "code": "data = [{'a': i, 'b': 2 * i}\n        for i in range(3)]\nprint(data)\npd.DataFrame(data)",
        "text": "$ \\omega $ construct dataframe from a list of dicts any list of dictionary can be make into a dataframe  .  we ll use a simple list comprehension to create some data ,",
        "id": 335
    },
    {
        "code": "class_count.plot.pie(sort_columns=True,\n                     colors=class_colors,\n                     fontsize=f_size,                        \n                     autopct='%.2f',\n                     title='Titanic Class Ratio');",
        "text": "display percentage of passenger in each class",
        "id": 336
    },
    {
        "code": "numbers = range(1, 11)\nsq_numbers = [num**2 for num in numbers]\nprint(sq_numbers)",
        "text": "square of a range of number",
        "id": 337
    },
    {
        "code": "def sevenfive(start, end):\n    numbers = []\n    for i in range(start, end + 1):\n        \n        if i % 7 == 0 and i % 5 != 0:\n            \n            numbers.append(str(i))\n    return numbers\n', '.join(sevenfive(2000, 3200))",
        "text": "write a program which will find all such number which be divisible by 7 but be not a multiple of 5 , between 2000 and 3200 ( both include )  .  the number obtain should be print in a comma   -   separate sequence on a single line  .  hint , consider use range (   -   begin ,   -   end ) method",
        "id": 338
    },
    {
        "code": "def f(t):\n    return np.exp(-t)*np.cos(2*np.pi*t)\nt1 = np.arange(0.0, 5.0, 0.1)\nt2 = np.arange(0.0, 5.0, 0.02)\nplt.figure(1)\nplt.subplot(211)\nplt.plot(t1, f(t1), 'bo', t2, f(t2), 'k')\nplt.subplot(212)\nplt.plot(t2, np.cos(2*np.pi*t2), 'r--')\nplt.show()",
        "text": "work with multiple figure and ax",
        "id": 339
    },
    {
        "code": "pitching_last_decade = pitching_last_decade.drop(['stint','cg','sho','sv','ipouts','h','er','hr','bb','so',\n                           'baopp','ibb','wp','hbp','bk','bfp','gf','sh','sf','g_idp','r','gs','w','l','g','league_id'],axis=1)\npitching_last_decade.head()",
        "text": "the above dataframe contain the pitch statistic for mlb player during the regular season from the year 2000   -   2015 .  we be go to drop column that have no relevance to the measurement of the team 's overall success in a season  .  this result in the dataframe below  . ",
        "id": 340
    },
    {
        "code": "df = read_from_csv('data/credit-data.csv', index='PersonID')\ndf.head()",
        "text": "read data from csv file",
        "id": 341
    },
    {
        "code": "ffinal = pd.merge(p_stats, finals[[\"season\", \"PLAYER_ID\", 'pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l', \n                                   'defenders', 'facilitator', 'game_winners', 'inside_gamers', 'normal', 'pure_scorers', 'useless']], \n                  on=['PLAYER_ID', \"season\"], how='left')\nffinal[['pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l']] = ffinal[['pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l']].fillna(0)\nffinal['season'] = ffinal['season'] + 1\nffinal.groupby(['GAME_ID', \"TEAM_CITY\", 'TEAM_ID'])['pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l', \n                                                    'defenders', 'facilitator', 'game_winners', \n                                                    'inside_gamers', 'normal', 'pure_scorers', 'useless'].sum().reset_index().head()",
        "text": "link back player stats to each game",
        "id": 342
    },
    {
        "code": "dta = pd.read_csv('../data/ready_for_logistic_clean_1.csv')\ndiv_percentage = 0.80\ntrain_test_boundary = math.floor(div_percentage*len(dta))\nend_boundary = len(dta)\ntrain_data = dta[:train_test_boundary]\ntest_data = dta[train_test_boundary:]\nfull_fl = csv.reader(open(\"../data/ready_for_logistic_clean.csv\",\"r\"))\nheaders = full_fl.__next__()\ndata = train_data[headers]\ntest_data = test_data[headers]\ndata['intercept'] = 1.0\ntest_data['intercept'] = 1.0\nheaders_1 = headers\nheaders_1.remove('class')\nheaders_1.append('intercept')",
        "text": "get the data ready for logistic regression",
        "id": 343
    },
    {
        "code": "\nprint(re.findall('.', example))\nre.findall('m.', example)\nprint(re.findall('\\w+', example))\nprint(re.findall('\\w{1,3}', example))\nre.findall('m\\w+\\s', example)\nre.findall('m[u,a]\\w+\\S', example)\nre.findall(' [c,d]\\w+ ',example)",
        "text": "re special character we can search for string , but the power of regular expression come in it special character  .  special character indicate a type of character to match  .  for example ,",
        "id": 344
    },
    {
        "code": "\nidx_PL = valuenames[valuenames['country'] == 'PL'].index\nvalue_dict_PL = valuenames.loc[idx_PL].set_index('original_name')['opsd_name'].to_dict()\nvalue_dict_PL\nPL_re_df.technology.replace(value_dict_PL, inplace=True)",
        "text": "translate value and harmonize energy source level",
        "id": 345
    },
    {
        "code": "\nclf = LogisticRegression()\nscores = cross_val_score(clf, X_train, y_train, cv=5)\nprint(scores)",
        "text": "fold cross validation this estimate the accuracy of the model by split the data , fit a model and compute the score 5 consecutive time  .  the result be a list of the score from each consecutive run  . ",
        "id": 346
    },
    {
        "code": "\nx = 2\ndef f():\n    x = 3\n    return x\nprint(x)      \nprint(f())    \nx = 5\ndef f():\n    y = 2*x        \n    return y\nprint(f())         \nimport builtins\ndir(builtins)",
        "text": "scope what we need to know about scope , * global , define main body in script * local , define in a function * build in scope , name in predefined build in scope module such a print , len     let make some basic example",
        "id": 347
    },
    {
        "code": "import tensorflow as tf\nfrom tensorflow.python.ops import rnn, rnn_cell\nimport numpy as np\ntf.reset_default_graph()\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)",
        "text": "example a recurrent neural network ( lstm ) implementation example use tensorflow library  .  this example be use the mnist database of handwritten digit ( <url> / ) long short term memory paper , <url> author , aymeric damien project , <url> /",
        "id": 348
    },
    {
        "code": "sns.countplot(x=df[\"Reason\"], data=df)",
        "text": "use seaborn to create a countplot of 911 call by reason   - ",
        "id": 349
    },
    {
        "code": "\nstartTime = datetime.datetime(2015, 1, 1)\nendTime = datetime.datetime(2017, 12, 31)\ncollection = ee.ImageCollection('VITO/PROBAV/C1/S1_TOC_100M').filterDate(startTime, endTime)\npoint = {'type':'Point', 'coordinates':[6.134136, 49.612485]};",
        "text": "load proba   -   v image collection and point geometry select location of point be from the proba   -   v footprint x18y02 in luxembourg , europe  . ",
        "id": 350
    },
    {
        "code": "\ncol_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\nprint(url)",
        "text": "task 1 read the iris data into a panda dataframe , include column name  .  name the dataframe iris  . ",
        "id": 351
    },
    {
        "code": "merged_dataset2_2 = pd.merge(GT16, GT21, on='2012 Oak Creek shooting: (Worldwide)', \n                           left_index=True, right_index=True, how='outer')\nmerged_dataset2_2.head()\nmerged_dataset2_2.max()\nax_merge2_2 = merged_dataset2_2.plot()\nax_merge2_2.legend(bbox_to_anchor=(1.05, 1), loc=2)",
        "text": "gt16 and gt21 can be merge directly  . ",
        "id": 352
    },
    {
        "code": "def lexical_sentiment(doc, sid=None):\n    if sid is None: sid = SentimentIntensityAnalyzer()\n    if sid.polarity_scores(doc)['compound'] > 0:\n        label='pos'\n    else:\n        label='neg'\n    return label\nfor doc in testing_docs:\n    doc = \" \".join(doc[0])\n    label = lexical_sentiment(doc, sid)\n    print(doc[:100] + \"...\", label)",
        "text": "compare two approach first we can transform the sentiment score by the lexical approach into label by the follow rule , + positive sentiment , compound score > 0 + negative sentiment , compound score   -  0",
        "id": 353
    },
    {
        "code": "image = cv2.imread('test_images/test3.jpg')\nundist = undistort(image, mtx, dist)\nhls = cvtHLS(image)\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,15))\nax1.imshow(cv2.cvtColor(undist, cv2.COLOR_BGR2RGB))\nax1.set_title('Original Image', fontsize=20)\nax1.axis('off')\nb_binary = (applyBinaryThresh(selectChannel(lab, 0), 225, 255, False))\nax2.imshow(b_binary)\nax2.set_title('L-Channel Binary Image', fontsize=20)\nax2.axis('off')",
        "text": "select the b   -   channel of lab we can observe that the line be most clear in l   -   channel and hence i apply thresholding to isolate the l   -   channel an obtain a binary image of the same",
        "id": 354
    },
    {
        "code": "n = 'YamanAhlawat'\na.indices(len(n))\nfor i in range(*a.indices(len(n))):\n    print(n[i])\ny = slice(1, 10, 2)\nx = 'PythonWillSaveTheWorld'\ny.indices(len(x))\nfor i in range(*y.indices(len(x))):\n    print(x[i])",
        "text": "in addition , we can map a slice onto a sequence of a specific size by use it   index ( size )   method  .  this return a tuple ( start , stop , step ) where all value have be suitably limit to fit within bind ( a to avoid indexerror exception when index )  .  for ex  . ",
        "id": 355
    },
    {
        "code": "plt.figure(figsize=(8,5))\nplt.boxplot([heterogeneity.values(), heterogeneity_smart.values()], vert=False)\nplt.yticks([1, 2], ['k-means', 'k-means++'])\nplt.rcParams.update({'font.size': 16})\nplt.tight_layout()\nplt.xlabel('Heterogeneity')",
        "text": "let 's compare the set of cluster heterogeneity we get from our 7 restart of k   -   mean use random initialization compare to the 7 restart of k   -   mean use k   -   means++ a a smart initialization  .  the follow code produce a [ box plot ] ( <url> ) for each of these method , indicate the spread of value produce by each method  . ",
        "id": 356
    },
    {
        "code": "class Numbers:\n    def __init__(self, location):\n        import pickle, gzip\n        \n        f = gzip.open(location, 'rb')\n        train_set, valid_set, test_set = pickle.load(f)\n        f.close()\n        \n        self.train_x, self.train_y = train_set\n        self.test_x, self.test_y = valid_set\ndata = Numbers(\"../data/mnist.pklz\")\nprint('Total examples in training set:', len(data.train_x))\nprint('Total examples in the test set:',len(data.test_x))\nprint('Total pixels in each image:', len(data.train_x[0]))",
        "text": "the class below will load and store the mnist data  .  load the data and then report ,   -   the number of example in the train set   -   the number of example in the test set   -   the number of pixel in each image  . ",
        "id": 357
    },
    {
        "code": "tX_poly = build_poly(tX, degree_star)\nloss, w_star = ridge_regression(y, tX_poly, lambda_star)\nprint(\"Loss = %f\"%(loss))\nDATA_TEST_PATH = 'data/test_cleaned.csv' \n_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\ntX_test_poly = build_poly(tX_test, degree_star)\nOUTPUT_PATH = 'output/LS_RR_clean.csv' \ny_pred = predict_labels(w_star, tX_test_poly)\ncreate_csv_submission(ids_test, y_pred, OUTPUT_PATH)",
        "text": "generate prediction and save ouput in csv format for submission , we retrain on all the data  . ",
        "id": 358
    },
    {
        "code": "recent_grads.plot(x='Sample_size', y='Median', kind='scatter')\nrecent_grads.plot(x=\"Sample_size\", y='Unemployment_rate', \n                 kind='scatter')\nrecent_grads.plot(x='Full_time',y='Median', kind='scatter')\nrecent_grads.plot(x='ShareWomen',y='Unemployment_rate',kind='scatter' )\nrecent_grads.plot(x='Men',y='Median', kind='scatter')\nrecent_grads.plot(x='Women', y='Median', kind='scatter')",
        "text": "use panda for scatter plot",
        "id": 359
    },
    {
        "code": "D = np.random.uniform(0,1,100)\nS = np.random.randint(0,10,100)\nimport pandas as pd\nprint(D)\nprint(S)\nprint(pd.Series(D).groupby(S).mean())",
        "text": "consider a one   -   dimensional vector d , how to compute mean of subset of d use a vector s of same size describe subset index ?",
        "id": 360
    },
    {
        "code": "with model:\n    sin_probe = nengo.Probe(sin)\n    A_probe = nengo.Probe(A, synapse=0.01)  \n    A_spikes = nengo.Probe(A.neurons) # Collect the spikes",
        "text": "probe output anything that be probe will collect the data it produce over time , allow u to analyze and visualize it late  . ",
        "id": 361
    },
    {
        "code": "\nempty_set = set()\nlanguages = {'python', 'r', 'java'}\nsnakes = set(['cobra', 'viper', 'python'])",
        "text": "set    -  set property ,   -  unordered , iterable , mutable , can contain multiple data type   -   make of unique element ( string , number , or tuples )   -   like dictionary , but with key only ( no value )",
        "id": 362
    },
    {
        "code": "net = tn.Classifier(layers=[train[0].shape[1], (200, 'tanh'), (100, 'tanh'), (50, 'tanh'), 7])\nfor tr, val in net.itertrain(train, test,  algo='nag', learning_rate=1e-3, momentum=0.9, weight_l2=10):\n    print('training loss:', tr['loss'], tr['acc'])\n    print('most recent validation loss:', val['acc'])\npredictions = net.predict(test[0])\nprint('classification_report:\\n', classification_report(test[1], predictions))\nprint('accuracy: ', accuracy_score(test[1], predictions))",
        "text": "hide layer with 200 , 100 and 50 hide nodes  -  tanh  activation for hide and relu  for input and output layers  -  nesterov  s accelerate gradient with learn rate 1e   -   3 and momentum 0 . 9  -  l2 regularization with parameter value 10  - ",
        "id": 363
    },
    {
        "code": "fl = Dataset('../../swift.dkrz.de/COREII_data/fesom.1951.oce.mean.nc')\nfl.variables['u'].shape",
        "text": "load data for one year ,",
        "id": 364
    },
    {
        "code": "import requests\nfrom bs4 import BeautifulSoup\nurl = 'https://www.python.org/~guido/'\nr = requests.get(url)\nhtml_doc = r.text\nsoup = BeautifulSoup(html_doc)\npretty_soup = soup.prettify()\nprint (pretty_soup)\nurl = 'https://www.python.org/~guido/'\nr = requests.get(url)\nhtml_doc = r.text\nsoup = BeautifulSoup(html_doc)\nguido_title = soup.title\nprint (guido_title)\nguido_text = soup.get_text()\nprint(guido_text)\na_tags = soup.find_all('a')\nfor link in a_tags:\n    print (link.get('href'))",
        "text": "scrap the web in python",
        "id": 365
    },
    {
        "code": "sq = map(lambda x: x**2, range(10))\nprint(list(sq))\nlist_a = [1, 2, 3]\nlist_b = [10, 20, 30]\n  \nmap(lambda x, y: x + y, list_a, list_b)",
        "text": "use both map and lambda together",
        "id": 366
    },
    {
        "code": "range(0,10)\nx = range(0,10)\ntype (x)\nstart = 0\nstop = 20\nx = range (start, stop)\nx\nx = range (start,stop,2)\nx",
        "text": "range ( ) range ( ) allow u to create a list of number range from a start point up to an end point",
        "id": 367
    },
    {
        "code": "S1.set_loads(_Tx=0, _Ty=0, _Nz=0, _Mx=Mx, _My=0, _Mz=0)",
        "text": "set   -  loads  -  on the section ,   -  example 1  -  ,  _ bending moment _  in   -  x  -  direction",
        "id": 368
    },
    {
        "code": "g = sns.FacetGrid(yelp,col='stars')\ng.map(plt.hist,'text length')",
        "text": "use facetgrid from the seaborn library to create a grid of 5 histogram of text length base off of the star rat   - ",
        "id": 369
    },
    {
        "code": "\nall_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))",
        "text": "lotfrontage , since the area of each street connect to the house property most likely have a similar area to other house in it neighborhood , we can fill in miss value by the median lotfrontage of the neighborhood  . ",
        "id": 370
    },
    {
        "code": "\ndef obp(x):\n    num=x['h']+x['bb']+x['hbp']\n    den=x['ab']+x['bb']+x['hbp']+x['sf']+1e-6\n    return num/den\nbaseball.apply(obp, axis=1).round(3)",
        "text": "exercise 4 calculate   -  on base percentage  -  for each player , and return the order series of estimate   -  obp = \\frac { h + bb + hbp } { ab + bb + hbp + sf }   - ",
        "id": 371
    },
    {
        "code": "\n_,_,_,xgb_model = runXGB(X_train, y_train, X_test, y_test)\npred_y_test, loss,_,xgb_model = runXGBC(X_train.values, y_train, X_test.values, y_test,rounds=180)\npred_y_test, loss,_,rf_model = runRF(X_train.values, y_train, X_test.values, y_test,rounds=180)\nfeature_importance(model=rf_model, X=X_train)",
        "text": "model build   -   binary classification",
        "id": 372
    },
    {
        "code": "x = np.arange(0, 5, 0.1)\ndist = stats.expon(0)\nplt.plot(x, dist.pdf(x), lw = 2)\nplt.show()",
        "text": "let 's start by look at a typical statistical distribution , [ the exponential distribution ] ( <url> )  .  here be what it look like ( it go to $ \\infty $ so we just look at the front )  . ",
        "id": 373
    },
    {
        "code": "ts2 = xs-(xs**3)/math.factorial(3)\nplt.plot(xs,ys, color='red')\nplt.plot(xs,ts2)",
        "text": "plot the $ \\sin $ function and the taylor series approximation ( 2 term ) on the same axis",
        "id": 374
    },
    {
        "code": "pp(\n    n_different_users(choose_color_assignment, n=150)\n)",
        "text": "different user _ ids be randomly assign   - ",
        "id": 375
    },
    {
        "code": "\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\ny_test = y_test.flatten()",
        "text": "create train and test split",
        "id": 376
    },
    {
        "code": "import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nops.reset_default_graph()\nsess = tf.Session()",
        "text": "layer nest operation    -  we start by load the necessary library and reset the computational graph",
        "id": 377
    },
    {
        "code": "negative_mse = grid_search.best_score_\nrmse = np.sqrt(-negative_mse)\nrmse",
        "text": "the best model achieve the follow score ( evaluate use 5   -   fold cross validation ) ,",
        "id": 378
    },
    {
        "code": "from sklearn.tree import DecisionTreeClassifier                                     \ndtr = model = DecisionTreeClassifier(max_leaf_nodes=4, random_state=0)      \ndtr.fit(X_train, y_train)                                                               \ndtr_prediction = dtr.predict(X_test)                                   \nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test,lr_prediction)                         \nprint('The accuracy of the Decision Tree is:',accuracy)\nprint(y_test[:5])                                               \nprint(dtr_prediction[:5])\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(dtr, X_train, y_train, cv=5)",
        "text": "< p style=  font   -   family , arial , font   -   size,1 . 75em , color , purple , font   -   style , bold  >   ( 2 ) decision tree classifier",
        "id": 379
    },
    {
        "code": "f = open('workfile.txt', 'r')\ns = f.read()\nprint(s)\nf.close()",
        "text": "to read from a file ,",
        "id": 380
    },
    {
        "code": "children = split_categorical(play_data, \"Outlook\", [{\"sunny\"}, {\"overcast\"}, {\"rain\"}])\ninfogain(play_data, children)",
        "text": "get the information gain between the root and the child node after split the entire dataset by  outlook  with it three possible value a partition ( just like in the exercise )  . ",
        "id": 381
    },
    {
        "code": "\nl.append(6)\nl\nl.count(2)",
        "text": "with ipython and jupyter notebook , see all available method use the tab key  .  method for a list be , * append * count * extend * insert * pop * remove * reverse * sort",
        "id": 382
    },
    {
        "code": "dfTRate = dfRate.groupby(['NOC', 'Summer', 'Winter'])[['Gold', 'Silver', 'Bronze', 'Total_Medals', 'Rating']].sum().reset_index()\ndfTRate = dfTRate.sort_values(by=['Summer', 'Winter', 'Rating'], ascending=False).reset_index()\ndfTRate = dfTRate.drop(dfTRate.columns[[0]], axis=1)",
        "text": "overall across all game now we can get the rat and rank of each noc across all the olympic game from 1960   -   > 2018 .  but we ll split it into the winter and summer game  . ",
        "id": 383
    },
    {
        "code": "newfig()\nplot(xs, ys, label='trajectory')\ndecorate(xlabel='x position (m)',\n         ylabel='y position (m)')\nsavefig('chap10-fig02.pdf')",
        "text": "another way to visualize the result be to plot y versus x .  the result be the trajectory of the ball through it plane of motion  . ",
        "id": 384
    },
    {
        "code": "ri_g4 = get_data('RI')\nnames = ['achievement level 3', 'achievement level 4']  \nnames_later = ['% level 3', '% level 4']\nri_g4 = None\n##BUG 2011 and 2012: Exception: Length mismatch (2 vs 1)",
        "text": "rhode island   us the new england common assessment program ( necap ) , <url>   -  level 3   -   proficient , level 4   -   proficient with distinction , <url>",
        "id": 385
    },
    {
        "code": "train.drop('Cabin',axis=1,inplace=True)\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')\ntrain.dropna(inplace=True)\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')",
        "text": "let 's drop the cabin column and the row in embark that be nan  . ",
        "id": 386
    },
    {
        "code": "namesRegex = re.compile(r'Agent \\w+')\nnamesRegex.sub('CENSORED', 'Agent Alice gave the secret documents to Agent Bob.')",
        "text": "substitute string with the sub ( ) method the sub ( ) method for regez object be pass with two argument  .  the first be the string to replace any match , the second be the string for the regular expression  .  the sub ( ) method return a string with the substitution apply  . ",
        "id": 387
    },
    {
        "code": "from sklearn.linear_model import Lasso\nLR = Lasso(alpha=1.0)\nLR = LR.fit(X_train, y_train)\ny_predict = LR.predict(X_test)\nprint(\"Training set score: {:.2f}\".format(LR.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(LR.score(X_test, y_test)))\nmse_test=mean_squared_error(y_test, y_predict)\nprint(\"Mean squared error: %.2f\" % mse_test)\nr2_test= r2_score(y_test, y_predict)\nprint('Variance score: %.2f' % r2_test)",
        "text": "lasso regression , the syntax    -  import the class contain the regression method    -  create an instance of the class    -  fit the instance on the data and then predict the expect value",
        "id": 388
    },
    {
        "code": "import os\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk import ngrams\nfrom collections import Counter\nimport string\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nstopwords.words('english')\nword_tokenize('This is just a test')\n\nprint('You are all set')",
        "text": "environment tester this notebook be intend to test all the dependency that be need for the rest of the notebook to run properly  .  if everything be test correctly you should see a  you be all set  message when run the code below information on how to install package be available at <url> /",
        "id": 389
    },
    {
        "code": "from sklearn.svm import SVC\ngrid_values = {'C': [110, 115, 120, 125, 150], 'gamma': [0.001, 0.006,  0.01, 0.02, 0.1]}\ngrid_clf_acc = GridSearchCV(SVC(), param_grid = grid_values, cv=10, scoring = 'accuracy')\ngrid_clf_acc.fit(X_train, y_train)\nprint('Mean score matrix: ', grid_clf_acc.cv_results_['mean_test_score'])\nprint('Grid best parameter (max. accuracy): ', grid_clf_acc.best_params_)\nprint('Grid best score (accuracy): ', grid_clf_acc.best_score_)",
        "text": "support vector classifier with cross validation ( cv=10 ) and hyperparameter optimization",
        "id": 390
    },
    {
        "code": "\nfolder2= str(r\"C:/Users/u67397/AnacondaProjects/aem/input_data/\")\ndat2= \"7.dat\"\nreference_conductivity2 = 0.1",
        "text": "user requirement , enter detail for the second dataset",
        "id": 391
    },
    {
        "code": "\nfor i, layer in enumerate(base_model.layers):\n    print(i, layer.name)\nfor layer in model.layers[:249]:\n    layer.trainable = False\nfor layer in model.layers[249:]:\n    layer.trainable = True\nopt = tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9)\nmodel.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory2 = model.fit_generator(datagen.flow(X_train, y_train),\n            steps_per_epoch = train_size // batch_size,\n            epochs = 30,\n            validation_data = (X_valid, y_valid),\n            callbacks=[tb_callback_ln])",
        "text": "fine tune of the last layer   -   we will freeze the bottom n layer and train the remain top layer  . ",
        "id": 392
    },
    {
        "code": "np.sum(np.nan_to_num(beh[0,:]-np.min(beh,axis=0)))\n_=plt.hist(np.nan_to_num(beh[0,:]-np.min(beh,axis=0)),100)\nplt.yscale('log')\nplt.xlim([0, 140])",
        "text": "diff between first and last point of loss history , all voxels",
        "id": 393
    },
    {
        "code": "plt.figure()\nfor i in range(n_gestures):\n    plt.subplot(n_gestures,2,i*2+1)    \n    path = g.gestures[i]    \n    plt.plot(path[:,0], '-C0')\n    plt.plot(path[:,1], '-C1')\n    plt.xlabel(\"Samples\")\n    plt.subplot(n_gestures,2,i*2+2)\n    plt.plot(path[:,0], path[:,1], 'C0')\n    plt.gca().invert_yaxis()\n    plt.axis(\"equal\")\n    plt.axis(\"off\")",
        "text": "timeseries view we can also see each gesture a a trajectory of two coordinate ( $ x , y $ coordinate ) over time  .  this be close to the way in which the match will work  . ",
        "id": 394
    },
    {
        "code": "for name in ('LUCAS', 'ADAM', 'HUGO', 'LÉO', 'RAPHAËL', 'ETHAN', 'NATHAN', 'LIAM', 'NOLAN', 'ENZO'):\n    plotname(name, 0)\nplt.xlim([1900,2015])\nplt.ylim(bottom = 0)\nplt.legend(loc='upper left')\nplt.xlabel('Year')\nplt.ylabel('Number of people')\nplt.title('\"New\" male names in Top 15 in 2015\\n')\nplt.show()",
        "text": "new  male name in top 15 in 2015    -  in term of absolute number of birth ,",
        "id": 395
    },
    {
        "code": "clf = learn.DNNLinearCombinedClassifier(\n                    linear_feature_columns=wide_columns,\n                    dnn_feature_columns=deep_columns,\n                    dnn_hidden_units=[100, 50])\nfit_save(clf, 'DNN with 2 hidden layers', compare_classifiers)",
        "text": "dnn with 2 hide layer",
        "id": 396
    },
    {
        "code": "r16 = re.compile(r\"\\S\")\nr17 = re.compile(r\"[^ \\t\\n\\r\\f\\v]\")\nresult16 = r16.findall(\"My work address is 729 Arapeen Drive, Salt Lake City, UT, 84108.\")\nresult17 = r17.findall(\"My work address is 729 Arapeen Drive, Salt Lake City, UT, 84108.\")\nprint (result16)\nprint (result17)\nprint (result16 == result17)",
        "text": "match any non   -   whitespace character , * this be equivalent to the class   -  [ ^ \\t\\n\\r\\f\\v ]   - ",
        "id": 397
    },
    {
        "code": "star_wars['Have you seen any of the 6 films in the Star Wars franchise?'].value_counts()\nstar_wars['Do you consider yourself to be a fan of the Star Wars film franchise?'].value_counts()\nboolean = {'Yes':True, 'No':False}\nstar_wars['Have you seen any of the 6 films in the Star Wars franchise?'] = star_wars['Have you seen any of the 6 films in the Star Wars franchise?'].map(boolean)\nstar_wars['Do you consider yourself to be a fan of the Star Wars film franchise?'] = star_wars['Do you consider yourself to be a fan of the Star Wars film franchise?'].map(boolean)\nstar_wars.head()",
        "text": "clean and map yes/no column",
        "id": 398
    },
    {
        "code": "\nmyTree = DecisionTreeClassifier()\nscores = cross_validation.cross_val_score(myTree, X, y, cv=5)\nprint(\"Scores:\", scores)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))",
        "text": "train and evaluate decision tree model",
        "id": 399
    },
    {
        "code": "lens.set_index('movie_id', inplace=True)\nby_age = lens.loc[most_50.index].groupby(['title', 'age_group'])\nby_age.rating.mean().head(15)",
        "text": "young user seem a bite more critical than other age group  .  let 's look at how the 50 most rat movie be view across each age group  .  we can use the   -  most _ 50  -  series we create early for filter  . ",
        "id": 400
    },
    {
        "code": "result_close = seasonal_decompose(btc_price_close['Close'], model='multiplicative', freq=100)\nresult_close.plot();",
        "text": "view seasonal decomposition for close price",
        "id": 401
    },
    {
        "code": "chosenM2_M3true = modelChoiceValidation.loc[(modelChoiceValidation['trueModel'] == 2) & (modelChoiceValidation['model2_BayesFactor'] > modelChoiceValidation['model3_BayesFactor']) & (modelChoiceValidation['chosenModel'] == 1)]\nchosenM2_M3true.head()",
        "text": "model 3 be the true model , but model 2 be choose a the best model with a large bay factor than model 3  . ",
        "id": 402
    },
    {
        "code": "import numpy as np\nimport math\nimport time\nimport mxnet as mx\nimport mxnet.ndarray as nd\nimport logging\nimport sys\nimport os\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)  \nnp.random.seed(777)\nmx.random.seed(777)",
        "text": "aws summit 2017   -   seoul , mxnet seq2seq example",
        "id": 403
    },
    {
        "code": "def distribution_plotter(column, data_set):\n    data = data_set[column]\n    sns.set(rc={\"figure.figsize\": (10, 7)})\n    sns.set(color_codes=True)\n    sns.set(style=\"white\", palette=\"muted\")\n    dist = sns.distplot(data, hist_kws={'alpha':0.2}, kde_kws={'linewidth':5})\n    dist.set_title('Distribution of ' + column + '\\n', fontsize=16)\ndistribution_plotter('volume_sold', sales_data)\ndistribution_plotter('2015_q1_sales', sales_data)\ndistribution_plotter('2015_margin', sales_data)\ndistribution_plotter('2016_q1_sales', sales_data)",
        "text": "plot the distribution we ve provide a plot function below call  distribution _ plotter ( )    .  it take two argument , the name of the column and the data associate with that column  .  in individual cell , plot the distribution for each of the four column  .  doe the data appear skew ? symmetrical ? if skew , what would be your hypothesis for why ?",
        "id": 404
    },
    {
        "code": "train.drop('Cabin',axis=1,inplace=True)\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')",
        "text": "let 's go ahead and drop the cabin column and the row in embark that be nan  . ",
        "id": 405
    },
    {
        "code": "\nPTruePositive = (PPositiveTrue * PTrue) / PPositive\n\"%.2f\" % (PTruePositive * 100) + '%'",
        "text": "probability a viewer who be in control group doe convert , or p ( convert|positive )   - ",
        "id": 406
    },
    {
        "code": "ustotaldata['co2diff'] = ustotaldata[ustotaldata.Source==\"Total\"][\"co2\"].diff(periods=1)\nustotaldata[ustotaldata.Source==\"Total\"].sort_values(by='co2diff').head()",
        "text": "identify the year with large year   -   over   -   year drop in co2 emission",
        "id": 407
    },
    {
        "code": "orders.eval_set.value_counts()\noprior=orders[orders.eval_set=='prior']\notrain=orders[orders.eval_set=='train']\notest=orders[orders.eval_set=='test']\noprior.head()\notrain.head()\notest.head()",
        "text": "order table be separate to three evaluation set   -   prior , train , test",
        "id": 408
    },
    {
        "code": "\nimport numpy as np\nimport csv\nimport keras as kr\niris = list(csv.reader(open('IRIS.csv')))[1:] \ninputs = np.array(iris)[:,:4].astype(np.float) \noutputs = np.array(iris)[:,4] \noutputs_vals, outputs_ints = np.unique(outputs, return_inverse=True)\n# output_ints contains the indices\n# The 1st string corresponds with the 1st interger in the array\n# species are represented as integers, with 0 denoting setosa, 1 denoting versicolor and 2 denoting virginica\n# Encoding the integers as binary categorical variables.\n# basically creating a binary matrix \n# E.g. if output_ints is 0 then its encoded into 1,0,0 or if 1 then its 0,1,0 or if 2 then its 0,0,1\noutputs_cats = kr.utils.to_categorical(outputs_ints)\n# This means that if the output is:\n# (1,0,0) = setosa\n# (0,1,0) = versicolor\n# (0,0,1) = virginica\n\n\n\n# A model is understood as a sequence or a graph of standalone, \n# ..fully-configurable modules that can be plugged together with as little restrictions as possible.\n# For more: https://keras.io/\n# Creating model and a neural network\n# model is used to organise layers\nmodel = kr.models.Sequential() # using sequential model which is a linear stack of layers\n\n# stacking 4 layers. \n# Add an initial layer with 4 input nodes and a hidden layer with 16 nodes/neurons.\n# Applying the sigmoid activation function to the layer. Range 0 to 1.\nmodel.add(kr.layers.Dense(16, input_shape=(4,), activation='sigmoid' ))\n\n\n# Adding another layer, connected to the layer with 16 nodes/neurons, containing 3 output nodes \n# Using the softmax activation function here. Softmax calculates the probabilities distribution. Range 0 to 1.\nmodel.add(kr.layers.Dense(3, activation='softmax'))",
        "text": "use tensorflow to create a model create a model to predict the specie of iris from a flower  s sepal width , sepal length , petal width , and petal length  . ",
        "id": 409
    },
    {
        "code": "def delayedLines(xmlroot):\n    SC_lines = Return_SC_Lines(xmlroot)\n    l=[]\n    for i in SC_lines:\n        if i[1]!='GOOD SERVICE':\n            l.append(i[0])\n    return l\ndelayed=delayedLines(root)",
        "text": "function to return list of line with delay or plan work",
        "id": 410
    },
    {
        "code": "mask = obj.isin(['b', 'c'])\nobj[mask]",
        "text": "isin  perform a vectorized set membership check and can be useful in filter a dataset down to a subset of value in a series or column in a dataframe ,",
        "id": 411
    },
    {
        "code": "result.explain(True)",
        "text": "execution plan again , let 's inspect the execution plan  . ",
        "id": 412
    },
    {
        "code": "opt = rpca.RobustPCA.Options({'Verbose': True, 'gEvalY': False,\n                              'MaxMainIter': 200, 'RelStopTol': 1e-3,\n                              'AutoRho': {'Enabled': True}})\nb = rpca.RobustPCA(S, None, opt)\nX, Y = b.solve()",
        "text": "set option for the robust pca solver , create the solver object , and solve , return the estimate of the low rank and sparse component  x  and  y   .  unlike most other sporco class for optimisation problem , [ rpca . robustpca ] ( <url> ) ha a meaningful default regularization parameter , a use here  . ",
        "id": 413
    },
    {
        "code": "data[['State', 'Allages']].groupby(['State'], as_index=False).mean()\nsns.barplot(x='State', y='Allages', data=data)\nplt.title('Average annual deaths based on State')\nplt.show()",
        "text": "plot the trend of average annual death across state",
        "id": 414
    },
    {
        "code": "outcomes99_downloaded = drive.CreateFile({'id': '1fGKcma3Nwk0ZxQ0XBYb9v7C19jrZgtal'})\noutcomes99_downloaded.GetContentFile('outcomes99.csv')\ndf_outcomes99 = pd.read_csv('outcomes99.csv')\ndf_outcomes99.head()",
        "text": "import outcomes99 dataset into panda dataframe",
        "id": 415
    },
    {
        "code": "photo_data = misc.imread('./wifire/sd-3layers.jpg')\nred_mask   = photo_data[:, : ,0] <150\ngreen_mask = photo_data[:, : ,1] >100\nblue_mask  = photo_data[:, : ,2] <100\nfinal_mask = np.logical_and(red_mask, green_mask, blue_mask)\nphoto_data[final_mask] = 0\nplt.figure(figsize=(15,15))\nplt.imshow(photo_data)",
        "text": "< p style=  font   -   family , arial , font   -   size,1 . 75em , color ,   -   2462c0 , font   -   style , bold  >   composite mask that take threshold on all three layer , red , green , blue",
        "id": 416
    },
    {
        "code": "ax2 = fig3.add_subplot(132, title='Farm Revenue Variability',\n                       xlabel='Target price', xticks=[0, 1, 2],\n                       ylabel='Standard deviation', yticks=[0, 0.2, 0.4])\nzeroline(Sr)\nax2.plot(ptarg, Sr, linewidth=4)",
        "text": "graph standard deviation of farm revenue v target price",
        "id": 417
    },
    {
        "code": "df1 = df.groupby(by=['Day of Week','Hour']).count()['Reason'].unstack()\ndf1.head()\nplt.figure(figsize=(12,6))\nsns.heatmap(df1)\nplt.title('Count of Calls in particular day and hour')\nsns.clustermap(df1)",
        "text": "each map represent the amount of the 911 call do every day in particular hour   - ",
        "id": 418
    },
    {
        "code": "\nresult['Happiness-diff'].max()\nresult[result['Happiness-diff'] == result['Happiness-diff'].max()]",
        "text": "country  s happiness rank increase the most",
        "id": 419
    },
    {
        "code": "def qd_analysis(data,printoutput,errorplots):\n    qda = sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis()\n    print('MODELS FOR QUADRATIC DISCRIMINANT ANALYSIS')\n    for pred in list(data):\n        if pred != 'Severity':\n            buildmodels(qda,data,pred,printoutput,errorplots)\n            \nqd_analysis(data,True,False)",
        "text": "task 3    -  10 point in this task , * use the quadratic discriminant alalysis ( lda ) to build the predictive model , * compute the parameter that will allow you to make comparison to other model , and * plot the confusion matrix  . ",
        "id": 420
    },
    {
        "code": "\nprint (dfoasx.CDR[(dfoasx.CDR == 0.0)].count(),\",\", dfoasx.CDR[(dfoasx.CDR == 0.5)].count())\nprint (dfoasx.CDR[(dfoasx.CDR == 1.0)].count(),\",\", dfoasx.CDR[(dfoasx.CDR == 2)].count())\nsum(pd.isnull(dfoasx['CDR']))",
        "text": "note categorical data for gender m/f , and label ( cdr ) value > =0 . 0 , and nan  .  cross   -   sectional mri dataset ha many row with nan value in multiple column  .  these nan value will be remove after merge the dataset with the longitudinal mri dataset  . ",
        "id": 421
    },
    {
        "code": "\ngray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\nface_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\nfaces = face_cascade.detectMultiScale(gray, 1.28, 6)\n \nkernel = np.ones((100,100), np.float32)/10000\nimage_copy = np.copy(image)\nfor (x,y,w,h) in faces:\n    image_copy[y:y+h,x:x+w,:] = cv2.filter2D(image_copy[y:y+h,x:x+w,:],-1,kernel)\n      \nplt.imshow(image_copy)",
        "text": "( implementation ) use blur to hide the identity of an individual in an image the idea here be to 1 ) automatically detect the face in this image , and then 2 ) blur it out  .  make sure to adjust the parameter of the *averaging* blur filter to completely obscure this person 's identity  . ",
        "id": 422
    },
    {
        "code": "import logging\nlogging.basicConfig(level = logging.DEBUG, format = '[%(levelname)s] (%(threadName)-10s) %(message)s',)\ndef worker():\n    logging.debug('Starting')\n    time.sleep(2)\n    logging.debug('Exiting')\ndef my_service():\n    logging.debug('Starting')\n    time.sleep(3)\n    logging.debug('Exiting')\nt = threading.Thread(name = 'my_service', target = my_service)\nw1 = threading.Thread(name = 'worker', target = worker)\nw2 = threading.Thread(target = worker)\nw1.start()\nw2.start()\nt.start()",
        "text": "most program do not use   -  print  -  to debug  .  the logging  module support embed the thread name in every log message use the formatter code  % ( threadname ) s   .  include thread name in log message make it easy to trace those message back to their source  . ",
        "id": 423
    },
    {
        "code": "print(\"Wait, bags of words are being created!!\\n\")\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000) \ntrain_data_features = vectorizer.fit_transform(data['processed'])\ntrain_data_features = train_data_features.toarray()\nvocab = vectorizer.get_feature_names()\ndist = np.sum(train_data_features, axis=0)\nex = pd.DataFrame(data = {'word': vocab, 'number of appearance': dist}).sort_values(by = 'number of appearance', \n                                                                                    ascending = False).head(40)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nax = sns.barplot( x=\"number of appearance\", y=\"word\", data=ex)",
        "text": "what if we use tf   -   idf instead of count vectorizer ? in tf   -   idf ( term frequency   -   inverse document frequency ) , we take into account both the frequency of the word in the text and the relative frequency within different text  . ",
        "id": 424
    },
    {
        "code": "c -= a\nc",
        "text": "subtract right from the leave and assign the result to leave ( c = a   -   c )",
        "id": 425
    },
    {
        "code": "army[(army.deaths > 500) | (army.deaths < 50)]",
        "text": "select row where df . deaths be great than 500 or le than 50",
        "id": 426
    },
    {
        "code": "orders.eval_set.value_counts()\noprior=orders[orders.eval_set=='prior']\notrain=orders[orders.eval_set=='train']\notest=orders[orders.eval_set=='test']",
        "text": "order table be separate to three evaluation set   -   prior , train , test",
        "id": 427
    },
    {
        "code": "\nstore1_data['Sales'].expanding(min_periods=1,freq='D').mean().plot()",
        "text": "expand function in addition to the set of rolling ( )  function , panda also provide a similar collection of expanding ( )  function , which , instead of use a window of n value , use all value up until that time  . ",
        "id": 428
    },
    {
        "code": "def normalize(problem, points):\n  \n  meta = problem.objectives\n  all_objs = []\n  for point in points:\n    objs = []\n    for i, o in enumerate(problem.evaluate(point)):\n      low, high = meta[i].low, meta[i].high\n      try:\n        objs.append((o-low)/(high - low))\n      except ZeroDivisionError:\n        objs.append(0)\n        \n    all_objs.append(objs)\n  return all_objs",
        "text": "to compute most measure , data ( i . e objective ) be normalize  .  normalization be scale the data between 0 and 1 .  why do we normalize ? todo2 , answer the above question answer , to compare different objective , we need to have them in same range  .  hence we normalize the objective  . ",
        "id": 429
    },
    {
        "code": "vect = TfidfVectorizer(min_df=7,max_df=0.90,ngram_range=(3,5),analyzer='char_wb').fit(X_train)\nX_train_transform = vect.transform(X_train)",
        "text": "build the feature use tf   -   idf vectorizer",
        "id": 430
    },
    {
        "code": "from sklearn.base import clone\nimport numpy as np\nforest = [clone(grid.best_estimator_) for _ in range(n_trees)]\naccuracy_scores = []\nfor tree, (X_mini_train, y_mini_train) in zip(forest, mini_sets):\n    tree.fit(X_mini_train, y_mini_train)\n    y_pred = tree.predict(X_test)\n    accuracy_scores.append(accuracy_score(y_test, y_pred))\nnp.mean(accuracy_scores)",
        "text": "train one decision tree on each subset , use the best hyperparameter value find above  .  evaluate these 1,000 decision tree on the test set  .  since they be train on small set , these decision tree will likely perform bad than the first decision tree , archieving only about 80 % accuracy  . ",
        "id": 431
    },
    {
        "code": "relevant_text = \"\"\nfor row in relevant_train_df.iterrows():\n    relevant_text += clean_text(row[1][text_index]) + \" \"\nwordcloud = WordCloud(max_font_size=40,\n                      relative_scaling=.5,\n                      max_words=max_words).generate(relevant_text)\nplt.figure(figsize=(14,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.savefig('relevantArticleWC.png')",
        "text": "generate a word cloud for relevant document text",
        "id": 432
    },
    {
        "code": "\ncheckNull = complaints.isnull().sum()\ncheckNull.sort()\nprint (checkNull)",
        "text": "be there miss value ? be there pattern in the miss value ?",
        "id": 433
    },
    {
        "code": "from IPython.display import HTML\nHTML(\"\"\"\n<video width=\"640\" height=\"360\" controls>\n  <source src=\"{0}\">\n</video>\n\"\"\".format('test_out.mp4'))",
        "text": "display video in jupyter notebook",
        "id": 434
    },
    {
        "code": "\ndef tri_area(w, h):\n    return .5 * w * h",
        "text": "write a function to calculate the area of a triangle uaing a height and width  .  test it out  . ",
        "id": 435
    },
    {
        "code": "add = tf.add(x,y)\nsub = tf.subtract(x,y)\nmul = tf.multiply(x,y)\ndiv = tf.divide(x,y)",
        "text": "use build   -   in operation with placeholder",
        "id": 436
    },
    {
        "code": "for tick in tickers:\n    returnsdf[tick+' Return'] = bank_stocks[tick]['Close'].pct_change()\nreturnsdf.head(10)",
        "text": "and then , we can use panda pct _ change ( ) metho on the close column to create a column represent this return value in a loop to set this for all bank   - ",
        "id": 437
    },
    {
        "code": "\nimport numpy as np\ndata=np.random.rand(100,100) \na.view(data)\na.zoomtofit()\na.get_viewer_info() \na.frame(2)\nfrom astropy.nddata import NDData\narray = np.random.random((12, 12, 12))  \nndd = NDData(array)\na.view(ndd.data[5])\na.zoom()\na.close() #disconnect and close DS9 window. This only works for DS9 process started from imexam",
        "text": "quick photometry can be pull with the  a  key and us the photutils package to do the work  .  more information on photutils can be find at <url> / you can also look at the example ds9 photometry notebook in the imexam repository which us photutils  . ",
        "id": 438
    },
    {
        "code": "import plot_binary as pb\nplt = pb.plot_binary(X_train,y_train,classifier,1)\nplt.title('Random Forest Classifier (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\nplt = pb.plot_binary(X_test,y_test,classifier,1)\nplt.title('Random Forest Classifier (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()",
        "text": "visualize the class boundry ( without feature scale )",
        "id": 439
    },
    {
        "code": "figure = plt.figure(figsize=(15,8))\nplt.hist([data[data['Survived']==1]['Age'],\n            data[data['Survived']==0]['Age']],\n            bins = 30,\n            stacked=True,\n            color = ['g','r'],\n            label = ['Survived','Dead'])\nplt.xlabel('Age')\nplt.ylabel('Number of passengers')\nplt.legend();",
        "text": "woman be more likely to survive , so the sex variable seem to be a decisive feature  . ",
        "id": 440
    },
    {
        "code": "\nif 1 < 2 :\n    print('yes')\nif 1 > 2:\n    print(1+2)\nif 1==2:\n    print('first')\nelse:\n    print('last')\nif 1==2:\n    print('first')\nelif 4==4:\n    print('second')\nelif 3==3:\n    print('Middle')\nelse:\n    print('last')\n#it's going to execute first condition whichever is true and ends",
        "text": "if , elif and else statement",
        "id": 441
    },
    {
        "code": "\nfrom sklearn.preprocessing import LabelBinarizer\nclass BooleanTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self,baseValue):\n        \n        self.baseValue = baseValue\n        \n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        return (X == self.baseValue)*1\npipeline4 = make_pipeline(ColumnSelector('Sex'),BooleanTransformer('female'))\npipeline4.fit_transform(titanic)",
        "text": "boolean column the sex  column only contain 2 value , male  and female   .  build a custom transformer that be initialise with one of the value and return a boolean column with value of true  when that value be find and false  otherwise  . ",
        "id": 442
    },
    {
        "code": "diabetes.groupby('frame')['hdl'].describe().unstack()\nfig, ax = plt.subplots(figsize = (8,6))\nsns.boxplot(x = \"frame\", y = \"hdl\", data = diabetes, order = [\"small\",\"medium\",\"large\"], ax=ax);\nsns.factorplot(x = \"frame\", y = \"hdl\", data = diabetes, kind=\"point\", size = 5, order = [\"small\",\"medium\",\"large\"]);\nsns.set_context('notebook', font_scale=2)\nfig = sns.FacetGrid(diabetes, hue = \"frame\", aspect = 3, size = 6)\nfig.map(sns.kdeplot, \"hdl\", shade = True)\nfig.add_legend();",
        "text": "one categorical vs .  a numerical variable",
        "id": 443
    },
    {
        "code": "list = []\nwith open('sat_scores.csv', 'rU') as csvfile:\n    reader = csv.reader(csvfile)\n    for row in reader:\n        list.append(row)",
        "text": "load data into a list of list",
        "id": 444
    },
    {
        "code": "m = 20\nmp_c1 = matrixProfile.stomp(signal_df_case01.signal.values, m)\nmp_adj_c1 = np.append(mp_c1[0],np.zeros(m-1)+np.nan)",
        "text": "calculate output ( matrix profile )",
        "id": 445
    },
    {
        "code": "def subTransposeMelody( melody_line, melody_tonic , final_key ):\n    current_tonic = mus.note.Note(melody_tonic)\n    final_tonic = mus.note.Note(final_key)\n   \n          \n    interval_wanted = mus.interval.notesToChromatic( current_tonic , final_tonic )\n    transposed_melody = melody_line.flat.transpose(interval_wanted.getDiatonic().directedName)\n    return(transposed_melody)",
        "text": "subtransposemelody ( melody _ line , melody _ tonic , final _ key ) * give melody in major key , transpose it to the final key",
        "id": 446
    },
    {
        "code": "for i in range(count):\n    command = \"nuttcp -t -i1 -sdz -xc \"+str(i)+\" -w 110M -P 5300\"+str(i)+\" -p 5310\"+str(i)+ \" \"+ nuttcp_server +\" < \"+ file_read_nuttcp[i] +\" & \"\n    print(command)\n    os.system(command)\n    command = \"nuttcp -t -i1 -sdz -xc \"+str(i)+\" -w 110M -P 5320\"+str(i)+\" -p 5330\"+str(i)+ \" \"+ nuttcp_server +\" < \"+ file2_read_nuttcp[i] +\" & \"\n    print(command)\n    os.system(command)\ndtn.run_monitor(timeout=None)",
        "text": "test network transfer by disk",
        "id": 447
    },
    {
        "code": "\ntraindemographics.head()\nunique_values(traindemographics)\ntraindemographics['bank_branch_clients'].fillna(value='Other', inplace=True)\ntraindemographics['employment_status_clients'].fillna(value='Other', inplace=True)\ntraindemographics['level_of_education_clients'].fillna(value='Other', inplace=True)\nassert traindemographics.isnull().sum().values.sum() == 0\ntitle_case(traindemographics)\ntraindemographics.info()",
        "text": "< a id=traindemographics  >      -  traindemographics . csv",
        "id": 448
    },
    {
        "code": "articles_similarity_score=cosine_similarity(article_tfidf_matrix, user_article_tfidf_vector)\nrecommended_articles_id = articles_similarity_score.flatten().argsort()[::-1]\nrecommended_articles_id\nfinal_recommended_articles_id = [article_id for article_id in recommended_articles_id \n                                 if article_id not in ARTICLES_READ ][:NUM_RECOMMENDED_ARTICLES]",
        "text": "calculate cosine similarity between user read article and unread article",
        "id": 449
    },
    {
        "code": "from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n     \ntrain_matrix = vectorizer.fit_transform(train_data['review_clean'])\ntest_matrix = vectorizer.transform(test_data['review_clean'])",
        "text": "build the word count vector for each review",
        "id": 450
    },
    {
        "code": "\ntitanic.drop(['ticket','boat','body','home.dest'], axis=1, inplace=True)\ntitanic.drop([1309], axis=0, inplace=True)\ntitanic.info()",
        "text": "get rid of column ( variable , feature ) that be not useful and row that contain no data  . ",
        "id": 451
    },
    {
        "code": "\nprint(re.findall('.', example))\nre.findall('m.', example)\nprint(re.findall('\\w+', example))\nprint(re.findall('\\w{1,3}', example))\nre.findall('m\\w+\\s', example)\nre.findall('m[u,a]\\w+\\S', example)\nre.findall(' [c,d]\\w+',example)",
        "text": "re special character we can search for string , but the power of regular expression come in it special character  .  special character indicate a type of character to match  .  for example ,",
        "id": 452
    },
    {
        "code": "\ncity_type = [\"Rural\", \"Suburban\", \"Urban\"]\nrides = [125, 657, 1625]\ncolor = [\"gold\", \"lightskyblue\", \"lightcoral\"]\nexplode = (0, 0, 0.1)\nplt.pie(rides, explode=explode, labels=city_type, colors=color, autopct='%1.1f%%',shadow=True, startangle=100)\nplt.title(\"% of Total Rides by City Type\")\nplt.axis('equal')\nplt.show()",
        "text": "of total ride by city type",
        "id": 453
    },
    {
        "code": "array = np.random.randint(10,20, (10))\nprint(array)\narray = array[::-1]\nprint(array)\narray = list(array)\narray.reverse()\nprint(np.array(array))",
        "text": "reverse a vector ( first element become last )",
        "id": 454
    },
    {
        "code": "cities=set([\"Frankfurt\",\"Basel\",\"Freiburg\"])\ncities.add(\"Strasbourg\")\ncities.add(1)\ncities\ncities.remove(1)\ncities",
        "text": "freeze set though set ca n't contain mutable object , set be mutable ,",
        "id": 455
    },
    {
        "code": "plt.plot(xrange(epochs), train_on_soft_evaluate_on_soft, label='TS-ES')\nplt.plot(xrange(epochs), train_on_hard_evaluate_on_soft, label='TH-ES')\nplt.title(\"Train on soft/hard and Evaluate on Soft\")\nyy, locs = plt.yticks()\nll = ['%.6f' % a for a in yy]\nplt.yticks(yy, ll)\nxx, locs = plt.xticks()\nll = ['%.0f' % a for a in xx]\nplt.legend()\nplt.show()",
        "text": "eh = train on soft , evaluate on hard th   -   eh = train on hard , evaluate on hard",
        "id": 456
    },
    {
        "code": "s = 'acdfd'\nss = 'bbjdfkjbbbb'\nprint(re.search(r'.b{3}', s))\nprint(re.search(r'.b{3}', ss))\nprint(re.search(r'.b{3}[^b]', ss))\nprint(re.search(r'.b{3,}', ss))",
        "text": "match a string that ha an a follow by three  b   . ",
        "id": 457
    },
    {
        "code": "from bs4 import BeautifulSoup\nfrom urllib.request import urlopen\nraw_html = urlopen(\"http://floatingmedia.com/columbia/topfivelists.html\").read()",
        "text": "import the urllib and beautifulsoup library and make http request",
        "id": 458
    },
    {
        "code": "\nloss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n                                     biases=nce_biases,\n                                     labels=y_target,\n                                     inputs=embed,\n                                     num_sampled=num_sampled,\n                                     num_classes=vocabulary_size))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\nnorm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\nnormalized_embeddings = embeddings / norm\nvalid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\nsimilarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\ninit = tf.global_variables_initializer()\nsess.run(init)",
        "text": "here be our loss function , optimizer , cosine similarity , and initialization of the model variable  .  for the loss function we will minimize the average of the nce loss ( noise   -   contrastive estimation )  . ",
        "id": 459
    },
    {
        "code": "river_entero['Date'] = Series([to_datetime(d) for d in river_entero['Date']])",
        "text": "uh oh , python be not recognize date and enterocount a numeric field  .  let 's fix that",
        "id": 460
    },
    {
        "code": "\nsess.run(tf.global_variables_initializer())\nfor step in range(2000+1) :\n    sess.run(train)\n    if step % 200 == 0:\n        print(step, sess.run(cost), sess.run(W), sess.run(b))\nprint(sess.run(hypothesis))",
        "text": "run/update graph and get result",
        "id": 461
    },
    {
        "code": "class Memoize: \n    def __init__(self, f):\n        self.f = f\n        self.memo = {}\n    def __call__(self, *args):\n        if not args in self.memo:\n            self.memo[args] = self.f(*args)\n        return self.memo[args]\ndef factorial(k):\n    if k < 2: \n        return 1\n    return k * factorial(k - 1)\nfactorial = Memoize(factorial)\nfactorial(5)",
        "text": "encapsulate the memoization process into a class",
        "id": 462
    },
    {
        "code": "for lisr in lisrgc:\n    gtdrndic.update({'title': lisr.title})\n    lisauth.append(str(lisr.author))\n    for osliz in os.listdir(metzdays):\n        with open(str(lisr.author) + '.meta', \"w\") as f:\n            rstrin = lisr.title.encode('ascii', 'ignore').decode('ascii')\n            \n            \n            \n            \n            \n            \n            \n            f.write(rstrin)\n#matdict",
        "text": "need to save json object  .  dict be create but it isnt save  .  loop through lisrgc twice , should only require the one loop  .  cycle through lisr and append to dict/concert to json , and also cycle through lisr . author meta folder save the json that wa create  . ",
        "id": 463
    },
    {
        "code": "subs2_clusters = subs2[np.ix_(subs2_df.index, subs2_df.index)]\nsubs2_clusters_labels = [str(x) for x in subs2_df.hashval]\n_ = sourmash_lib.fig.plot_composite_matrix(subs2_clusters, subs2_clusters_labels, vmax=0.2)",
        "text": "example , select only the cluster out of the original matrix",
        "id": 464
    },
    {
        "code": "class CallMe:\n    def __call__(self):\n        print(\"Called\")\n        \ncall_me=CallMe()\ncallable(call_me)\ncallable(\"This is not callable\")",
        "text": "instance object can be callable use the dunder   -   call method  . ",
        "id": 465
    },
    {
        "code": "\nrange(0,100,10)\nm = []\nfor i in [0,1,2,3,4,5,6,7,8,9]:\n    m.append(i*10)\nm\nx = []\nfor l in range(1,10):\n    x.append(l * 10)\nx\nmap(lambda x: x*10, range(10))\nz = []\nt = 0\nwhile t < 10:\n    z.append(t*10)\n    t = t + 1\nz",
        "text": "how many way can you generate the follow list ,",
        "id": 466
    },
    {
        "code": "app_pivot['Percent with Application'] = app_pivot.Application / app_pivot['Total']\napp_pivot",
        "text": "calculate another column call percent with application  , which be equal to application  divide by total   . ",
        "id": 467
    },
    {
        "code": "a = np.array([[1, 2], [3, 4]])\na\nb = a\nb[0,0]=9 \nprint (b)\nprint (\"---\")\nprint (a)",
        "text": "copy and  deep copy  to achieve high performance , assignment in python usually do not copy the underlay object  .  this be important for example when object be pass between function , to avoid an excessive amount of memory copy when it be not necessary ( technical term , pas by reference )  . ",
        "id": 468
    },
    {
        "code": "cities = set([\"Frankfurt\", \"Basel\",\"Freiburg\"])\ncities.add(\"Strasbourg\")\ncities",
        "text": "though set ca n't contain mutable object , set be mutable ,",
        "id": 469
    },
    {
        "code": "\nsns.regplot(x='GDP per Capita - High HDI', y='CO2 per Capita - High HDI', data=time_data)\ncorr_hdi_2011 = pd.DataFrame(hdi_2011).join(pd.DataFrame(co2_per_person_country),how='inner',lsuffix='_co2', rsuffix='_gdp')\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.scatter(corr_hdi_2011.iloc[:,0], corr_hdi_2011.iloc[:,1])\nplt.show()",
        "text": "each dot be a country",
        "id": 470
    },
    {
        "code": "laplacian(v).display()\ncurl(curl(v)).display()\ngrad(div(v)).display()",
        "text": "laplacian of a vector field ,",
        "id": 471
    },
    {
        "code": "\ndef list_of_periods(filename): \n    TimePeriods = []\n    periods_dog = open(filename, \"r\", encoding = \"ISO-8859-15\") \n    list_dog = periods_dog.readlines()[3:]\n    for line in list_dog:\n        name = line.split(\",\")[5]      \n        early = line.split(\",\")[12]    \n        occurrences = line.split(\",\")[0] \n        print(str(name) + \", \" + str(early))\n        print(occurrences)\n    return TimePeriods\nlist_of_periods(\"my-final-formatted-dog.csv\")",
        "text": "make a list of time period ( early interval ) where canis specie be find  . ",
        "id": 472
    },
    {
        "code": "wine.loc[2:3, 'magnesium'] = np.nan\nwine.head()",
        "text": "now set the value of the row 3 and 4 of magnesium a nan",
        "id": 473
    },
    {
        "code": "(a,b,c)= ('alpha','beta','gamma') \na,b,c= 'alpha','beta','gamma' \nprint(a,b,c)\na,b,c = ['Alpha','Beta','Gamma'] \nprint(a,b,c)\n[a,b,c]=('this','is','ok') \nprint(a,b,c)",
        "text": "map one tuple to another tupples can be use a the leave hand side of assignment and be match to the correct right hand side element   -   assume they have the right length",
        "id": 474
    },
    {
        "code": "\n_ = plt.plot( virginica_petal_length,  virginica_petal_width,\n             marker='.', linestyle='none', color='green')\n_ = plt.plot(setosa_petal_length, setosa_petal_width,\n             marker='.', linestyle='none', color='red')\n_ = plt.plot(versicolor_petal_length, versicolor_petal_width,\n             marker='.', linestyle='none')\nplt.margins(0.02)\n_ = plt.xlabel('petal length (cm)')\n_ = plt.ylabel('petal width (cm)')\nplt.show()",
        "text": "make a scatter plot for represent the relation between petal length and petal width in class iris   -   virginica",
        "id": 475
    },
    {
        "code": "lmbda = 5.0\nopt = spline.SplineL1.Options({'Verbose': True, 'gEvalY': False})",
        "text": "set regularization parameter and option for ℓ1   -   spline solver  .  the regularization parameter use here ha be manually select for good performance  . ",
        "id": 476
    },
    {
        "code": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\nmodel_ft = model_ft.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)",
        "text": "finetuning the convnet    -  load a pretrained model and reset final fully connect layer  . ",
        "id": 477
    },
    {
        "code": "if (rebuildFromZipFiles == True):\n    for c in DATA_CLASSES:\n        make_dir(DATA_HOME_DIR + 'sample/train/' + c)\n        make_dir(DATA_HOME_DIR + 'sample/valid/' + c)\n        make_dir(DATA_HOME_DIR + 'train/' + c)\n        make_dir(DATA_HOME_DIR + 'valid/' + c)\n        \n    make_dir(DATA_HOME_DIR + 'test/unknown')",
        "text": "create train , validation , test , and sample directory",
        "id": 478
    },
    {
        "code": "def get_dense_layers():\n    return [\n        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n        Flatten(),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(4096, activation='relu'),\n        Dropout(0.5),\n        Dense(1000, activation='softmax')\n        ]\ndense_model = Sequential(get_dense_layers())\nfor l1, l2 in zip(dense_layers, dense_model.layers):\n    l2.set_weights(l1.get_weights())",
        "text": "this be our usual vgg network just cover the dense layer ,",
        "id": 479
    },
    {
        "code": "from pandas.tools.plotting import scatter_matrix\nscatter_matrix(recent_grads[['Sample_size','Median']],figsize=(6,6))\nfrom pandas.tools.plotting import scatter_matrix\nscatter_matrix(recent_grads[['Sample_size','Median','Unemployment_rate']],figsize=(6,6))",
        "text": "assignment   -   make a scatter matrix of sample size and median   -   make a scatter matrix of sample size , median and unemployment rate",
        "id": 480
    },
    {
        "code": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\ngrid_values = {'C': [10, 100, 1000, 10000]}\ngrid_clf_acc = GridSearchCV(LogisticRegression(), param_grid = grid_values, cv=10, scoring = 'accuracy')\ngrid_clf_acc.fit(X_train, y_train)\nprint('Mean score matrix: ', grid_clf_acc.cv_results_['mean_test_score'])\nprint('Grid best parameter (max. accuracy): ', grid_clf_acc.best_params_)\nprint('Grid best score (accuracy): ', grid_clf_acc.best_score_)",
        "text": "logistic regression classifier with cross validation ( cv=10 ) and hyperparameter optimization",
        "id": 481
    },
    {
        "code": "ax = df['gdp'].plot(kind='barh', alpha=0.5)\nax.set_title('GDP', loc='left', fontsize=14)\nax.set_xlabel('Trillions of US Dollars')\nax.set_ylabel('')",
        "text": "style ( optional ) graph set you might like  . ",
        "id": 482
    },
    {
        "code": "a = ENSAEPROJECT()\na.create_datasets()\na.oversample()\na.automagic_feature_selection()\nprint(a.X_train.shape, a.X_test.shape, a.X_train_cv.shape, a.X_valid_cv.shape)\na.set_model(\"rf\")\na.do_cv = True\na.run_model()",
        "text": "random forest , no oversampling , automatic feature selection , no feature correlation removal",
        "id": 483
    },
    {
        "code": "draw_graph({30}, mode='i')\ndraw_graph({80}, mode='i')\ndraw_graph({1000}, mode='i')",
        "text": "draw one topic set at a time",
        "id": 484
    },
    {
        "code": "from sklearn import model_selection\nX_train, X_dev, y_train, y_dev, train_idx, test_idx = model_selection.train_test_split(X, y, range(len(X)), test_size=200, shuffle=True, random_state=0)\n(X_train.shape, X_dev.shape, y_train.shape, y_dev.shape)\ntrain_idx[100] == 1410",
        "text": "split into train and development set",
        "id": 485
    },
    {
        "code": "train = all_data[:ntrain]\ntest = all_data[ntrain:]\ntrain.head()\ntest.head()",
        "text": "get new train and test dataset",
        "id": 486
    },
    {
        "code": "array([-2,0,1,2,3,4,5,6],float64)",
        "text": "you can pas in a second argument to   -  array  -  that give the numeric type  .  there be a number of type [ list here ] ( <url> ) that your array can be  .  the most common one be float64 ( double precision float point number ) , and int64  . ",
        "id": 487
    },
    {
        "code": "print([len(i) for i in my_string.split()])",
        "text": "use a list comprehension to find the length of each word  . ",
        "id": 488
    },
    {
        "code": "ufo = pd.read_csv('http://bit.ly/uforeports')\nufo.head()\nufo.columns\nufo.shape\nufo.rename(columns= {'Colors Reported': 'Colors_Reported', 'Shape Reported': 'Shape_Reported'}, inplace=True)\nufo.columns\nufo_cols = ['city', 'colors reported', 'shape reported', 'state', 'time']\nufo.columns = ufo_cols\nufo.head()\nufo = pd.read_csv('http://bit.ly/uforeports', names=ufo_cols, header=None)    \nufo.head()\nufo = pd.read_csv('http://bit.ly/uforeports', names=ufo_cols, header=0)\nufo.head()\nufo.columns\nufo.columns = ufo.columns.str.replace(' ', '_')\nufo.columns",
        "text": "how do i rename column in a panda dataframe ?",
        "id": 489
    },
    {
        "code": "pd.scatter_matrix(feature_df[['q1_length', 'q2_length', 'q1_numwords', 'q2_numwords', \n                               'dmr_leven_dist', 'jaro_winker_dist','jaccard_sim_score', 'hamming_dist','unique_word_count',\n                                'normalized_diff_num_nouns', 'polarity_score_diff', 'subjectivity_score_diff','shared_words_normalized']], alpha = 0.3, figsize = (14,8), diagonal = 'kde');",
        "text": "scatter matrix of all textual feature",
        "id": 490
    },
    {
        "code": "P\nP + [[-0.5], [0.4]]  # same as P + H2, thanks to NumPy broadcasting",
        "text": "although matrix can only be add together if they have the same size , numpy allow add a row vector or a column vector to a matrix , this be call *broadcasting* and be explain in further detail in the numpy tutorial  .  we could have obtain the same result a above with ,",
        "id": 491
    },
    {
        "code": "\nf, axarr = plt.subplots(5, 8, figsize=(17, 10))\nrow = 0\ncol = 0\nfor index, frame in enumerate(bench_video[21]):\n    if index in [8, 16, 24, 32, 40]:\n        row += 1\n        col = 0\n    axarr[row, col].imshow(np.squeeze(frame, axis=2), cmap='gray')\n    col += 1",
        "text": "the sequence of frame of a sample video of *walking*",
        "id": 492
    },
    {
        "code": "df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E'])\ndf1.loc[dates[0]:dates[1],'E'] = 1\ndf1\ndf1.dropna(how='any') \ndf3 = df1.fillna(value=5)\ndf3\ndf1\npd.isna(df1)",
        "text": "miss data panda primarily us the value np . nan to represent miss data  .  it be by default not include in computation  .  reindexing allow you to change/add/delete the index on a specify axis  .  this return a copy of the data  . ",
        "id": 493
    },
    {
        "code": "\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nn_list = numpy.array([])\nfor n_estimators in n_list:\n    model = BaggingRegressor(n_jobs=-1,n_estimators=n_estimators)\n    algo = \"Bag\"\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % n_estimators )\n#very high computation time",
        "text": "evaluation , prediction , and analysis * bag decision tree ( bag )",
        "id": 494
    },
    {
        "code": "print ('{feature}\\t\\t\\t\\t{correlation_with_Y1}\\t{correlation_with_Y2}'.format(feature='Key', \n                                                                       correlation_with_Y1='correlation_with_Y1', \n                                                                       correlation_with_Y2='cor_spearman_y2'))\nfor key in mapping:\n    if key[0] != 'Y':\n        cor_spearman_y1 = df_norm[mapping[key]].corr(df_norm[mapping[\"Y1\"]], method='spearman')\n        cor_spearman_y2 = df_norm[mapping[key]].corr(df_norm[mapping[\"Y2\"]], method='spearman')\n        print ('{feature}\\t\\t\\t{correlation_with_Y1}\\t\\t{correlation_with_Y2}'.format(feature=mapping[key], \n                                                                               correlation_with_Y1=cor_spearman_y1, \n                                                                               correlation_with_Y2=cor_spearman_y2))",
        "text": "correlation after plot data in form of scatter and histogram , it 's make sense to prove correlation between input variable and output onces  . ",
        "id": 495
    },
    {
        "code": "sw_class = bnn.CnvClassifier(\"streetview\", bnn.RUNTIME_SW)\nclass_out = sw_class.classify_image(im)\nprint(\"Class number: {0}\".format(class_out))\nprint(\"Class name: {0}\".format(classifier.class_name(class_out)))",
        "text": "launch bnn in software the inference on the same image be perform in sofware on the arm core",
        "id": 496
    },
    {
        "code": "Oracle = make_odd_oracle(1)\nb0 = TP(zero, zero)\nb1 = TP(one, zero)\nb2 = TP(one, zero, zero)\nb3 = TP(one, one, zero)\nPrint(r'$%s \\rightarrow %s$'%( dirac(b0), dirac(Oracle*b0) ))\nPrint(r'$%s \\rightarrow %s$'%( dirac(b1), dirac(Oracle*b1) ))",
        "text": "test a one bite oracle",
        "id": 497
    },
    {
        "code": "\nmodel = baseline_model()\nmodel.summary()\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"CNN Error: %.2f%%\" % (100-scores[1]*100))",
        "text": "we evaluate the model the same way a before with the multi   -   layer perceptron  .  the cnn be fit over 10 epoch with a batch size of 200  . ",
        "id": 498
    },
    {
        "code": "import json\nloaded_config = json.load(open(cfg_file_path))\nloaded_config\ndecoding_config = {\n    'beam_width': 100,\n    'use_beamsearch_decode' : True,\n    'max_decode_step': lang_data_preprocessor.max_dest_seq_length,\n    'write_n_best' : True,\n    'log_device_placement' : True,\n    'batch_size': 1\n}\nfor k, v in loaded_config.items():\n    if k not in decoding_config.keys():\n        decoding_config[k] = v\ndecoding_config\nimport tensorflow as tf\ntf.reset_default_graph() \nsess = tf.Session(config=tf.ConfigProto(\n      allow_soft_placement=True))\n\ndecoding_model = CharacterSeq2SeqModel(session=sess,\n                              config=decoding_config, \n                              mode='decode')\n\ndecoding_model.restore(sess, decoding_config[\"model_saved_path\"])",
        "text": "load the train model from the save file",
        "id": 499
    },
    {
        "code": "cifar10_dataset_folder_path = 'cifar-10-batches-py'\nclass DLProgress(tqdm):\n    last_block = 0\n    def hook(self, block_num=1, block_size=1, total_size=None):\n        self.total = total_size\n        self.update((block_num - self.last_block) * block_size)\n        self.last_block = block_num\nif not isfile('cifar-10-python.tar.gz'):\n    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n        urlretrieve(\n            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n            'cifar-10-python.tar.gz',\n            pbar.hook)\nif not isdir(cifar10_dataset_folder_path):\n    with tarfile.open('cifar-10-python.tar.gz') as tar:\n        tar.extractall()\n        tar.close()",
        "text": "get the data run the follow cell to download the [ cifar   -   10 dataset for python ] ( <url> )  . ",
        "id": 500
    },
    {
        "code": "n_col_to_display = 20\npca_range = np.arange(n_col_to_display) + 1\npca_names = ['PCA_%s' % i for i in pca_range]\nplt.figure(figsize=(10, 10))\nplt.bar(pca_range, pca.explained_variance_[:n_col_to_display], align='center')\nxticks = plt.xticks(pca_range, pca_names, rotation=90)\nplt.ylabel('Variance Explained')\nplt.show()",
        "text": "plot proportion of variance explain with top principal component",
        "id": 501
    },
    {
        "code": "poly4= preprocessing.PolynomialFeatures(4,interaction_only=False)\nX4=poly4.fit_transform(X)[:,1:] \nregress.fit(X4,Y)\nprint('The coefficients for fitted regression model are: a =', regress.intercept_,', b =',regress.coef_[0],',')\nprint ('c =',regress.coef_[1], ', d =',regress.coef_[2], 'and e=' ,regress.coef_[3])\nprint ('\\nThe 4th order poly. regression line fitted is: ')\nprint(regress.intercept_,'+',regress.coef_[0],'x',regress.coef_[1],'x^2+',regress.coef_[2],'x^3',regress.coef_[3],'x^4')",
        "text": "fitting the third order polynomial regression model $ y=a+bx+cx^2+dx^3+ex^4 $ to the data , where x   -   lift _ kg and y   -   put _ m  - ",
        "id": 502
    },
    {
        "code": "TVTimeLineFirstAired = {}\nTVTimeLineFirstAired[\"2011-04-17\"]= 40\nTVTimeLineFirstAired[\"2012-04-01\"]= 40\nTVTimeLineFirstAired[\"2013-03-31\"]= 40\nTVTimeLineFirstAired[\"2014-04-06\"]= 40\nTVTimeLineFirstAired[\"2015-04-12\"]= 40\nTVTimeLineFirstAired[\"2016-04-24\"]= 40\nTVTimeLineFirstAired[\"2017-04-01\"]= 40\nprint(TVTimeLineFirstAired)\nodTVTimeLineFirstAired = collections.OrderedDict(sorted(TVTimeLineFirstAired.items()))\nprint(odTVTimeLineFirstAired)",
        "text": "create data set for firstaired date",
        "id": 503
    },
    {
        "code": "(float(purchases[30]) / 100000.0) / PF",
        "text": "p ( e , f ) = p ( e ) p ( f ) , and they be pretty close in this example  .  due to the random nature of the data it wo n't be exact  .  we can also check that p ( e|f ) = p ( e , f ) /p ( f ) and sure enough , it be ,",
        "id": 504
    },
    {
        "code": "gbm = oo[(oo.Medal == 'Gold') & (oo.Gender == 'Men') & (oo.Sport == 'Badminton')]\ngbm.sort_values(by='Athlete')",
        "text": "which country ha win the most men 's gold medal in single badminton over the year ? sort the result alphabetically by the player 's name  . ",
        "id": 505
    },
    {
        "code": "from astropy import units as u\nlength = 26.2 * u.meter\nprint(length) \ntype(length)\ntype(u.meter)\nlength\nlength.value\nlength.unit\nlength.info",
        "text": "basic how do we define a quantity and which part doe it have ?",
        "id": 506
    },
    {
        "code": "merged_acq['founded_on'] = pd.to_datetime(merged_acq['founded_on'])\nmerged_acq['last_funding_on'] = pd.to_datetime(merged_acq['last_funding_on'])\nmerged_acq['founded_on'].dtype",
        "text": "change founded _ on , and last _ funding _ on to datetime object",
        "id": 507
    },
    {
        "code": "from flair.embeddings import StackedEmbeddings\nstacked_embeddings = StackedEmbeddings(\n    embeddings=[glove_embedding, character_embeddings])",
        "text": "now instantiate the stackedembeddings class and pas it a list contain these two embeddings  . ",
        "id": 508
    },
    {
        "code": "df[\"gross_margin\"]=df[\"revenue\"]-df[\"budget\"]\ndf[[\"movie_id\",\"gross_margin\"]].head(10)",
        "text": "for each movie , compute the gross margin ( difference between revenue and budget )",
        "id": 509
    },
    {
        "code": "log_clf2 = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\nt0 = time.time()\nlog_clf2.fit(X_train_reduced, y_train)\nt1 = time.time()\nprint(\"Training took {:.2f}s\".format(t1 - t0))\ny_pred = log_clf2.predict(X_test_reduced)\naccuracy_score(y_test, y_pred)",
        "text": "softmax regression take much long to train on this dataset than the random forest classifier , plus it perform bad on the test set  .  but that 's not what we be interest in right now , we want to see how much pca can help softmax regression  .  let 's train the softmax regression model use the reduce dataset ,",
        "id": 510
    },
    {
        "code": "ffffffffff = np.ones(10) * 5\nffffffffff",
        "text": "create an array of 10 five",
        "id": 511
    },
    {
        "code": "import statsmodels.api as sm \nX = df[\"RM\"] \ny = target[\"MEDV\"] \nX = sm.add_constant(X) \nmodel = sm.OLS(y, X).fit() \npredictions = model.predict(X)\nmodel.summary()",
        "text": "use statsmodels and run a regression with constant",
        "id": 512
    },
    {
        "code": "library(tidyverse)\ninstall.packages('gvlma', repos = 'http://cran.us.r-project.org')\nlibrary(gvlma)",
        "text": "exam r mark klik   -   misja mikkers   -   introduction the exam consist of 2 part  .  in the first part , you have to run a regression , test if the assumption of a linear regression model be meet , and make 2 graph  .  in the second part of the exam , you will have to make a map of catholic and protestant school in the netherlands   -  package",
        "id": 513
    },
    {
        "code": "\nlog_eval_target = pd.DataFrame(data=log_predictions_on_eval, columns=[\"Predictions\"])\nlog_final_eval_data = pd.concat([eval_raw, log_eval_target], axis = 1)",
        "text": "now that we have the prediction for the evaluation data we need to merge these prediction with the original eval data",
        "id": 514
    },
    {
        "code": "regiment.groupby(['regiment', 'company'])[['preTestScore']].mean()",
        "text": "present the mean pretestscores group by regiment and company",
        "id": 515
    },
    {
        "code": "tr_data = read_data(\"datatraining.txt\")\nts_one_data = read_data(\"datatest.txt\")\nts_two_data = read_data(\"datatest2.txt\")",
        "text": "read train and test dataset",
        "id": 516
    },
    {
        "code": "from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(features, labels)",
        "text": "train a decision tree classifier",
        "id": 517
    },
    {
        "code": "from sklearn import metrics as mt\nacc = mt.accuracy_score(y,yhat)\nconf = mt.confusion_matrix(y,yhat)\nprint(\"accuracy of KNN Model\", acc )\nprint(\"confusion matrix\\n\",conf)\nprint('Cost Matrix \\n', cb)\nprint ('Cost of the KNN model =',find_expected_value(conf, cb))\nmodel_acc['KNN']=acc*100\nmodel_fp['KNN']=conf[1][0]\nmodel_tp['KNN']=conf[0][0]\nmodel_cost['KNN']=find_expected_value(conf, cb)\nmodel_time['KNN']=(end_time - start_time)\n#print(model_time)",
        "text": "confusion , cost matrix and cost for the knn model with 10   -   fold stratify cross   -   validation",
        "id": 518
    },
    {
        "code": "for n in range(2, 10):\n    for x in range(2, n):\n        if n % x == 0:\n            print(n, 'equals', x, '*', n//x)\n            break\n    else:\n        \n        print(n, 'is a prime number')",
        "text": "loop statement may have an else  clause , it be execute when the loop terminate through exhaustion of the list ( with for  ) or when the condition become false ( with while  ) , but not when the loop be terminate by a break  statement  .  this be exemplify by the follow loop , which search for prime number ,",
        "id": 519
    },
    {
        "code": "data2[['Location', 'Allages']].groupby(['Location'], as_index=False).mean()\ndata2 = data2[data2.Location!= 'Total']\nsns.barplot(x='Location', y='Allages', data=data2)\nplt.title('Average annual deaths based on region')\nplt.show()",
        "text": "plot the trend of average annual death across region",
        "id": 520
    },
    {
        "code": "train_set = pickle.load(open('data/train_set_forecasting.pickle'))\nplt.figure(figsize=(20,4))\nplt.plot(train_set)\nplt.show()",
        "text": "forecast for the forecast we be go to use page view data , very similar to the data use in the anomaly detection section  .  it be also page view data and contain 1 sample per hour  . ",
        "id": 521
    },
    {
        "code": "s = \"Hello world\"\ntype(s)\nlen(s)\ns2 = s.replace(\"world\", \"test\")\nprint(s2)",
        "text": "string , list and dictionary    -  string string be the variable type that be use for store text message  . ",
        "id": 522
    },
    {
        "code": "f, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(16, 10))\nsns.countplot(data=df, x=\"OutcomeMonth\", hue=\"OutcomeType\", ax=ax1)\nsns.countplot(data=df, x=\"OutcomeDay\", hue=\"OutcomeType\", ax=ax2)\nplt.show()\nweekdist = pd.DataFrame(data=df[[\"DateTime\", \"OutcomeType\"]])\nweekdist[\"DayOfWeek\"] = weekdist[\"DateTime\"].apply(lambda ts: ts.weekday())\nsns.countplot(data=weekdist, x=\"DayOfWeek\", hue=\"OutcomeType\")",
        "text": "doe the day of the week make a difference to the outcome ? it look like adoption be much more likely during the weekend  . ",
        "id": 523
    },
    {
        "code": "\ndfoas_merge=dfoas_merge.dropna(how='any')\ndfoas_merge.shape\ndfoas_merge.dtypes\ndfoas_merge.info()\ndfoas_merge['CDR'] = dfoas_merge['CDR'].replace(0.5, 1)\ndfoas_merge['CDR'] = dfoas_merge['CDR'].replace(2, 1)\ndfoas_merge.head(10)\nprint (dfoas_merge.CDR[(dfoas_merge.CDR == 0.0)].count(),\",\", dfoas_merge.CDR[(dfoas_merge.CDR == 0.5)].count(), \",\",\n       dfoas_merge.CDR[(dfoas_merge.CDR == 1.0)].count(), \",\", dfoas_merge.CDR[(dfoas_merge.CDR == 2)].count())",
        "text": "drop row with nan    -  now drop row in the merge dataset which have nan in any column  .  we see that there be 809   -   570=239 row with nan value ( miss data ) in at least one column  . ",
        "id": 524
    },
    {
        "code": "def update(kernel_size, min_thresh, max_thresh):\n    exampleImg_sobelDir = dir_thresh(exampleImg_unwarp, kernel_size, (min_thresh, max_thresh))\n    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n    f.subplots_adjust(hspace = .2, wspace=.05)\n    ax1.imshow(exampleImg_unwarp)\n    ax1.set_title('Unwarped Image', fontsize=30)\n    ax2.imshow(exampleImg_sobelDir, cmap='gray')\n    ax2.set_title('Sobel Direction', fontsize=30)\ninteract(update, kernel_size=(1,31,2), \n                 min_thresh=(0,np.pi/2,0.01), \n                 max_thresh=(0,np.pi/2,0.01))\nprint('...')",
        "text": "visualize sobel direction threshold on example image",
        "id": 525
    },
    {
        "code": "\nalphas = alphas\nalpha_optim = alpha_optim\nplt.semilogx(alphas, train_errors, label='Train')\nplt.semilogx(alphas, test_errors, label='Test')\nplt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n           linewidth=3, label='Optimum on test')\nplt.legend(loc='lower left')\nplt.ylim([0, 1.2])\nplt.xlabel('Regularization parameter')\nplt.ylabel('Performance')\nplt.show()",
        "text": "plot the train perforamnce versus the test performance , and observe where the test performance be maximize  .  i ve write an outline of the code you need  . ",
        "id": 526
    },
    {
        "code": "make_offer(delta*offer[4][2],delta*offer[4][1])",
        "text": "what be the payoff for player 1 and 2 in a three period version of the bargain model in the lecture ?   -  7 )   -  in which period will the player reach an agreement ?",
        "id": 527
    },
    {
        "code": "filepath = os.path.join(datasetslib.datasets_root,\n                        'ts-data',\n                        'international-airline-passengers-cleaned.csv'\n                       ) \ndataframe = pd.read_csv(filepath,\n                        usecols=[1],\n                        header=0)\ndataset = dataframe.values\ndataset = dataset.astype(np.float32)\nscaler = skpp.MinMaxScaler(feature_range=(0, 1))\nnormalized_dataset = scaler.fit_transform(dataset)\ntrain,test=dsu.train_test_split(normalized_dataset,train_size=0.67)\nn_x=1\nX_train, Y_train, X_test, Y_test = dsu.mvts_to_xy(train,test,n_x=n_x,n_y=1)",
        "text": "read and pre   -   process the dataset",
        "id": 528
    },
    {
        "code": "results['covs'][2]\nresults['covs'][2][0,0]",
        "text": "use the same set of result , obtain the covariance that em assign the third component  .  what be the variance in the first dimension ?",
        "id": 529
    },
    {
        "code": "optimize.fsolve(lambda x: my_function(x,-2),+1)",
        "text": "this only give one solution ? but with a quadratic equation , there be usually two solution   -  exercise  -  in the follow cell give the python command to give the other solution   - ",
        "id": 530
    },
    {
        "code": "train = pd.read_csv(\"train_121117.csv\", sep=',', error_bad_lines=False, encoding='latin1')#, encoding='utf-8') #header=None,",
        "text": "read a csv into a data frame",
        "id": 531
    },
    {
        "code": "import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\nX_train, y_train           = mnist.train.images, mnist.train.labels\nX_validation, y_validation = mnist.validation.images, mnist.validation.labels\nX_test, y_test             = mnist.test.images, mnist.test.labels\nprint(\"Image Shape: {}\".format(X_train[0].shape))\nprint(\"Training Set:   {} samples\".format(len(X_train)))\nprint(\"Validation Set: {} samples\".format(len(X_validation)))\nprint(\"Test Set:       {} samples\".format(len(X_test)))",
        "text": "load mnist data in tensorflow run the cell above to load the mnist data that come with tensorflow  .  you will use this data in   -  section 1  -  and   -  section 2  - ",
        "id": 532
    },
    {
        "code": "cancer = ''\nfor article in cancer_raw['response']['docs']:\n    art = re.sub(r'[\\[\\]]','', str(article['abstract']))\n    cancer = cancer + art\ncancer[0:200]\nhiv = ''\nfor article in hiv_raw['response']['docs']:\n    art = re.sub(r'[\\[\\]]','', str(article['abstract']))\n    hiv = hiv + art\nhiv[0:200]\nheart = ''\nfor article in heart_raw['response']['docs']:\n    art = re.sub(r'[\\[\\]]','', str(article['abstract']))\n    heart = heart + art\nheart[0:200]",
        "text": "pull out just the abstract of each article into one string  . ",
        "id": 533
    },
    {
        "code": "\nC = 100\nkernel_params = {\n    'a' : 1, \n    'c' : 1, \n    'd' : 3, \n    'sigma' : 0.5\n}\nmodel = svm.SVM(kernel=polynomial_kernel, C=C)\nmodel.fit(X=Xs, y=ys, gram_matrix=gram_matrix, support_vectors=support_vectors, kernel_params=kernel_params)\ny_predict = model.predict(Xt)\ncorrect = np.sum(y_predict == yt[:,0])\nprint(\"%d out of %d predictions correct\" % (correct, len(y_predict)))",
        "text": "$ \\bf tm3 $ , $ \\bf tm4 $ again copy and run the test with the polynomial kernel and c = 100 .  keep the parameter a , c , and d to their default value  . ",
        "id": 534
    },
    {
        "code": "gbm = oo[(oo.Medal == 'Gold')& (oo.Gender == 'Men') & (oo.Sport=='Badminton')]\ngbm\ngbm.sort_values(by='Athlete')",
        "text": "which country ha win men 's gold medal in single badminton over the year ? sort the result alphabetically by the player 's name  . ",
        "id": 535
    },
    {
        "code": "\ntrainV = keras.utils.to_categorical(trainY, num_classes)\ntestV = keras.utils.to_categorical(testY, num_classes)\ntrainR = trainX/255.0\ntestR = testX/255.0\ntrainR=trainR.reshape(60000,784)\ntestR=testR.reshape(10000,784)",
        "text": "in order for our dataset to be compatible with the kera api , we need our label to be represent use [ one   -   hot encode ] ( <url> )  .  we also want to convert the image representation from 8   -   bite ( integer ) grayscale pixel to 32   -   bite float point between 0 and 1  . ",
        "id": 536
    },
    {
        "code": "\nclass Critter(object):\n    def __init__(self):\n        print(\"A new critter has been born!\")\n    def talk(self):\n        print(\"\\nHi. I'm an instance of class Critter.\")\n        \n    def run(self):\n        print(\"\\nI am running fast! \")\ncrit1 = Critter()\ncrit2 = Critter()\ncrit1.talk()\ncrit2.talk()\ncrit1.run()\ncrit2.run()",
        "text": "the constructor critter program define a new critter class that include a simple construc   -   tor method  .  the program also show how easy it be to create multiple object from the same class  . ",
        "id": 537
    },
    {
        "code": "train, test, validation = processedData.randomSplit([0.60, 0.20, 0.20])",
        "text": "split the dataset into train , test and validation set  . ",
        "id": 538
    },
    {
        "code": "\nX_train = train.iloc[:, 1:]\ny_train = train.iloc[:, 0]\nX_test = test.iloc[:, 1:]\ny_test = test.iloc[:, 0]\nfrom sklearn.ensemble import BaggingRegressor\nbagreg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=500, \n                          bootstrap=True, oob_score=True, random_state=1)\nbagreg.fit(X_train, y_train)\ny_pred = bagreg.predict(X_test)\ny_pred\nnp.sqrt(mean_squared_error(y_test, y_pred))",
        "text": "bag decision tree in scikit   -   learn ( with b=500 )",
        "id": 539
    },
    {
        "code": "sln = single_layer_network(alpha=0.001, max_steps=1000)\nsln.fit(np.asarray(X_train), np.asarray(Y_train))\nsln.calc_scores(np.asarray(X_test), np.asarray(Y_test))",
        "text": "calculate accuracy and f1 score",
        "id": 540
    },
    {
        "code": "import plotly.offline as pyo\nimport plotly.graph_objs as go\nimport numpy as np\nnp.random.seed(42)\nrandom_x = np.random.randint(1,101,100)\nrandom_y = np.random.randint(1,101,100)\ndata = [go.Scatter(\n    x = random_x,\n    y = random_y,\n    mode = 'markers',\n)]\nlayout = go.Layout(\n    title = 'Random Data Scatterplot', \n    xaxis = dict(title = 'Some random x-values'), \n    yaxis = dict(title = 'Some random y-values'), \n    hovermode ='closest' \n)\nfig = go.Figure(data=data, layout=layout)\npyo.plot(fig, filename='scatter2.html')",
        "text": "plotly scatter plot example 2",
        "id": 541
    },
    {
        "code": "def vertical_edge_detection(self,image) : \n    kernel = [1,[[-1,0,1],\n                 [-1,0,1],\n                 [-1,0,1]]]\n    processed_image = self.convolution(image,kernel)\n    return processed_image\nimage_processing.vertical_edge_detection = vertical_edge_detection\nimpr = image_processing()\nim = impr.vertical_edge_detection(image)\ndisplay(im)",
        "text": "vertical edge detection ( only detect black to white without smoothen filter )",
        "id": 542
    },
    {
        "code": "\nthemes = pd.DataFrame(columns=['code','name'])\nfor row in worldbank.mjtheme_namecode:\n   themes = themes.append(json_normalize(row))\nthemes.reset_index(drop=True, inplace=True)\nprint('Top 10 major project themes:')\nthemes.name.value_counts().head(10)",
        "text": "find the top 10 major project theme ( use column mjtheme _ namecode  )",
        "id": 543
    },
    {
        "code": "new_observations = pd.DataFrame({'Department':['IT','?','Trade','HR','HR'],\\\n                             'Title':['VP','associate','associate','analyst','analyst'],\\\n                             'Sex':['F', 'F','F','M','F'], 'Year':[9,5,np.NaN,2,2],\\\n                             'Education':['','PHD','Master','Master','Bachelor']},\\\n                             index=['Mary','Amy','Jennifer','John','Judy'])\nnew_observations\n#### Your code here",
        "text": "exercise 15   -  consider if we have a data frame with new observation record ( a in the follow data frame ) , how should we combine it with the old data frame ?",
        "id": 544
    },
    {
        "code": "ecom['Language'].head(5)\necom[ecom['Language']=='en'].count()",
        "text": "how many people have english en  a their language of choice on the website ?",
        "id": 545
    },
    {
        "code": "plt.figure (figsize=(15,15))\nX= train[train.TotalPop < M3P].TotalPop\nY = train[train.TotalPop < M3P].FireAll\nn = train[train.TotalPop < M3P].index\nfig, ax = plt.subplots(figsize=(20,10))\nax.scatter( X, Y,\n         marker='x',\n         color='g',   \n         edgecolor='b',\n       \n         alpha=0.9 )\nxlabel('Population for 2012', size= 25)\nylabel('Fire', size=20)\nfor i,txt in enumerate(n):\n    ax.annotate(txt, (X[i], Y[i]))",
        "text": "scatter graph of population v fire/smoke death for country with population below 3m for a give year",
        "id": 546
    },
    {
        "code": "dh=datahub.datahub(server,version,API_key)\npackage = package_api.package_api(dh,dataset_key,variable,longitude_west,longitude_east,latitude_south,latitude_north,time_start,time_end,area_name=area)\npackage.make_package()\npackage.download_package()",
        "text": "download the data with package api   -   create package object   -   send command for the package creation   -   download the package file",
        "id": 547
    },
    {
        "code": "pp.plot()\nfrom pysal.contrib.points.centrography import hull, mbr, mean_center, weighted_mean_center, manhattan_median, std_distance,euclidean_median,ellipse",
        "text": "we can use pointpattern class method   -  plot  -  to visualize   -  pp  - ",
        "id": 548
    },
    {
        "code": "gmh = oo[(oo.Gender == 'Men') & (oo.Medal =='Gold') &(oo.Event=='100m')]\ngmh\ngmh.sort_values('Edition',ascending=False)[['City','Edition','Athlete','NOC']]",
        "text": "display the male gold medal winner for the 100m sprint event over the year  .  list the result start with the most recent  .  show the olympic city , edition , athlete and the country they represent  . ",
        "id": 549
    },
    {
        "code": "writer = pd.ExcelWriter('Code_Practice_3.xlsx')\nweo.to_excel(writer,'Question 1q')\nwriter.save()",
        "text": "export the dataframe to an excel spreadsheet  . ",
        "id": 550
    },
    {
        "code": "def lasso_plot_runner(alpha=0):\n    coef_plotter(l_alphas, l_coefs, X.columns, alpha, regtype='lasso')\ninteract(lasso_plot_runner, alpha=(0.001,0.2,0.0025))",
        "text": "run the same plot function above , but now with the calculate coefficient of alpha for the lasso  . ",
        "id": 551
    },
    {
        "code": "plt.imshow(arr_lr[10].astype('uint8'));\nplt.imshow(p[0].astype('uint8'));\ntop_model.save_weights(path + 'lesson9/results/' + 'sr_final.h5')\ntop_model.load_weights(path + 'lesson9/results/' + 'sr_final.h5')",
        "text": "after tarining for some time , we get some very impressive result  .  look at these two image , we can see that the predict high resolution image ha fill in a lot of detail , include the shadow under the green and the texture of the food  . ",
        "id": 552
    },
    {
        "code": "from sklearn.cluster import KMeans\nest = KMeans(4)  \nest.fit(X)\ny_kmeans = est.predict(X)\nplt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='rainbow');",
        "text": "by eye , it be relatively easy to pick out the four cluster  .  if you be to perform an exhaustive search for the different segmentation of the data , however , the search space would be exponential in the number of point  . ",
        "id": 553
    },
    {
        "code": "def load_iris_data(filename):\n    records = []\n    with open(filename, 'rt') as csvfile:\n        csvreader = csv.reader(csvfile, delimiter=',')\n        for row in csvreader:\n            if len(row) == 5:  \n                records.append({\n                    \"sepal_length\": float(row[0]),\n                    \"sepal_width\": float(row[1]),\n                    \"petal_length\": float(row[2]),\n                    \"petal_width\": float(row[3]),\n                    \"class\": row[4]\n                })\n    return records\niris_data = load_iris_data(\"data/iris.data\")",
        "text": "the data set be store in a comma   -   separate text file  .  we read it and store it a a list of record , where each record be represent use a dict  . ",
        "id": 554
    },
    {
        "code": "\nproduct=[]\nwith open(file2[0]) as data_file:\n    data=data_file.read()\n    for i in data.split('\\n'):\n        product.append(i)\n        \nproductDataframe=[]\nfor x in product:\n    try:\n        y=x.replace(\"'\",'\"')\n        jdata=json.loads(y)\n        productDataframe.append((jdata['asin'],jdata['title'])) \n    except:\n        pass\nProduct_dataset=pd.DataFrame(productDataframe,columns=['Asin','Title'])",
        "text": "clean of productsample . json  file and import the data a panda dataframe  . ",
        "id": 555
    },
    {
        "code": "print(model.predict_output_word(['sport']))\nmodel.similar_by_word('work')",
        "text": "give the context word you can get the probability of center word",
        "id": 556
    },
    {
        "code": "def toks2ids(sents):\n    voc_cnt = collections.Counter(t for sent in sents for t in sent)\n    vocab = sorted(voc_cnt, key=voc_cnt.get, reverse=True)\n    vocab.insert(PAD, \"<PAD>\")\n    vocab.insert(SOS, \"<SOS>\")\n    w2id = {w:i for i,w in enumerate(vocab)}\n    ids = [[w2id[t] for t in sent] for sent in sents]\n    return ids, vocab, w2id, voc_cnt\nfr_ids, fr_vocab, fr_w2id, fr_counts = toks2ids(fr_qtoks)\nen_ids, en_vocab, en_w2id, en_counts = toks2ids(en_qtoks)",
        "text": "enumerate the unique word ( *vocab* ) in the corpus , and also create the reverse map ( word   -   > index )  .  then use this map to encode every sentence a a list of int index  . ",
        "id": 557
    },
    {
        "code": "conditional_loss = vlb_binomial(x, cond_x_decoded_mean, cond_t_mean, cond_t_log_var)\ncvae = Model([x, label], cond_x_decoded_mean)\ncvae.compile(optimizer=keras.optimizers.RMSprop(lr=0.001), loss=lambda x, y: conditional_loss)",
        "text": "define the loss and the model",
        "id": 558
    },
    {
        "code": "cats = ['sci.space', 'talk.politics.mideast']\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=cats, remove=('headers', 'footers', 'quotes'), shuffle = False)\ntrain_data = newsgroups_train.data\ny_train = newsgroups_train.target\nnewsgroups_valid = fetch_20newsgroups(subset='test', categories=cats, remove=('headers', 'footers', 'quotes'), shuffle = False)\nvalid_data = newsgroups_valid.data\ny_valid = newsgroups_valid.target",
        "text": "decision boundry for binary logistic regression ,",
        "id": 559
    },
    {
        "code": "\nnp.random.seed(222)\nmsk = np.random.rand(len(emailFeatures)) < 0.7\ntrain = emailFeatures[ msk]\ntest  = emailFeatures[~msk]\nprint ( 'Our training dataset has', round ( train.shape[0]/emailFeatures.shape[0] * 100, 2 ),\n        '% of the observations and our test dataset has', round ( test.shape[0]/emailFeatures.shape[0] * 100, 2 ), '%' )",
        "text": "split dataset into test and train",
        "id": 560
    },
    {
        "code": "data = DataFrame(np.arange(16).reshape((4,4)),\n                index = ['Ohio', 'Colorado', 'Utha', 'New York'],\n                columns = ['one', 'two', 'three', 'four']\n                )\ndata\ndata.drop(['Colorado', 'Ohio'])\ndata.drop('two', axis = 1) \ndata.drop(['two', 'four'], axis = 1)",
        "text": "with dataframe , index value can be delete from either axis ,",
        "id": 561
    },
    {
        "code": "\nauthored_contents['text_length'] = authored_contents['text'].str.len()\nauthored_contents.head()",
        "text": "below , we shall study the text length in the train dataset  . ",
        "id": 562
    },
    {
        "code": "test_indis = 10\ntrain_dataset = dataset[dataset.index % test_indis != 0]\ntrain_dataset.describe()",
        "text": "extract train dataset from dataset  . ",
        "id": 563
    },
    {
        "code": "\nstoplist = set('for a of the to in and'.split())\ntexts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\nfrom pprint import pprint\npprint(texts)\nfrom collections import defaultdict\nfrequency = defaultdict(int)\nfor text in texts:\n    for token in text:\n        frequency[token]+=1\ntexts = [[token for token in text if frequency[token]>1] for text in texts]\npprint(texts)",
        "text": "this be a tiny corpus of nine document , each consist of only a single sentence  .  first , let  s tokenize the document , remove common word ( use a toy stoplist ) a well a word that only appear once in the corpus ,",
        "id": 564
    },
    {
        "code": "sns.set_style('whitegrid')\nsns.barplot(x=carrier_grouped.index, y =\"DepDel15\", data=carrier_grouped, color ='green')\nfig = plt.title(\"Carrier Vs Probability of Departure Delay15\")",
        "text": "the carrier b6 and nk seem to have high number of flight arrival delay",
        "id": 565
    },
    {
        "code": "\nimport base64\nimport json\nIMAGE=\"gs://video-ted/images/8.png\"\nvservice = build('vision', 'v1', developerKey=APIKEY)\nrequest = vservice.images().annotate(body={\n        'requests': [{\n                'image': {\n                    'source': {\n                        'gcs_image_uri': IMAGE\n                    }\n                },\n                'features': [{\n                    'type': 'TEXT_DETECTION',\n                    'maxResults': 3,\n                }]\n            }],\n        })\nresponses = request.execute(num_retries=3)\nparse_this =  json.dumps(responses[\"responses\"][0]['textAnnotations'][0]['description'])\nx = parse_this.split('\\\\n')\nprint (x)",
        "text": "invoke vision api   the vision api can work off an image in cloud storage or embed directly into a post message  .  i ll use cloud storage and do ocr on this image ,   < img src=  <url>  width=  450  / >",
        "id": 566
    },
    {
        "code": "for fi in range(len(train_datasets)):\n    with open(train_datasets[fi],'rb') as f:\n        train_class = pickle.load(f)\n        print(train_datasets[fi],\" with shape \",str(train_class.shape))",
        "text": "another check , we expect the data to be balance across class  .  verify that  .  各个 label 下的数据是平衡的 ?    - ",
        "id": 567
    },
    {
        "code": "for k in range(0, len(path)):\n    xml = objectify.parse(open(path[k]))\n    root = xml.getroot()\n    column_header = []\n    column_name = root.findall(\".//*[@name='Table']\")[0].getchildren()[0].getchildren()[0].getchildren()\n    for item in column_name:\n        column_header.append(item.values()[0])\n    length_column = len(column_header)\n    final_data = daata()\n    df = df.append(pd.DataFrame(data = final_data,columns = column_header),ignore_index = True)",
        "text": "create different dataframes for different file and append them in one",
        "id": 568
    },
    {
        "code": "from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(C=1e5)\ntrain_class = oh_train['class']\noh_train_data= oh_train.drop('class',axis=1)\nlogreg.fit(oh_train_data, train_class)\nprint(logreg)",
        "text": "build the model , logistic regression",
        "id": 569
    },
    {
        "code": "svm_cv_scores = cross_val_score(svm.SVC(), SVM_X_train, SVM_y_train, scoring = 'accuracy', cv = 10)\nprint(svm_cv_scores *100, '%')\nprint(svm_cv_scores.mean()*100,'%')",
        "text": "evaluate the model use cross validation",
        "id": 570
    },
    {
        "code": "data.Age[data.Pclass == 1].plot(kind='kde')    \ndata.Age[data.Pclass == 2].plot(kind='kde')\ndata.Age[data.Pclass == 3].plot(kind='kde')\nplt.xlabel(\"Age\")    \nplt.title(\"Age Distribution, within classes\")\nplt.legend(('1st Class', '2nd Class','3rd Class'))",
        "text": "analysis by age and class",
        "id": 571
    },
    {
        "code": "(lr2.predict_proba(X_test_std[0,:].reshape(1, -1)),\nlr3.predict_proba(X_test_std[0,:].reshape(1, -1)))\nlr2.predict_proba(X_test_std[21,:].reshape(1,-1))",
        "text": "predict the probability of a pattern in the test set  . ",
        "id": 572
    },
    {
        "code": "\ndata = pd.read_table(\"wind.data\", sep = \"\\s+\", parse_dates = [[0,1,2]]) \ndata.head()\ndata.shape",
        "text": "read wind . data file and assign it to a variable call data and replace the first 3 column by a proper datetime index  . ",
        "id": 573
    },
    {
        "code": "import logging\nimport threading\nimport time\nlogging.basicConfig(level=logging.DEBUG,\n                    format='[%(levelname)s] (%(threadName)-10s) %(message)s',\n                    )\ndef worker():\n    logging.debug('Starting')\n    time.sleep(2)\n    logging.debug('Exiting')\ndef my_service():\n    logging.debug('Starting')\n    time.sleep(3)\n    logging.debug('Exiting')\nt = threading.Thread(name='my_service', target=my_service)\nw = threading.Thread(name='worker', target=worker)\nw2 = threading.Thread(target=worker) \n\nw.start()\nw2.start()\nt.start()",
        "text": "the  _  _ logging _  _  module support embed the thread name in every log message use the formatter code % ( threadname ) s  -  include thread name in log message make it easy to trace those message back to their source . mb",
        "id": 574
    },
    {
        "code": "from keras.preprocessing.text import Tokenizer\nsamples = ['The cat sat on the mat.', 'The dog ate my homework.']\ntokenizer = Tokenizer(num_words=100)\ntokenizer.fit_on_texts(samples)\nseuqences = tokenizer.texts_to_sequences(samples)\none_hit_results = tokenizer.texts_to_matrix(samples, mode='binary')\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' %len(word_index))",
        "text": "use kera for word   -   level one   -   hot encode",
        "id": 575
    },
    {
        "code": "def match_ends(words):\n    i = 0\n    count_words = 0\n    for word in words :\n        if len(word) >= 2 and word[0]==word[-1]:\n            count_words += 1\n        else :\n            pass\n    return count_words",
        "text": "match _ ends     give a list of string , return the count of the number of string where the string length be 2 or more and the first and last char of the string be the same  .    note , python doe not have a ++ operator , but += work",
        "id": 576
    },
    {
        "code": "A = rand(3, 3)\nx = fill(1, (3,))\nb = A * x",
        "text": "before we get start , let 's set up a linear system",
        "id": 577
    },
    {
        "code": "testingList = [0, 0, 0, 0 , 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\nsearchItem = 1\ntestingList.sort() \nprintResults(testingList, searchItem, True)",
        "text": "value be all the same , except 1",
        "id": 578
    },
    {
        "code": "coefficients = logistic_regression(\n    feature_matrix, sentiment, initial_coefficients = np.zeros(194), step_size = 1e-7, max_iter = 301, verbose = True)",
        "text": "run the logistic regression solver  . ",
        "id": 579
    },
    {
        "code": "train = bike_rentals.sample(frac=.8, random_state = 1)\ntest = bike_rentals.loc[~bike_rentals.index.isin(train.index)]\nfrom sklearn.linear_model import LinearRegression\npredictors = list(train.columns)\npredictors.remove(\"cnt\")\npredictors.remove(\"casual\")\npredictors.remove(\"registered\")\npredictors.remove(\"dteday\")\nreg = LinearRegression()\nreg.fit(train[predictors], train[\"cnt\"])\nimport numpy\npredictions = reg.predict(test[predictors])\nnumpy.mean((predictions - test[\"cnt\"]) ** 2)\ntest[\"cnt\"]",
        "text": "the mean square error metric make the most sense to evaluate our error  .  mse work on continuous numeric data , which fit our data quite well  . ",
        "id": 580
    },
    {
        "code": "print (filename)\nwriteFile = contourPath(filename)\nwriteFileCopy = contourPath(filenamecopy)\ndataIn.to_csv(writeFile, sep=',',index=False)\ndataIn.to_csv(writeFileCopy, sep=',',index=False)\nprint ('Complete')",
        "text": "manual overwrite just for good measure",
        "id": 581
    },
    {
        "code": "spectrum = segment.make_spectrum()\nspectrum.plot(high=50)\nthinkplot.config(xlabel='frequency (Hz)', ylabel='amplitude')",
        "text": "and let 's see the spectrum ,",
        "id": 582
    },
    {
        "code": "\nfrom __future__ import print_function\nimport numpy as np\nimport sys\nimport nsfg\nimport thinkstats2",
        "text": "this header import all of the module and library you will need in this exercise ,",
        "id": 583
    },
    {
        "code": "temperature = random.uniform(0.0,100.0)\nassert abs(FtoC(CtoF(temperature)) - CtoF(FtoC(temperature))) < epsilon\nassert abs(CtoF(-40.0)+40.0) < epsilon\nassert abs(FtoC(41)-5.0) < epsilon",
        "text": "you can test your function below  . ",
        "id": 584
    },
    {
        "code": "apple_isnull = pd.isnull(data[\"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Apple\"])\npumpkin_isnull = pd.isnull(data[\"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Pumpkin\"])\npecan_isnull = pd.isnull(data[\"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Pecan\"])\napple_isnull.value_counts()\nate_pies = (apple_isnull) & (pumpkin_isnull) & (pecan_isnull)\nate_pies.value_counts()",
        "text": "figure out what type of pie people eat",
        "id": 585
    },
    {
        "code": "if 1 == 2:\n    print('hello')\nelif 2 ==2 :\n    print('middle')\nelif 3 ==3 :\n    print('three is true')\nelse:\n    print('last')",
        "text": "conditional statement    -  if elif else",
        "id": 586
    },
    {
        "code": "\nLR_cv_scores = cross_val_score(LogisticRegression(), X_train, y_train, scoring = 'accuracy', cv = 10)\nprint(LR_cv_scores *100, '%')\nprint(LR_cv_scores.mean()*100,'%')",
        "text": "evaluate the model use cross   -   validation",
        "id": 587
    },
    {
        "code": "cm\nprint(cr)\nfrom sklearn.metrics import accuracy_score\naccuracy_test= accuracy_score(y_test, y_predtest)\nprint(accuracy_test)\nfrom sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_predtest)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplt.plot(false_positive_rate, true_positive_rate)\nplt.plot([0,1],[0,1],'r--')",
        "text": "step   -   7 evaluate the model",
        "id": 588
    },
    {
        "code": "response = thinkdsp.read_wave('180960__kleeb__gunshot.wav')\nstart = 0.12\nresponse = response.segment(start=start)\nresponse.shift(-start)\nresponse.plot()\nthinkplot.config(xlabel='time (s)', ylabel='amplitude', ylim=[-1.05, 1.05], legend=False)",
        "text": "this file contain one of the cool example in think dsp  .  it us lti system theory to characterize the acoustic of a record space and simulate the effect this space would have on the sound of a violin performance  .  i ll start with a record of a gunshot ,",
        "id": 589
    },
    {
        "code": "\nPTrueNegative = (PNegativeTrue * PTrue) / PNegative\n\"%.2f\" % (PTrueNegative * 100) + '%'",
        "text": "probability a viewer who be in treatment group doe convert , or p ( convert|negative )   - ",
        "id": 590
    },
    {
        "code": "v60 = np.random.randint(0, 3, (3, 3))\nprint(\"v60:\\n{}\".format(v60))\nprint((~v60.any(axis=0)).any())",
        "text": "how to tell if a give 2d array ha null column ?",
        "id": 591
    },
    {
        "code": "df_top_breeds = cocombined_df.groupby('borough')['Primary Breed'].value_counts().groupby('borough').head(5).plot(kind='barh')",
        "text": "make a bar graph of the top 5 breed in each borough  .  how do you groupby and then only take the top x number ? you really should ask me , because it 's kind of crazy  . ",
        "id": 592
    },
    {
        "code": "@print_before_after\ndef function_to_decorate():\n    print('value inside the function')\nfunction_to_decorate()",
        "text": "recreate the function with the new decorator",
        "id": 593
    },
    {
        "code": "d = 13 \nS_W = np.zeros((d, d))\nfor label,mv in zip(range(1, 4), mean_vecs):\n    class_scatter = np.zeros((d, d)) \n    for row in X[y == label]:\n        row, mv = row.reshape(d, 1), mv.reshape(d, 1) \n        class_scatter += (row-mv).dot((row-mv).T)\n    S_W += class_scatter                             \nprint('Within-class scatter matrix: %sx%s' % (S_W.shape[0], S_W.shape[1]))",
        "text": "compute the within   -   class scatter matrix ,",
        "id": 594
    },
    {
        "code": "pAudio.record(5)\npAudio.save(\"recording_1.wav\")",
        "text": "record and play record a 5   -   second sample and save it into a file  . ",
        "id": 595
    },
    {
        "code": "class Critter(object):\n    total = 0\n    @staticmethod\n    def status():\n        print(\"\\nThe total number of critters is\", Critter.total)\n    def __init__(self, name):\n        print(\"A new critter has been born!\")\n        self.name = name\n        Critter.total += 1\n    def __str__(self):\n        rep = \"Critter object\\n\"\n        rep += \"name: \" + self.name + \"\\n\"\n        return rep\n    def talk(self):\n        print(\"Hi. I'm\", self.name, \"\\n\")\nCritter.status()\ncrit1 = Critter(\"Poochie\")\ncrit1.talk()\ncrit2 = Critter(\"Randolph\")\ncrit2.talk()\nCritter.status()\n#How do we count all critters? We make Total static attribute!",
        "text": "static attribute be an attribute , that belong to the class , not to an object  .  static method be a method , that us such attribute",
        "id": 596
    },
    {
        "code": "def P1(num_examples=5):\n    for i in range(0, num_examples):\n        print ('message: ' + str(newsgroups_test.data[i]) + ', label:' + newsgroups_train.target_names[newsgroups_test.target[i]])\nP1(5)",
        "text": "for each of the first 5 train example , print the text of the message along with the label  .  [ 2 pt ]",
        "id": 597
    },
    {
        "code": "def filter_words(word_list, letter):\n    return filter(lambda word: word[0]==letter,word_list)\nl = ['hello','are','cat','dog','ham','hi','go','to','heart']\nfilter_words(l,'h')",
        "text": "use filter to return the word from a list of word which start with a target letter  . ",
        "id": 598
    },
    {
        "code": "def calc(a, b, op='add'):\n    if op == 'add':\n        return a + b\n    elif op == 'sub':\n        return a - b\n    else:\n        print('valid operations are add and sub')\ncalc(10, 4, op='add')\ncalc(10, 4, 'add')\ncalc(10, 4)\ncalc(10, 4, 'sub')\ncalc(10, 4, 'div')",
        "text": "define a function with two positional argument  ( no default value ) and one keyword argument  ( ha a default value ) ,   - ",
        "id": 599
    },
    {
        "code": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=100, random_state=1)\nlr.fit(X_train_std,y_train)\nplot_decision_regions(X_combined_std,y_combined,classifier=lr,test_idx=range(105,150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='upper left')\nplt.show()\nlr.predict_proba(X_test_std[:3,:])\nlr.predict_proba(X_test_std[:3,:]).argmax(axis=1)",
        "text": "logistic regression in scikit   -   learn",
        "id": 600
    },
    {
        "code": "with graph.as_default():\n    conv1 = tf.layers.conv1d(inputs=inputs_, filters=4, kernel_size=4, strides=1, \n                             padding='same', activation = tf.nn.tanh)\n    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=4, strides=4, padding='same')\n    \n    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=24, kernel_size=6, strides=1, \n                             padding='same', activation = tf.nn.tanh)\n    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=4, strides=4, padding='same')\n    \n                             \n    \n    \n                             \n    #max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')",
        "text": "build convolutional layer note , should we use a different activation ? like tf . nn . tanh ?",
        "id": 601
    },
    {
        "code": "\nnp.random.seed(222)\nmsk = np.random.rand(len(verbatimFeatures)) < 0.7\ntrain = verbatimFeatures[ msk]\ntest  = verbatimFeatures[~msk]\nprint ( 'Our training dataset has', round ( train.shape[0]/verbatimFeatures.shape[0] * 100, 2 ),\n        '% of the observations and our test dataset has', round ( test.shape[0]/verbatimFeatures.shape[0] * 100, 2 ), '%' )",
        "text": "split dataset into test and train",
        "id": 602
    },
    {
        "code": "y=data[['label']].copy(deep=True)\ndata['relative_humidity_3pm'].head()\ny.head()",
        "text": "< p style=  font   -   family , arial , font   -   size,1 . 75em , color , purple , font   -   style , bold  >   target be store in  y",
        "id": 603
    },
    {
        "code": "from __future__ import print_function\nfrom time import time\nimport os\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom sklearn.cross_validation import train_test_split\nimport numpy as np\nimport pickle\nvalidDocsDict = dict()\nfileList = os.listdir(\"PubMedPOS\")\nfor f in fileList:\n    validDocsDict.update(pickle.load(open(\"PubMedPOS/\" + f, \"rb\")))",
        "text": "topic model part of speech this be a notebook for try to use topic model for classify set of text that be more syntactically similar than topically similar  .  this notebook attempt to distinguish between discussion and conclusion section of scientific paper  .  below we be load the dataset for use  . ",
        "id": 604
    },
    {
        "code": "merged_acq_funded['acquiree_uuid'].isin(raw_org['acquiree_uuid']).value_counts()\nlen(merged_acq_funded)",
        "text": "check out if uuids of acquiree company be all available in organization table ,",
        "id": 605
    },
    {
        "code": "newfig()\nplot(xs, ys, label='trajectory')\ndecorate(xlabel='x position (m)',\n         ylabel='y position (m)')",
        "text": "another way to visualize the result be to plot y versus x .  the result be the trajectory through the plane of motion  . ",
        "id": 606
    },
    {
        "code": "Image(\"./img/all_1_cluster.png\", width = 300, height = 300)\nImage(\"./img/all_1.png\", width = 1200, height = 1200)",
        "text": "we ve run the above line of code and save the result , so here we just load the save plot and image  . ",
        "id": 607
    },
    {
        "code": "import statsmodels.api as sm\nX = df[\"RM\"] \ny = target[\"MEDV\"] \nmodel = sm.OLS(y, X).fit()\npredictions = model.predict(X) # make the predictions by the model",
        "text": "use statsmodels and run a regression without constant",
        "id": 608
    },
    {
        "code": "sum((mult_regression_model_one.predict(test_data[columns_one].values) - test_data['price'].values) ** 2)\nsum((mult_regression_model_two.predict(test_data[columns_two].values) - test_data['price'].values) ** 2)\nsum((mult_regression_model_three.predict(test_data[list(columns_three)].values) - test_data['price'].values) ** 2)",
        "text": "calculate r for each model on test data",
        "id": 609
    },
    {
        "code": "\nplt.figure(figsize=(5,5))\nfor i, label in enumerate(groups.keys()):\n    plt.scatter(x=data.loc[data['label']==label, 'x'], \n                y=data.loc[data['label']==label,'y'], \n                color=customPalette[i], \n                alpha=0.20)\n    \n    plt.annotate(label, \n                 data.loc[data['label']==label,['x','y']].mean(),\n                 horizontalalignment='center',\n                 verticalalignment='center',\n                 size=20, weight='bold',\n                 color=customPalette[i])",
        "text": "style 5 , label center on cluster mean",
        "id": 610
    },
    {
        "code": "travel_q = 'How far will you travel for Thanksgiving?'\nless_than_50 = data[data['income integer'] < 50000]\ntravel_less = less_than_50[travel_q]\nprint('For those with a household income under 50,000: ')\ntravel_less_counts = travel_less.value_counts()\ntravel_less_counts\ntravel_less_counts.plot.barh()",
        "text": "how far low   -   income survey respondent travel",
        "id": 611
    },
    {
        "code": "print ([len(w) for w in myss])",
        "text": "use a list comprehension to find the length of each word  . ",
        "id": 612
    },
    {
        "code": "canvas = ROOT.TCanvas(\"c1\", \"c1\", 900, 900)\nHistograms = []\nStacks = []\nFiles = []\nTextarr = []\nlegend = ROOT.TLegend(0.101,0.101,0.899,0.899);\nPlotHistograms(\"../output/Source_Histograms_DsMu_{0}_LHCb.root\")\ncanvas.Draw()",
        "text": "print yield and plot all the template    -  b _ s \\to d _ s  -  \\mu^+ \\nu _  { \\mu } $",
        "id": 613
    },
    {
        "code": "objectives = {\n    'total_kg_throughput': 1 \n}\nconstraints = {\n    'total_kg_backlog': [-1, 0], \n    'total_kg_waste': [-1, 0] \n}\nga_params = {\n    'num_runs': 20,\n    'num_gens': 100,\n    'popsize': 100,\n    'starting_length': 1,\n    'p_xo': 0.108198,\n    'p_product_mut': 0.041373,\n    'p_plus_batch_mut': 0.608130,\n    'p_minus_batch_mut': 0.765819,\n    'p_gene_swap': 0.471346,\n}\n\nmodel1 = DetSingleSiteSimple(**ga_params, random_state=7, num_threads=-1, verbose=True).fit(\n    start_date,\n    objectives,\n    kg_demand,\n    product_data,\n    changeover_days,\n    kg_inventory_target,\n    constraints\n)",
        "text": "[ back to top ] (   -   index ) < a id=so _ problem  >      -  single objective problem 1 * objective , * maximisie   -  total kg throughput  -  constraint ,   -  total kg backlog  -  0 kg   -  total kg waste  -  0 kg",
        "id": 614
    },
    {
        "code": "\nunames = ['user_id', 'gender', 'age', 'occupation', 'zip']\nusers = pd.read_table('./data/movielens/users.dat', sep=';', header=None, names=unames)\nprint('size of users=',len(users))\nusers.head()\nrnames = ['user_id', 'movie_id', 'rating', 'timestamp']\nratings = pd.read_table('../data/movielens/ratings.dat', sep=';', header=None, names=rnames)\nprint('size of ratings=',len(ratings))\nratings.head()\nmnames = ['movie_id', 'title', 'genres']\nmovies = pd.read_table('../data/movielens/movies.dat', sep=';', header=None, names=mnames, encoding='latin-1')\nprint('size of movies=',len(movies))\nmovies.head()",
        "text": "data 's manipulation",
        "id": 615
    },
    {
        "code": "def model_opt(d_loss, g_loss, learning_rate, beta1):\n    \n    t_vars = tf.trainable_variables()\n    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n    d_train = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        g_train = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n    return d_train, g_train",
        "text": "optimization implement model _ opt  to create the optimization operation for the gans  .  use [ tf . trainable _ variables  ] ( <url> ) to get all the trainable variable  .  filter the variable with name that be in the discriminator and generator scope name  .  the function should return a tuple of ( discriminator train operation , generator train operation )  . ",
        "id": 616
    },
    {
        "code": "import mxnet as mx\nimport numpy as np\nimport logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)",
        "text": "in this example , we will show how to use  simple _ bind   api  .  note it be a low level api , by use low level api we be able to touch more detail about mxnet",
        "id": 617
    },
    {
        "code": "\nx = np.array([[1,2],[3,4]])\nx\nnp.sum(x)  \nnp.sum(x, axis=0)  \nnp.sum(x, axis=1) \nx.mean()\nnp.sqrt(a)",
        "text": "numpy provide many useful function for perform computation on array , one of the most useful be sum . for more detail on numpy function visit [ here ] ( <url> )  . ",
        "id": 618
    },
    {
        "code": "print(val_roc_auc * 100)\nX_train = None\ny_train = None\nX_test, non_var = read_data(test, is_train=False)\ndummy_y = np.array(range(len(X_test)))\nmodel.train(False)\ny_pred = []\nfor X_batch, y_batch in iterate_minibatches(X_test, dummy_y, batch_size, shuffle=False):\n    X_batch = Variable(torch.FloatTensor(X_batch)).cuda(0)\n    y_pred.extend(model(X_batch).data.cpu().numpy())\ny_predicted = np.asarray(y_pred)",
        "text": "read test data , fee it to neural network , and save the output in kaggle fromat  . ",
        "id": 619
    },
    {
        "code": "fig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.scatter(*X.T, s=50, c='b', label='original points')\nax1.scatter(*kmeans.cluster_centers_.T, s=50, c='r', label='cluster centers')\nplt.legend(loc='upper left')\nplt.show()",
        "text": "now let 's also plot out cluster center along with the point  . ",
        "id": 620
    },
    {
        "code": "s\ns[0]\ns[0] = 'x'",
        "text": "string property it important to note that string have an important property know a immutability  .  this mean that once a string be create , the element within it can not be change or replace  .  for example ,",
        "id": 621
    },
    {
        "code": "\nimport datetime as dt\ntoday = \"2016-02-3\"\nmydt = dt.datetime.strptime(today, '%Y-%m-%d')\nprint(mydt)",
        "text": "it be impossible for february in any year can have more than 29 day  .  instead of use regular expression to validate date , you can also use python 's datetime  module  .  if a give date string can not be convert to a python date object , then the date would n't be valid  . ",
        "id": 622
    },
    {
        "code": "plt.plot(x, np.sin(x), \"ro\", label=\"sine\")\nplt.plot(x, np.cos(x), \"b^\", label=\"cosine\")\nplt.xlim(0.0, 2.0*np.pi)\nplt.legend(frameon=False, loc=\"best\")",
        "text": "we can use symbol instead of line pretty easily too and label them  . ",
        "id": 623
    },
    {
        "code": "list_lit = [1,2,3,4]\nlist_lit\nlist_fun = list((1,2,3,4))\nlist_fun\nlist_x = range(4)\nprint(list_x)\nprint(list(list_x)) \nempty = []\nempty\nsame_value_repeated = [1] * 10\nsame_value_repeated",
        "text": "list [ array ] we have see several example of list  .  list be mutable sequence that be iterate in order  .  there be two form we can create a list with a list literal use the square bracket or with the list function",
        "id": 624
    },
    {
        "code": "spectrum = segment.make_spectrum()\nspectrum.plot_power()\nthinkplot.config(xlabel='Frequency (Hz)')",
        "text": "and here 's it spectrum ,",
        "id": 625
    },
    {
        "code": "\nimport helper\nimport problem_unittests as tests\nsource_path = 'data/small_vocab_en'\ntarget_path = 'data/small_vocab_fr'\nsource_text = helper.load_data(source_path)\ntarget_text = helper.load_data(target_path)",
        "text": "language translation in this project , build a neural network machine translation and train a sequence to sequence model on a dataset of english and french sentence that can translate new sentence from english to french   -  get the data since translate the whole language of english to french will take lot of time to train , we have provide you with a small portion of the english corpus  . ",
        "id": 626
    },
    {
        "code": "name = (input(\"Enter Name:\"))\nage = int(input(\"Enter Age:\"))\nDegree = (input(\"Enter Degree:\"))\nNumber = int(input(\"Enter Phone Number:\"))\naddress = (input(\"Enter Adress:\"))\nprint(\"**********************************************************\")\nprint(\"Name:\",name)\nprint(\"Agfe:\",age)\nprint(\"Adress:\",address)\nprint(\"Phone Number:\",Number)\nprint(\"Degree:\",Degree)\nprint(\"**********************************************************\")",
        "text": "write a small program in python to print your cv",
        "id": 627
    },
    {
        "code": "import os\ntest_imgs = os.listdir(\"test_images/\")\nprint(\"Number of test images: \", len(test_imgs))\nos.listdir(\"test_images/\")",
        "text": "test image build a pipeline to work on the image in the directory  test _ images   . ",
        "id": 628
    },
    {
        "code": "new_observations = pd.DataFrame({'Department':['IT','?','Trade','HR','HR'],\\\n                             'Title':['VP','associate','associate','analyst','analyst'],\\\n                             'Sex':['F', 'F','F','M','F'], 'Year':[9,5,np.NaN,2,2],\\\n                             'Education':['','PHD','Master','Master','Bachelor']},\\\n                             index=['Mary','Amy','Jennifer','John','Judy'])\nnew_observations\nEmployee = pd.concat([Employee, new_observations])\nEmployee",
        "text": "exercise 15   -  consider if we have a data frame with new observation record ( a in the follow data frame ) , how should we combine it with the old data frame ?",
        "id": 629
    },
    {
        "code": "oo.groupby(['Athlete', 'Medal']).size()\ng = oo.groupby(['Athlete', 'Medal']).size().unstack('Medal', fill_value=0)\ng\ng.sort_values(['Gold','Silver','Bronze'], ascending=False)[['Gold','Silver','Bronze']]\ng.sort_values(['Gold','Silver','Bronze'], ascending=False).head()\noo.head()\ng.sort_values(['Gold','Silver','Bronze'], ascending=False).head().plot(kind='bar');",
        "text": "and finally , plot the five athlete who have win the most gold medal over the history of the olympics  .  when there be a tie , consider the number of silver medal and then bronze medal  . ",
        "id": 630
    },
    {
        "code": "\nfrom re_test_patterns import test_patterns\ntest_patterns(\n    'This is some text -- with punctuation.',\n    [('[^-. ]+', 'sequences without -, ., or space')],\n)",
        "text": "a character set can also be use to exclude specific character  .  the carat ( ^ ) mean to look for character that be not in the set follow the carat  .  this pattern find all of the substring that do not contain the character   -   ,  .  , or a space  . ",
        "id": 631
    },
    {
        "code": "batch_size = 100\nepochs = 20\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_split=1/12)",
        "text": "train the model train the model ( use stochastic gradient descent ) with give batch size , for give number of epoch  .  we split of 1/12   -   th of the data ( 5,000 of the 60,000 sample ) a validation data , such that we can use the validation accuracy for hyperparameter tune  . ",
        "id": 632
    },
    {
        "code": "\nendpoint = 'https://wikimedia.org/api/rest_v1/metrics/legacy/pagecounts/aggregate/{project}/{access}/{granularity}/{start}/{end}'\nheaders={'User-Agent' : 'https://github.com/niharikasharma', 'From' : 'njsharma@uw.edu'}\nparams = {'project' : 'en.wikipedia.org',\n            'access' : 'desktop-site',\n            'granularity' : 'monthly',\n            'start' : '2008010100',\n            'end' : '2016080100'\n         }\napi_call = requests.get(endpoint.format(**params))\nresponse_pagecounts_desktop_site_200801_201607 = api_call.json()\ntry:\n    os.makedirs('data')\nexcept OSError as e:\n    if e.errno != errno.EEXIST:\n        raise\n        \nwith open('data/pagecounts_desktop_site_200801_201607.json', 'w') as outfile:\n    json.dump(response_pagecounts_desktop_site_200801_201607, outfile)",
        "text": "data acquisition    -  call the legacy pagecounts api ( documentation , endpoint ) that provide access to desktop traffic data from january 2008 through july 2016 and save the raw result into a json source data file  . ",
        "id": 633
    },
    {
        "code": "import ast\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\npd.set_option('display.max_rows', 10000)\nfile_name = 'tlg_auth_sent_data_v3.txt'\ndir_location = '~/Downloads'\nrel_path = os.path.join(dir_location, file_name)\nabs_path = os.path.expanduser(rel_path)\nwith open(abs_path) as f:\n    r = f.read()\nd = ast.literal_eval(r)\ndf = pd.DataFrame(d)",
        "text": "next , i offer some simple view of the file tlg _ auth _ sent _ data _ v3 . txt  use the data analysis library panda  . ",
        "id": 634
    },
    {
        "code": "def answer_four():\n    Top15 = answer_one()\n    avg_GDP = answer_three()\n    answer_country = b.index[5]\n    answer_country_data = Top15.loc[answer_country]\n    return answer_country_data['2015'] - answer_country_data['2006']",
        "text": "( 6 . 6 % ) by how much have the gdp change over the 10 year span for the country with the 6th large average gdp ? this function should return a single number  . ",
        "id": 635
    },
    {
        "code": "poly = PolynomialFeatures(degree=3)\nx_new = poly.fit_transform(x_old)\nprint(\"Polynomial Degree 3 Data Shape = \", x_new.shape)",
        "text": "create a polynomial regression of degree 3",
        "id": 636
    },
    {
        "code": "\nplt.subplot(2,1,1)\nplt.xticks(rotation=45)\nplt.title('AAPL: 2001 to 2011')\nplt.plot(aapl, color='blue')\nview = aapl['2007':'2008']\nplt.subplot(2,1,2)\nplt.xticks(rotation=45)\nplt.title('AAPL: 2007 to 2008')\nplt.plot(view, color='black')\nplt.tight_layout()\nplt.show()",
        "text": "multiple time series slice ( 1 )   - ",
        "id": 637
    },
    {
        "code": "S1.set_loads(_Tx=0, _Ty=Ty, _Nz=0, _Mx=Mx, _My=0, _Mz=0)",
        "text": "set   -  loads  -  on the section ,   -  example 1  -  ,  _ shear _  in   -  y  -  direction and  _ bending moment _  in   -  x  -  direction",
        "id": 638
    },
    {
        "code": "image = cv2.imread('test_images/test3.jpg')\nundist = undistort(image, mtx, dist)\nhls = cvtHLS(image)\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,15))\nax1.imshow(cv2.cvtColor(undist, cv2.COLOR_BGR2RGB))\nax1.set_title('Original Image', fontsize=20)\nax1.axis('off')\ns_binary = (applyBinaryThresh(selectChannel(hls, 2), 175, 255))\nax2.imshow(s_binary)\nax2.set_title('S-Channel Binary Image', fontsize=20)\nax2.axis('off')",
        "text": "select the s   -   channel of hl we can observe that the line be most clear in s   -   channel and hence i apply thresholding to isolate the s   -   channel an obtain a binary image of the same  . ",
        "id": 639
    },
    {
        "code": "samples_1 = np.random.normal(loc=1, scale=.5, size=10000)\nsamples_2 = np.random.standard_t(df=10, size=10000)\nbins = np.linspace(-3, 3, 50)\nplt.hist(samples_1, bins=bins, alpha=0.5, label='samples 1')\nplt.hist(samples_2, bins=bins, alpha=0.5, label='samples 2')\nplt.legend(loc='upper left')",
        "text": "two histogram on the same plot",
        "id": 640
    },
    {
        "code": "m_all_mean = np.mean(trace_all[:,0])\nplt.figure(figsize=(3.0,3.0))\nplt.hist(trace_all[:,0], bins=200, histtype='step', normed=1, color='blue')\nplt.axvline(m_all_mean, c = 'maroon', ls='--')\nplt.xlabel('slope $m$');plt.ylabel('frecuency of appearance')\nplt.title('PDF for slope - all data points')\nplt.show()",
        "text": "all data point finally we consider the 1d posterior distribution of $ m $ , $ c $ and $ s $ when we use all data point  .  we will use them late to answer the second question  . ",
        "id": 641
    },
    {
        "code": "grid_searchl1cv20 = GridSearchCV(estimator = log1, param_grid = hparameters, cv=20, verbose = 1,\n                           n_jobs = -1, return_train_score = True)\nX_train.shape\ngrid_searchl1cv20.fit(X_train, y_train)\ngrid_searchl1cv20.best_estimator_\ngrid_searchl1cv20.best_params_\ngrid_searchl1cv20.best_score_",
        "text": "gridsearchcv for different value of c , cross validation ( cv ) = 20 , penalty=l1",
        "id": 642
    },
    {
        "code": "\ndistribution_types = ['pd', 'pcd']\nstep_sizes = [7.5, 15.0, 25.0, 50.0, 100.0]\nkernel_widths = [0, 7.5, 15.0, 25.0, 50.0, 100.0]\nmodel_types = ['single', 'multi']\nsave_path = os.path.join('.', 'data')",
        "text": "set the dataset , feature and train model path and define the parameter",
        "id": 643
    },
    {
        "code": "train = read_csv(\"train.csv\")\ntest = read_csv(\"test.csv\")",
        "text": "read the train and test data",
        "id": 644
    },
    {
        "code": "model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\nif use_gpu:\n    model_ft = model_ft.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)",
        "text": "finetuning the convnet load a pretrained model and reset final fully   -   connect layer",
        "id": 645
    },
    {
        "code": "def fit_and_predict(rdd):\n    \n    \n    return rdd\nlabels_and_preds = fit_and_predict(labeled_point)\nprint(labels_and_preds.take(5))\nassert_is_instance(labels_and_preds, pyspark.rdd.PipelinedRDD)\nassert_equal(labels_and_preds.count(), len_data)\nassert_equal(\n    labels_and_preds.take(5),\n    [(0.0, 0.0),\n     (0.0, 0.0),\n     (1.0, 1.0),\n     (0.0, 0.0),\n     (1.0, 0.0)]\n    )",
        "text": "use [ logisticregressionwithsgd ] ( <url> ) to train a [ logistic regression ] ( <url> ) model   -  use 10 iteration  .  use default parameter for all other parameter other than iterations    -  use the result logistic regression model to make prediction on the entire data , and return an rdd of ( label , prediction ) pair  . ",
        "id": 646
    },
    {
        "code": "def customer_proc(env, name, shoppingTime, serviceTime, customerQ, serverQ, qMonitor, waitMonitor):\n    \n    timeIn = env.now\n    print( \"T=%3.1f Customer %s enters store\" % (timeIn, name) )\n    \n    yield env.timeout(shoppingTime)\n    print( \"T=%3.1f Customer %s enters queue\" % (env.now, name) )\n    \n    \n    if serverQ != []:\n        arrivalEvent = serverQ.pop()\n        arrivalEvent.succeed()\n        \n    serviceDoneEvent = env.event() \n    customerQ.append((serviceDoneEvent, serviceTime )) \n    qMonitor.append((env.now, len(customerQ))) \n    yield serviceDoneEvent \n    \n    waitMonitor.append(env.now-timeIn)\n    print( \"T=%3.1f Customer %s leaves store\" % (env.now, name) )",
        "text": "the customer process the customer arrive at the store , spend some time choose item to buy , then enter the queue to pay at the cashier   -  img src=    -  figures/customer _ proc . png  alt=  customer process  style=  width , 400px ,  / >",
        "id": 647
    },
    {
        "code": "record = ('ACME', 50, 123.45, (12, 18, 2012))\nname, *_, (*_, year) = record\nprint(name, year)",
        "text": "sometimes you might want to unpack value and throw them away  .  you can  t just specify a bare * when unpack , but you could use a common throwaway variable name , such a  _  or i gn ( ignore )  .  for example ,",
        "id": 648
    },
    {
        "code": "a = np.array([1, 2, 3], float) \ntype(a.tolist())",
        "text": "list can also be create from array ,",
        "id": 649
    },
    {
        "code": "bm_score = pd.Series(y).value_counts()[0] / len(y)\ntrain_score = model.evaluate(X_train, y_train)[1]\ntest_score = model.evaluate(X_test, y_test)[1]\nprint(\"Accuracy | Benchmark: {:0.3}, Train: {:0.3}, Test: {:0.3}\".format(bm_score, train_score, test_score))",
        "text": "accuracy score on benchmark , train and test set",
        "id": 650
    },
    {
        "code": "signal = thinkdsp.SinSignal(freq=440)\nduration = signal.period * 30.25\nwave = signal.make_wave(duration)\nspectrum = wave.make_spectrum()\nspectrum.plot(high=880)\nthinkplot.config(xlabel='Frequency (Hz)')",
        "text": "exercise run and listen to the example in chap03 . ipynb  .  in the leakage example , try replace the ham window with one of the other window provide by numpy , and see what effect they have on leakage  .  see <url>    -  solution here 's the leakage example ,",
        "id": 651
    },
    {
        "code": "draw_graph({30, 40}, mode='i')\ndraw_graph({70, 80}, mode='i')\ndraw_graph({30,40,70,80}, mode='i')\ndraw_graph({1000,1010,1020}, mode='i')",
        "text": "draw multiple topic set at a time",
        "id": 652
    },
    {
        "code": "import numpy as np\nimport tensorflow as tf\nwith open('../sentiment_network/reviews.txt', 'r') as f:\n    reviews = f.read()\nwith open('../sentiment_network/labels.txt', 'r') as f:\n    labels = f.read()\nreviews[:2000]\nlabels[:100]",
        "text": "sentiment analysis with an rnn",
        "id": 653
    },
    {
        "code": "\ndef exportToCSV(pandasSorDF, filename):\n    path = os.getcwdu() + \"\\\\\" + filename\n    if os.path.isfile(path + \".csv\"):\n        for i in range(1,20):\n            testFileName = filename  + \"-\" + str(i) + \".csv\"\n            if os.path.isfile(os.getcwdu() + \"\\\\\" +  testFileName)!=True:\n                pandasSorDF.to_csv(testFileName)\n                break\n    else:\n        pandasSorDF.to_csv(filename + \".csv\")",
        "text": "function to export calculate data to csv",
        "id": 654
    },
    {
        "code": "G = nx.read_edgelist(\"facebook_network.txt\", create_using = nx.Graph(), nodetype = int)\npos = nx.spring_layout(G)\nfor i in G:\n    G.node[i] = {'pos': pos[i]}\nfor key in pos:\n    pos[key] = list(pos[key])\ndmin=1\nncenter=0\nfor n in pos:\n    x,y=pos[n]\n    d=(x-0.5)**2+(y-0.5)**2\n    if d<dmin:\n        ncenter=n\n        dmin=d\np=nx.single_source_shortest_path_length(G,ncenter)",
        "text": "get node position store position a node attribute data for random _ geometric _ graph and find node near center ( 0 . 5 , 0 . 5 )",
        "id": 655
    },
    {
        "code": "df.loc['Garland',:]",
        "text": "access one row by specify index label",
        "id": 656
    },
    {
        "code": "plot_corr(pd.concat([df_stage3[['song_hotttnesss']],df_stage3[df_stage3.columns[-17:]]], axis=1), \"../datastory/figures/correlationGenre.png\")",
        "text": "and let 's do the same thing for genre only  . ",
        "id": 657
    },
    {
        "code": "import pandas as pd\norders = pd.read_table('http://bit.ly/chiporders')\norders.head()\nuser_cols = ['user_id','age','gender','occupation','zip_code']\nusers = pd.read_table('http://bit.ly/movieusers', sep = '|', names = user_cols, header = None)\nusers.head()",
        "text": "how to read a tabular data file in panda ?",
        "id": 658
    },
    {
        "code": "import numpy as np\nn = np.array(mat.get_complete_refractive())\nprint(n)",
        "text": "get all material refractive index data a a numpy array",
        "id": 659
    },
    {
        "code": "models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)",
        "text": "model evaluation we can now rank our evaluation of all the model to choose the best one for our problem  .  while both decision tree and random forest score the same , we choose to use random forest a they correct for decision tree  habit of overfitting to their train set  . ",
        "id": 660
    },
    {
        "code": "dates[0]\ndf.loc[dates[0]]",
        "text": "selection by label see more in [ selection by label ] ( <url> ) for get a cross section use a label",
        "id": 661
    },
    {
        "code": "def Odds(p):\n    return p / (1-p)",
        "text": "odds the follow function convert from probability to odds  . ",
        "id": 662
    },
    {
        "code": "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope,\n                         output_fn, keep_prob):\n    train_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_train(encoder_state)\n    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, train_decoder_fn, dec_embed_input, sequence_length, scope=decoding_scope)\n    train_logits =  output_fn(train_pred)\n    return train_logits\ntests.test_decoding_layer_train(decoding_layer_train)",
        "text": "decode   -   train create train logits use [ tf . contrib . seq2seq . simple _ decoder _ fn _ train ( )  ] ( <url> ) and [ tf . contrib . seq2seq . dynamic _ rnn _ decoder ( )  ] ( <url> )  . ",
        "id": 663
    },
    {
        "code": "val81 = topology.residue(80)\nasn116 = topology.residue(115)\nprint(val81, asn116)\ngtp201 = topology.residue(166)",
        "text": "list the atom contact most common within a give residue contact",
        "id": 664
    },
    {
        "code": "def worker(num):\n    print ('Worker: %s' %num)\n    return\nthreads = []\nfor i in range(5):\n    t = threading.Thread(target = worker, args = (i,))\n    threads.append(t)\n    t.start()",
        "text": "it be useful to be able to spawn a thread and pas it argument to tell it what work to do  .  the example below pass a number , which the thread then print  . ",
        "id": 665
    },
    {
        "code": "\nt0 = time()\nout_separable = conv_nested(img, k1)\nout_separable = conv_nested(out_separable, k2)\nt1 = time()\nt_separable = t1 - t0\nplt.subplot(1,2,1)\nplt.imshow(out)\nplt.title('Normal convolution')\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.imshow(out_separable)\nplt.title('Separable convolution')\nplt.axis('off')\nplt.show()\nprint(\"Normal convolution: took %f seconds.\" % (t_normal))\nprint(\"Separable convolution: took %f seconds.\" % (t_separable))\n\nassert np.max(out_separable - out) < 1e-10",
        "text": "we now apply the two version of convolution to the same image , and compare their run time  .  note that the output of the two convolution must be the same  . ",
        "id": 666
    },
    {
        "code": "PlayerMerge <- merge(temp,No_of_leagues,by=\"player_api_id\")\nhead(PlayerMerge)",
        "text": "merge the no _ of _ leagues column that we get above with the data frame we make merge n0 _ of _ matches",
        "id": 667
    },
    {
        "code": "\nRegionUType = 399726\nDisFrmCBD = -63171\nNumOfRoom = 267220\nNumOfCar = 89556\nPP1 = DisFrmCBD*2 + NumOfRoom*2 + RegionUType\nprint('The price of the Property which located 2 KM from CBD is AUD',PP1)\nPP2 = DisFrmCBD*3 + NumOfRoom*2 + NumOfCar*1 + RegionUType\nprint('The price of the Property which located 3 KM from CBD is AUD',PP2)",
        "text": "from the regression model , assume northern region with property type a unit*",
        "id": 668
    },
    {
        "code": "\na = 5\na = a + a\na",
        "text": "python allow you to write over assign variable name  .  we can also use the variable themselves when do the reassignment  .  here be an example of what i mean ,",
        "id": 669
    },
    {
        "code": "\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=6)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\nfrom scipy.stats import mode\nlabels = np.zeros_like(y_true)\nfor i in range(6):\n    mask = (y_kmeans == i)\n    labels[mask] = mode(y_true[mask])[0]\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_true, labels)",
        "text": "set the number of cluster to 6 and apply kmeans cluster to the data  .  compute the accuracy score between the true label and the one estimate by the kmeans algorithm  . ",
        "id": 670
    },
    {
        "code": "smaller = int(input('What is your smaller number? '))\nlarger = int(input('What is your larger number? '))\nproduct = 1\nfor i in range(smaller+1, larger):\n    print(i)\n    product = product*i\nprint(product)",
        "text": "suppose you want to ask the user for two number and multiply all the number between those number ( non   -   inclusive )  .  how would you do it ?",
        "id": 671
    },
    {
        "code": "a=np.floor(10*np.random.random((3,4)))\na\na.shape",
        "text": "shape manipulation    -  change the shape of an array an array ha a shape give by the number of element along each axis ,",
        "id": 672
    },
    {
        "code": "X_reduced = PCA(n_components=2).fit_transform(X)\ncolors = ['navy', 'turquoise', 'darkorange']\nlw = 2\nplt.figure()\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    plt.scatter(X_reduced[y == i, 0], X_reduced[y == i, 1], color=color, alpha=.8, lw=lw,\n                label=target_name)\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nplt.title('PCA of IRIS dataset')\nplt.show()",
        "text": "2d plot of first 2 principal component of pca",
        "id": 673
    },
    {
        "code": "targets = test_data['sentiment']\nprobabilities = model.predict(test_data, output_type='probability')\npredictions = apply_threshold(probabilities, 0.98)\ngraphlab.evaluation.confusion_matrix(targets, predictions)",
        "text": "quiz question  -  , use threshold  = 0 . 98 , how many   -  false negatives  -  do we get on the   -  test _ data  -  ? (   -  hint  -  , you may use the graphlab . evaluation . confusion _ matrix  function implement in graphlab create  .  )   -  answer ,   -  5826",
        "id": 674
    },
    {
        "code": "def count_unknown(): \n    counter = {} \n    for line_number,line in enumerate(fastq):\n        if line_number%4==1: \n            line=line.rstrip() \n            for base_number,base in enumerate(line): \n                if base_number != (len(line)-1): \n                    kmer = line[base_number]+line[base_number +1] \n                    if kmer in counter:\n                        counter[kmer]+=1 \n                    else: counter[kmer]=1 \n    print(counter)\ncount_unknown()",
        "text": "count the number of each pair of base , without assume you know in advance the possible pair",
        "id": 675
    },
    {
        "code": "coffee_red[coffee_red['drinks_coffee'] == False]['height'].mean()",
        "text": "of the individual who do not drink coffee , what be the average height ?",
        "id": 676
    },
    {
        "code": "m_city.info()\nm_city.head()\nm_city.isnull().sum()",
        "text": "global temperature by major city      -  clean data",
        "id": 677
    },
    {
        "code": "5+6\n3-2\n5*8",
        "text": "elementary arithmetic operation    -  python be capable of work like a calculator with some caveat  . ",
        "id": 678
    },
    {
        "code": "\ncountry_names = medals['NOC']\nmedal_counts = country_names.value_counts()\nprint(medal_counts.head(15))",
        "text": "top 15 country rank by total number of medal",
        "id": 679
    },
    {
        "code": "for dtype in [np.int8, np.int32, np.int64]:\n   print(np.iinfo(dtype).min)\n   print(np.iinfo(dtype).max)\nfor dtype in [np.float32, np.float64]:\n   print(np.finfo(dtype).min)\n   print(np.finfo(dtype).max)\n   print(np.finfo(dtype).eps)",
        "text": "print the minimum and maximum representable value for each numpy scalar type",
        "id": 680
    },
    {
        "code": "d = {'person':2, 'cat':4, 'spider':8}\nfor animal in d:\n    legs = d[animal]\n    print('A %s has %d legs' % (animal, legs))",
        "text": "loop , it be easy to iterate over the key in a dictionary",
        "id": 681
    },
    {
        "code": "\nfeature_cols = ['CompetitionDistance','Promo','Promo2','NewAssortment','NewStoreType']\nX = combined_train_data1[feature_cols]\ny = combined_train_data1.Sales\ny1 = combined_train_data1.Customers\ntest=combined_test_data\nfrom sklearn import cross_validation\nfeatures_train, features_test, labels_train, labels_test =cross_validation.train_test_split(X, y, test_size=0.3, random_state=42)\nlabels_train.head()",
        "text": "split train data into 70 % train data and 30 % test data",
        "id": 682
    },
    {
        "code": "pickle_file = train_datasets[7]  \nwith open(pickle_file, 'rb') as f:\n    letter_set = pickle.load(f)  \n    sample_idx = np.random.randint(len(letter_set))  \n    sample_image = letter_set[sample_idx, :, :]  \n    plt.figure()\n    plt.imshow(sample_image)  \npickle_file = train_datasets[9]  \nwith open(pickle_file, 'rb') as f:\n    letter_set = pickle.load(f)  \n    sample_idx = np.random.randint(len(letter_set))  \n    sample_image = letter_set[sample_idx, :, :]  \n    plt.figure()\n    plt.imshow(sample_image)  # display it",
        "text": "let 's verify that the data still look good  .  display a sample of the label and image from the ndarray  . ",
        "id": 683
    },
    {
        "code": "long = 103.8\nlat = 1.366667\nfor i in df[df.Country == 'Singapore'].index:\n    df.loc[i,'Centroid_Longitude'] = long\n    df.loc[i,'Centroid_Latitude'] = lat\ndf[df.Country == 'Singapore'].head(1)",
        "text": "singapore     source    -  url>",
        "id": 684
    },
    {
        "code": "\nstmt = select([census.columns.state])\nstmt = stmt.order_by(census.columns.state)\nresults = connection.execute(stmt).fetchall()\nprint(results[:10])",
        "text": "ordering by a single column  - ",
        "id": 685
    },
    {
        "code": "experiment_dir = '/output'\noutput_dir = 'datasink'\nworking_dir = 'workingdir'\nsubject_list = ['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-05',\n                'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-10']\ntask_list = ['fingerfootlips']\nfwhm = [4, 8]\nwith open('/data/ds000114/task-fingerfootlips_bold.json', 'rt') as fp:\n    task_info = json.load(fp)\nTR = task_info['RepetitionTime']\niso_size = 4",
        "text": "experiment parameter it 's always a good idea to specify all parameter that might change between experiment at the begin of your script  .  we will use one functional image for fingerfootlips task for ten subject  . ",
        "id": 686
    },
    {
        "code": "train_cols_main = [ 'has_wheat' ,'has_milk','has_cocoa','has_palm',\n               'has_water', 'has_garlic','has_flour' ,'has_ginger','has_honey','is_plant','is_bev',\n                 'is_sugar','is_frozen','is_fresh','text_count','intercept']\nlogit_train6 = sm.Logit(train['high_add'], train[train_cols_main])\nresult_train6 = logit_train6.fit()\nresult_train6.summary()",
        "text": "remove is _ dairy  from the model",
        "id": 687
    },
    {
        "code": "xgb_model = xgb.XGBClassifier()\neval_set = [(X_test[training_vars], y_test)]\nxgb_model.fit(X_train[training_vars], y_train, eval_metric=\"auc\", eval_set=eval_set, verbose=False)\npred = xgb_model.predict_proba(X_train[training_vars])\nprint('xgb train roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\npred = xgb_model.predict_proba(X_test[training_vars])\nprint('xgb test roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))",
        "text": "the scaler be now ready , we can use it in a machine learn algorithm when require  .  see below   -  machine learn algorithm build    -  xgboost",
        "id": 688
    },
    {
        "code": "\ndef webData_to_DF(system_datafram, dict_of_webData):\n    for ea in dict_of_webData.keys():\n        usr_lines = list(filter(lambda a: a != 0, dict_of_webData[ea][1:]))\n        addProfile(system_datafram, ea, dict_of_webData[ea][0], usr_lines)\nplay_dict = get_textUsrData()\nfor ea in play_dict.keys(): print(play_dict[ea][1:])\nfor ea in play_dict.keys(): print(list(filter(lambda a: a != 0, play_dict[ea][1:])))\nwebData_to_DF(data,play_dict)",
        "text": "below function take data dict from web or text file and add it to exist system dataframe",
        "id": 689
    },
    {
        "code": "\nplt.plot('YearOnly','Horsepower', data= hp_year, \n            marker='o'\n           );\nplt.title('Horsepower Over Time')\nplt.xlabel('Year')\nplt.ylabel('Average Horsepower')",
        "text": "use matplotlib to create a line chart of average horsepower ( y ) versus year ( x ) , * out both line and point on your chart   -  use the data  keyword argument   -  label the x   -   and y   -   ax and plot title   -  use x and y ax gridlines  . ",
        "id": 690
    },
    {
        "code": "Shearlab.imageplot(real(Array(coeffs_nopar[:,:,1])))\nShearlab.imageplot(real(Array(coeffs_nopar[:,:,5])))\nShearlab.imageplot(real(Array(coeffs_nopar[:,:,10])))\nShearlab.imageplot(real(Array(coeffs_nopar[:,:,16])))",
        "text": "lets look at the shearlets coefficient at different scale   - ",
        "id": 691
    },
    {
        "code": "mu_BOS = trace['mu_BOS']\nplot_cdf(mu_BOS, label='mu_BOS posterior')\nmu_ANA = trace['mu_ANA']\nplot_cdf(mu_ANA, label='mu_ANA posterior')\ndecorate_cdf_rates()\nnp.mean(mu_BOS), np.mean(mu_ANA)\nplt.savefig('zigzag14.png', dpi=150)",
        "text": "here be the posterior distribitions for mu _ bos  and mu _ ana   . ",
        "id": 692
    },
    {
        "code": "response = requests.get('https://api.forecast.io/forecast/4da699cf85f9706ce50848a7e59591b7/25.761680,-80.191790, 2016-06-09T12:01:00-0400')\ndata = response.json()\nTem = data['hourly']['data']\ncount = 0\nfor i in Tem:\n    count = count +1\n    print(\"The temperature in Miami, Florida on 9th June in the\", count, \"hour is\", i['temperature'])\n    if float(i['cloudCover']) > 0.5:\n        print(\"and is cloudy\")",
        "text": "weather in florida what 's the weather look like for the rest of today in miami , florida ? i d like to know the temperature for every hour , and if it 's go to have cloud cover of more than 0 . 5 say  { temperature } and cloudy  instead of just the temperature  . ",
        "id": 693
    },
    {
        "code": "xarr = np.arange(1.1,1.6,0.1)\nyarr = np.arange(2.1,2.6,0.1)\ncond = np.array([True,False,True,True,False])\nresult = [(x if c else y) for x,y,c in zip(xarr,yarr,cond)] \nresult = np.where(cond,xarr,yarr)\nresult\narr = np.random.randn(4,4)\nnp.where(arr > 0, 2, arr)",
        "text": "conditional logic in array operation",
        "id": 694
    },
    {
        "code": "testingList = [1,1,1,1,1,1,1,1,1,1,2]\nprintSortResults(testingList, True)",
        "text": "all be the same value except one",
        "id": 695
    },
    {
        "code": "1 < 2 < 3",
        "text": "chain comparison operator an interest feature of python be the ability to *chain* multiple comparison to perform a more complex test  .  you can use these chain comparison a shorthand for large boolean expression  .  comparison operator ,   -  and  -  and   -  or  - ",
        "id": 696
    },
    {
        "code": "M[:,1] \nM[1,:] #row 1",
        "text": "the same thing can be achieve with use , instead of an index ,",
        "id": 697
    },
    {
        "code": "\nrange(0,10) \nrange(20) \nx = range(0,10)\nx\ntype(x)\nstart = 5\nstop = 20\nrange(start,stop)\nstart = 2\nstop = 20\nstep = 2\nrange(start, stop, step)",
        "text": "range ( ) allow u to create a list of number range from a start point up to an end point  .  we can also specify step size  . ",
        "id": 698
    },
    {
        "code": "e_approximation(3)\nassert e_approximation(0) == 0.0\nassert e_approximation(1) == 1.0\nassert e_approximation(2) == 2.0\nassert e_approximation(3) == 5 / 2\nassert e_approximation(10) == sum(1 / factorial(i) for i in range(10))\nassert e_approximation(100) == sum(1 / factorial(i) for i in range(100))\nassert e_approximation.__doc__ != None",
        "text": "the approximation obtain a sum of the first $ 3 $ term of the series be $ 2 . 5   -  verify that this be the answer you get ,",
        "id": 699
    },
    {
        "code": "def my_squared_function(x):\n    return x**2\ndef my_function_that_calls_another(value, func):\n    return func(value)\nmy_function_that_calls_another(5, my_squared_function)",
        "text": "when pass function a argument you need to pas the name of the function without the parenthesis  . ",
        "id": 700
    },
    {
        "code": "test_stat_dist = np.array([test_stat(run_model(data)) \n                           for i in range(1000)])\nnp.mean(test_stat_dist)",
        "text": "that 's the result of one simulate experiment  .  we can run the experiment 1000 time and collect the result  . ",
        "id": 701
    },
    {
        "code": "def get_tensors(loaded_graph):\n    InputTensor = loaded_graph.get_tensor_by_name(\"input:0\")\n    InitialStateTensor = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n    FinalStateTensor = loaded_graph.get_tensor_by_name(\"final_state:0\")\n    ProbsTensor = loaded_graph.get_tensor_by_name(\"probs:0\")\n    return (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\ntests.test_get_tensors(get_tensors)",
        "text": "implement generate function    -  get tensor get tensor from loaded _ graph  use the function [ get _ tensor _ by _ name ( )  ] ( <url> )  .  get the tensor use the follow name ,   -    input,0    -    initial _ state,0    -    final _ state,0    -    probs,0",
        "id": 702
    },
    {
        "code": "X = np.linspace(0, 1, 10240)\nnoise = np.random.normal(0, .1, X.shape)\nf_real = np.vstack([np.sin(i ** 2 * np.pi * X) for i in range(10)])\nnoise = np.vstack([np.random.normal(0, .1 * (i + 1), X.shape) for i in range(10)])\nf_noisy = f_real + noise",
        "text": "good simulation 1 , small additive gaussian noise on a smooth function    -  generate data",
        "id": 703
    },
    {
        "code": "R1 = R1 - 2*R2",
        "text": "eliminate $ y $ from $ r _ 1 $ ( solve for $ x $ )",
        "id": 704
    },
    {
        "code": "def build_rnn(cell, inputs):\n    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n    final_state = tf.identity(final_state, name= 'final_state')\n    return outputs, final_state\ntests.test_build_rnn(build_rnn)",
        "text": "build rnn i create a rnn cell in the get _ init _ cell ( )  function  .  time to use the cell to create a rnn   -  build the rnn use the [ tf . nn . dynamic _ rnn ( )  ] ( <url> )   -   apply the name  final _ state  to the final state use [ tf . identity ( )  ] ( <url> )",
        "id": 705
    },
    {
        "code": "def build_heatmap(z, synset):\n    class_ids = imagenet_tool.synset_to_dfs_ids(synset)\n    class_ids = np.array([id_ for id_ in class_ids if id_ is not None])\n    x = z[0, :, :, class_ids].sum(axis=0)\n    print(\"size of heatmap: \" + str(x.shape))\n    return x\ndef display_img_and_heatmap(img_path, heatmap):\n    dog = imread(img_path)\n    fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(12, 8))\n    ax0.imshow(dog)\n    ax0.axis('off')\n    ax1.imshow(heatmap, interpolation='nearest', cmap=\"viridis\")\n    ax1.axis('off')",
        "text": "unsupervised heatmap of the class  dog  the follow function build a heatmap from a forward pas  .  it sum the representation for all id correspond to a synset",
        "id": 706
    },
    {
        "code": "\nmodel = Sequential()\nmodel.add(LSTM(4, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)",
        "text": "we will now design and fit our lstm network  .  the network ha a visible layer with 1 input , a hide layer with 4 lstm block or neuron , and an output layer that make a single value prediction  .  the default sigmoid activation function be use for the lstm block  .  the network be train for 20 epoch and a batch size of 1 be use  . ",
        "id": 707
    },
    {
        "code": "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : 1 if x == 0 else -1)\nloans = loans.remove_column('bad_loans')",
        "text": "like the previous assignment , we reassign the label to have 1 for a safe loan , and   -   1 for a risky ( bad ) loan  . ",
        "id": 708
    },
    {
        "code": "squares = [x ** 2 for x in nums]\nprint(squares)\neven_squares = [x**2 for x in nums if x % 2 == 0]\nprint(even_squares)",
        "text": "you can make this code simple use a list comprehension",
        "id": 709
    },
    {
        "code": "x = linspace(0,2*pi)\nsin(x)",
        "text": "linspace  -  be an easy way to make coordinate for plot  .  function in the numpy library ( all of which be import into ipython notebook ) can act on an entire vector ( or even a matrix ) of point at once  .  thus ,",
        "id": 710
    },
    {
        "code": "min_price = min(zip(stock_prices.values(), stock_prices.keys()))\nprint(min_price)\nmax_price = max(zip(stock_prices.values(), stock_prices.keys()))\nprint(max_price)\nsorted_prices = sorted(zip(stock_prices.values(), stock_prices.keys()))\nprint(sorted_prices)",
        "text": "in order to perform useful calculation on the dictionary content , it be often useful to invert the key and value of the dictionary use zip ( )  .  let 's see how to find the minimum and maximum price and stock name ,",
        "id": 711
    },
    {
        "code": "class Critter(object):\n    def __init__(self, name):\n        print(\"A new critter has been born!\")\n        self.name = name\n    def __str__(self):\n        rep = \"Critter object\\n\"\n        rep += \"name: \" + self.name + \"\\n\"\n        return rep\n    def talk(self):\n        print(\"Hi. I'm\", self.name, \"\\n\")\ncrit1 = Critter(\"Poochie\")\ncrit1.talk()\ncrit2 = Critter(\"Randolph\")\ncrit2.talk()\nprint(crit1)\nprint(crit2)",
        "text": "the attribute critter program create a new type of object with an attribute , name  .  the critter class ha a constructor method that create and initialize name  .  the program us the new attribute so that the critter can offer a more personalize greet  . ",
        "id": 712
    },
    {
        "code": "n = 10\nplt.figure(figsize=(20, 2))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(x_test_noisy[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()",
        "text": "here 's how the noisy digit look like ,",
        "id": 713
    },
    {
        "code": "from h2o.estimators.deepwater import H2ODeepWaterEstimator\nmodel_mnist_mylenet_keras = H2ODeepWaterEstimator(epochs=80,\n                                            network_definition_file=network_def_path,\n                                            backend=\"tensorflow\",\n                                            image_shape=[28,28],\n                                            channels=1, model_id=\"model_mnist_mylenet_keras\")\nmodel_mnist_mylenet_keras.train(x=[\"uri\"], y=\"label\", training_frame=mnist_training, validation_frame=mnist_testing)\nmodel_mnist_mylenet_keras.show()",
        "text": "build deep water kera ( tensorflow ) model",
        "id": 714
    },
    {
        "code": "ser_2 = Series([-1,  2, -3,  4, -5,  6], index=['a', 'b', 'c', 'd', 'e', 'f'])\nser_2",
        "text": "we can create a series with a custom index ,",
        "id": 715
    },
    {
        "code": "from keras.datasets import cifar10\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('X_train shape : ', x_train.shape)\nprint(x_train.shape[0], ' train samples')",
        "text": "load prepare data",
        "id": 716
    },
    {
        "code": "t1 = time.time()\ntrain_data['text'] = train_data['text'].apply(clean_text)\nprint(\"Finished cleaning the train set.\", \"Time needed:\", time.time()-t1,\"sec\")\nt2 = time.time()\ntest_data['text'] = test_data['text'].apply(clean_text)\nprint(\"Finished cleaning the test set.\", \"Time needed:\", time.time()-t2,\"sec\")",
        "text": "clean train and test data  - ",
        "id": 717
    },
    {
        "code": "month_abbr = list(calendar.month_abbr)\ngroup_key = interest[\"X15\"].map(lambda x: month_abbr.index(x.split(\"-\")[0]))\nstd = interest[\"X1\"].groupby(group_key).apply(np.std)\ninterest[\"X1\"].groupby(group_key).apply(np.mean).plot(kind=\"bar\", yerr=std, alpha=0.7)\nplt.xlabel(\"X15:Month\")\nplt.ylabel(\"Mean interest rate\")\nplt.show()",
        "text": "project x15 to month , and see if correlate with x1 ,   - ",
        "id": 718
    },
    {
        "code": "long = -59.533333\nlat = 13.183333\nfor i in df[df.Country == 'Barbados'].index:\n    df.loc[i,'Centroid_Longitude'] = long\n    df.loc[i,'Centroid_Latitude'] = lat\ndf[df.Country == 'Barbados'].head(1)",
        "text": "barbados     source    -  url>",
        "id": 719
    },
    {
        "code": "def update(beliefs, i, outcome):\n    beliefs[i].Update(outcome)",
        "text": "this function update our belief about one of the machine base on one outcome  . ",
        "id": 720
    },
    {
        "code": "\nbook_result = data['results']\nprint(\"The hardcover Fiction NYT best-sellers on mothers day in 2010 are:\")\nfor i in book_result:\n    for item in i['book_details']:\n        print(\"-\", item['title'])",
        "text": "what book top the hardcover fiction nyt best   -   seller list on mother 's day in 2009 and 2010 ? how about father 's day ?",
        "id": 721
    },
    {
        "code": "obj1 = FirstClass()\nobj2 = FirstClass()\nobj1.x = 5\nobj2.x = 6\nprint(\"x in obj1 =\",obj1.x,\"and x in obj2 =\",obj2.x)",
        "text": "object ( instance of a class ) can hold data  .  a variable in an object be also call a field or an attribute  .  to access a field use the notation object . field   .  for example ,",
        "id": 722
    },
    {
        "code": "algorithm = alg.NSGAIII( dpp, 100, log_frequency=100 )\nalgorithm.run(1000)",
        "text": "set an optimization algorithm on the differential privacy problem and run it  . ",
        "id": 723
    },
    {
        "code": "_ = plot_signal_and_fft(flip(np.conj(x)), show=[1,2], signal_label='Flip$(\\\\bar{x})$', spectrum_label='$DFT($Flip$(\\\\bar{x}))$')\n_ = plot_signal_and_fft(spectrum=X, show=[2], spectrum_operator=np.conj, spectrum_label='$\\\\bar{X}$')",
        "text": "theorem ,   -  for any $ x \\in \\mathbb { c } ^n $ , $ \\fbox { flip $ ( \\overline { x } ) \\longleftrightarrow \\overline { x } $ }   - ",
        "id": 724
    },
    {
        "code": "\ndiff_young_coffee = []\nfor _ in range(int(1e4)):\n    boot = sample_data.sample(200, replace=True)\n    mean_coff_young = boot[(boot['age'] == '<21') & (boot['drinks_coffee'] == True)]['height'].mean()\n    mean_nocoff_young = boot[(boot['age'] == '<21') & (boot['drinks_coffee'] == False)]['height'].mean()\n    diff_young_coffee.append(mean_coff_young - mean_nocoff_young)\nplt.hist(diff_young_coffee);\nnp.percentile(diff_young_coffee, 2.5), np.percentile(diff_young_coffee, 97.5)",
        "text": "for 10,000 iteration bootstrap your sample data , compute the   -  difference  -  in the average height for coffee drinker and the average height for non   -   coffee drinker for individual   -  under  -  21 year old  .  use your sample distribution , build a 95 % confidence interval  .  use your interval to start answer question 2 below  . ",
        "id": 725
    },
    {
        "code": "g = sns.FacetGrid(titanic, col=\"Survived\", row=\"Pclass\") \ng.map(sns.kdeplot, \"Age\", shade=True)\nsns.despine(left=True, bottom=True)\nplt.show()",
        "text": "create conditional plot use two condition",
        "id": 726
    },
    {
        "code": "plot_decision_boundary(X, Z, W=W, b=b)",
        "text": "decision boundary on the train set",
        "id": 727
    },
    {
        "code": "bat_df['avg'] = bat_df['H']/bat_df['AB']\nbat_df[(bat_df['yearID']==2013) & (bat_df['PA']>=300)].nlargest(8,'avg')",
        "text": "top 8 high average in 2013 with at least 300 pa ?",
        "id": 728
    },
    {
        "code": "Manish_wb1=json.load((open('data/world_bank_projects.json')))\nManish_wb1=json_normalize(Manish_wb1,['mjtheme_namecode'])\nManish_wb2=Manish_wb1.groupby(['code'])['code'].count()\nManish_wb2\nManish_wb3=Manish_wb1.groupby([\"code\", \"name\"]).size().reset_index()\nManish_wb3\nManish_wb2.sort_values(ascending=False).head(10)",
        "text": "find the top 10 major project theme ( use column mjtheme _ namecode  )",
        "id": 729
    },
    {
        "code": "record = ('ACME', 50, 123.45, (12, 18, 2012))\nname, *_, (*_,year) = record\nname\nyear",
        "text": "sometimes you might want to unpack value and throw them away  .  you can  t just specify a bare * when unpack , but you could use a common throwaway variable name , such a  _  or ign ( ignore )  . ",
        "id": 730
    },
    {
        "code": "[3,'LMU']  # 'integer' and 'string' type objects are in a same 'list'-type object",
        "text": "list  of multiple object ( of different type )",
        "id": 731
    },
    {
        "code": "import re\ndef ReadVariables():\n    vars1 = thinkstats2.ReadStataDct('2002FemPreg.dct').variables\n    vars2 = thinkstats2.ReadStataDct('2002FemResp.dct').variables\n    all_vars = vars1.append(vars2)\n    all_vars.index = all_vars.name\n    return all_vars\ndef MiningReport(variables, n=30):\n    all_vars = ReadVariables()\n    variables.sort(reverse=True)\n    for r2, name in variables[:n]:\n        key = re.sub('_r$', '', name)\n        try:\n            desc = all_vars.loc[key].desc\n            if isinstance(desc, pd.Series):\n                desc = desc[0]\n            print(name, r2, desc)\n        except KeyError:\n            print(name, r2)",
        "text": "the follow function report the variable with the high value of $ r^2   - ",
        "id": 732
    },
    {
        "code": "\nimport threading\nimport time\ndef worker():\n    print(threading.current_thread().getName(), 'Starting')\n    time.sleep(0.2)\n    print(threading.current_thread().getName(), 'Exiting')\ndef my_service():\n    print(threading.current_thread().getName(), 'Starting')\n    time.sleep(0.3)\n    print(threading.current_thread().getName(), 'Exiting')\nt = threading.Thread(name='my_service', target=my_service)\nw = threading.Thread(name='worker', target=worker)\nw2 = threading.Thread(target=worker)  \nw.start()\nw2.start()\nt.start()",
        "text": "determine the current thread use argument to identify or name the thread be cumbersome and unnecessary  .  each thread instance ha a name with a default value that can be change a the thread be create  .  name thread be useful in server process with multiple service thread handle different operation  .  the debug output include the name of the current thread on each line  . ",
        "id": 733
    },
    {
        "code": "filename = '../QBdata/forwardModelTestValues.mat'\nsiops_data = loadmat(filename, squeeze_me=True)\nprint(siops_data.keys())\nfilename = resource_filename(\n            sb.__name__,\n            'tests/data/test_resample.mat')\nsensor_data = loadmat(filename, squeeze_me=True)\nprint()\nprint(sensor_data.keys())\nprint()\nprint(sensor_data['filt'].shape)\nfilename = resource_filename(sb.__name__, 'tests/data/test_error_noise.mat')\nnoise_data = loadmat(filename, squeeze_me=True)",
        "text": "load the sambuca model input ( siops )",
        "id": 734
    },
    {
        "code": "pop_Brand=[]\nfor x in product:\n    try:\n        y=x.replace(\"'\",'\"')\n        jdata=json.loads(y)\n        if jdata['asin'] in list_Pack2_5:\n            pop_Brand.append(jdata['brand'])  \n    except:\n        pass",
        "text": "get the brand name of those asin which be present in the list list _ pack2 _ 5   . ",
        "id": 735
    },
    {
        "code": "hist_10_bins = plot_histogram(df=local, bins=10)\nassert_is_instance(hist_10_bins, mpl.axes.Axes)\npatches = hist_10_bins.patches\nassert_equal(len(patches), 10)\nbins_a = [25.,  31.,  37.,  43.,  49.,  55.,  61.,  67.,  73.,  79.,  85.]\ncounts_a = [550, 792, 245,  61,  15,   7,   4,   4,   0,   3]\n                    \nfor i in range(len(patches)):\n    assert_equal(patches[i].get_x(), bins_a[i])\n    assert_equal(patches[i].get_height(), counts_a[i])",
        "text": "a histogram with 10 bin  . ",
        "id": 736
    },
    {
        "code": "plt.figure(figsize=(10,10))\ncreate_shrinkage_plot_using_rid('dn0yw0WPTsH8P0SAstDQWw')",
        "text": "plot your estimate for the θ 's against the value in the  mean  column ( correspond to this restaurant   -   few review ,",
        "id": 737
    },
    {
        "code": "\ntreeReg3 = DecisionTreeRegressor(max_depth=3)\nscores = cross_val_score(treeReg3, X, y, cv=3, scoring=\"mean_squared_error\")\nprint(np.mean(np.sqrt(-scores)))\ntreeReg4 = DecisionTreeRegressor(max_depth=4)\nscores = cross_val_score(treeReg4, X, y, cv=3, scoring=\"mean_squared_error\")\nprint(np.mean(np.sqrt(-scores)))\ntreeReg3.fit(X, y)\npd.DataFrame({'feature':feature_cols, 'importance':treeReg3.feature_importances_})",
        "text": "scikit report mean square error a negative  .  some debate/discussions on why the score be negative , <url>",
        "id": 738
    },
    {
        "code": "\nr\"\\$(\\d+) ?(\\w+)\"\nre.findall(r\"\\$(\\d+) ?(\\w+)\", all_subjects)\nre.findall(r\"\\$(\\d+) ?([bBmM])\", all_subjects)\nvals = []\nfor item in re.findall(r\"\\$(\\d+) ?([bBmM])\", all_subjects):\n    multiplier = item[1].lower()\n    number_val = int(item[0])\n    if multiplier == 'k':\n        number_val *= 1000\n    elif multiplier == 'm':\n        number_val *= 1000000\n    elif multiplier == 'b':\n        number_val *= 1000000000\n    vals.append(number_val)\nsum(vals)",
        "text": "find monetary amount in the subject line match something like $ 10 m , k , b",
        "id": 739
    },
    {
        "code": "dfbystd = olives.groupby('region').std()\ndfbystd.head()\ndfbymean = region_groupby.aggregate(mean)\ndfbymean.head()",
        "text": "or one might let panda take care of concatenate the series obtain by run std on each dataframe back into a dataframe for u  .  notice that the output dataframe be automatically index by region for u  . ",
        "id": 740
    },
    {
        "code": "def lasso_plot_runner(alpha=0):\n    coef_plotter(l_alphas, l_coefs, simple_feature_names, alpha, regtype='lasso')\ninteract(lasso_plot_runner, alpha=(0.0,0.2,0.0025))",
        "text": "run the same plot function above , but now with the calculate coefficient by alpha for the lasso  . ",
        "id": 741
    },
    {
        "code": "dimensions = { 'longitude': {'range': (149.06, 149.17)},   \n               'latitude':  {'range': (-35.33, -35.27)},\n               'time':      {'range': ((2013, 1, 1), (2014, 1, 1))} }\nlake_str = \"Lake_Burley_Griffin\"        \nlake_dispname = \"Lake Burley Griffin\"   \nwater_mask_thr = 90.0     \nwater_mask_buffer = 1.5   \nmin_valid_pix_thr = 10.0  \nsave_basedir = '/g/data/jr4/vis_data_v1.1/'   \nWQ_type = \"WQ_(B2+B3)/2\"  # Type of WQ info generated by this code",
        "text": "user input here be the input parameter need from the user ,",
        "id": 742
    },
    {
        "code": "\ndf_train=pd.get_dummies(df_train)\ndf_train.head()\ndf_train.describe",
        "text": "last but not the least , dummy variable",
        "id": 743
    },
    {
        "code": "import statsmodels.api as sm\nfrom sklearn.metrics import mean_absolute_error\nmodel = sm.tsa.ARIMA(train, (1, 0, 0)).fit()\npredictions = model.predict(\n    \"2012-02-27\",\n    \"2012-10-29\",\n    dynamic = True)\nprint(\"Mean absolute error: %f\\n\" % mean_absolute_error(test, predictions))\nprint(model.summary())",
        "text": "create an ar ( 1 ) model on the train data and compute the mean absolute error of the prediction",
        "id": 744
    },
    {
        "code": "trainer = NaiveBayesClassifier.train\nclassifier = sentim_analyzer.train(trainer, training_set)\nevaluate = sentim_analyzer.evaluate(test_set)\nfor key,value in sorted(evaluate.items()):\n    print('{0}: {1}'.format(key, value))",
        "text": "we can now train the classifier on the train set , and subsequently output the evaluation result ,",
        "id": 745
    },
    {
        "code": "dic={\"treatment\":[\"A\",\"A\",\"B\",\"B\"],\"gender\":[\"F\",\"M\",\"F\",\"M\"],\"response\":[10,45,5,9],\"age\":[15,4,72,65]}\ndf=pd.DataFrame(dic)\ndf\ndf.pivot(index=\"treatment\",columns=\"gender\",values=\"response\")\ndf.pivot(index=\"treatment\",columns='gender',values=\"age\")",
        "text": "pivot data frame pivot through the reshape tool",
        "id": 746
    },
    {
        "code": "\nX = employee.drop(['Attrition','EmployeeCount','EmployeeNumber','Over18','StandardHours',\n                   'HourlyRate','DailyRate','MonthlyRate'],axis=1)\ny = employee['Attrition']\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 2)",
        "text": "train and test set split",
        "id": 747
    },
    {
        "code": "titanic.loc[((titanic.who == \"man\") | (titanic.who == \"woman\")) & (titanic.age < 18), \"who\"].head()\ntitanic.loc[((titanic.who == \"man\") | (titanic.who == \"woman\")) & (titanic.age < 18), \"who\"] = \"child\"\ntitanic.age.groupby(titanic.who).describe()",
        "text": "there be 30 passenger should be classify a man and woman respectively  .  now we can replace the value of < font color=  blue  > who   for the above record to  child  , give the assumption we make  .  change the value of < font color=  blue  > who   to  child  for the row you find above  . ",
        "id": 748
    },
    {
        "code": "sns.set_palette(\"GnBu_d\")\nsns.set_style('whitegrid')\nsns.jointplot(x='Time on Website',y='Yearly Amount Spent',data=customers)\nsns.jointplot(x='Time on App',y='Yearly Amount Spent',data=customers)\nsns.pairplot(customers)",
        "text": "exploratory data analysis let 's explore the data  .  for the rest of the exercise we ll only be use the numerical data of the csv file  .  use seaborn to create a jointplot to compare the time on website and yearly amount spend column  .  doe the correlation make sense ?",
        "id": 749
    },
    {
        "code": "from collections import Counter\ncounts = Counter(words)\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\nreviews_ints = []\nfor each in reviews:\n    reviews_ints.append([vocab_to_int[word] for word in each.split()])\nprint(reviews_ints[0])",
        "text": "encode the word the embed lookup require that we pas in integer to our network  .  the easy way to do this be to create dictionary that map the word in the vocabulary to integer  .  then we can convert each of our review into integer so they can be pass into the network  . ",
        "id": 750
    },
    {
        "code": "\nyhat_ts = svc.predict(Xts)\nacc = np.mean(yhat_ts == yts)\nprint('accuracy = {0:f}'.format(acc))",
        "text": "measure the accuracy on the test sample  .  you should get about 96 % accuracy  .  you can get good by use more train sample , but it will just take long to run  . ",
        "id": 751
    },
    {
        "code": "print(top3_val)\nf,plts=plt.subplots(7,4)\nfor i in range(len(myfiles)):\n    plts[i,0].imshow(imlist[i])\n    for j in range(0,3):\n        s=np.where(y_train==top3_val[1][i][j])\n        plts[i,j+1].imshow(X_train[s[0][0]].squeeze(),cmap='gray')\n        \n        #plts[i,j+2].set_title(top3_val[0][i][j])",
        "text": "output top 5 softmax probability for each image find on the web",
        "id": 752
    },
    {
        "code": "mydict ={k:v for (k,v) in zip(alist, asquaredlist)}\nmydict",
        "text": "the key do not have to be string  .  from python 2 . 7 you can use dictionary comprehension a well",
        "id": 753
    },
    {
        "code": "RSS = np.sum((Y_test - test_predict) ** 2)\nprint(RSS)\nESS = np.sum(test_predict - np.mean(Y_test)) ** 2\nprint(ESS)\nR_squared = ESS/(ESS+RSS)\nprint(RSS)",
        "text": "mse   -   msr > use test data",
        "id": 754
    },
    {
        "code": "\naggregation = {\n    'title_1':{\n        'failing_scores1':lambda x:len(x)-sum(x)\n        },\n    'title_2':{\n        'failing_scores2':lambda x:len(x)-sum(x)\n    }\n}\nfailed_title2 = df.groupby('division').agg(aggregation)\ndifference = failed_title2['title_2']['failing_scores2']-failed_title2['title_1']['failing_scores1']\ndifference.order(ascending=False)",
        "text": "look at fail score by division < a id=  t2 _ scoresbydiv  >",
        "id": 755
    },
    {
        "code": "\nhuman_dog_faces = 0\nfor human in human_files_short:\n    if dog_detector(human):\n        human_dog_faces += 1\ndog_faces = 0\nfor dog in dog_files_short:\n    if dog_detector(dog):\n        dog_faces += 1\nprint(\"There were %d percent of human faces detected as dogs.\" % ((human_dog_faces / len(human_files_short) * 100)))        \nprint(\"There were %d percent of dog faces detected.\" % ((dog_faces / len(dog_files_short) * 100)))",
        "text": "( implementation ) ass the dog detector  _  _ question 3 ,  _  _  use the code cell below to test the performance of your dog _ detector  function   -  what percentage of the image in human _ files _ short  have a detect dog ?   -   what percentage of the image in dog _ files _ short  have a detect dog ?  _  _ answer ,  _  _ ",
        "id": 756
    },
    {
        "code": "\nfig = plt.figure(figsize = (15,20))\nfor i,name in enumerate(skewed):\n    ax = fig.add_subplot(5,3,i+1)\n    ax.hist(features_trs[name],bins = 100)\n    ax.set_title(name)\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"bottom\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"left\"].set_visible(False)\nfig.tight_layout()\nplt.savefig('transformed.PNG')\nplt.show()",
        "text": "visualization of distribution of each feature after transformation",
        "id": 757
    },
    {
        "code": "\nts['1949-01-01':'1949-05-01']\nts['1949-01-01':'1949-05-01']\nts[:'1949-05-01']",
        "text": "both would return the value  112  which can also be confirm from previous output  .  suppose we want all the data upto may 1949 .  this can be do in 2 way ,",
        "id": 758
    },
    {
        "code": "def normalize(problem, points):\n  \n  meta = problem.objectives\n  all_objs = []\n  for point in points:\n    objs = []\n    for i, o in enumerate(problem.evaluate(point)):\n      low, high = meta[i].low, meta[i].high\n      \n      if high == low: \n        objs.append(0)\n        continue\n      n_val = (o - low) / (high - low)\n      objs.append(n_val)\n    all_objs.append(objs)\n  return all_objs",
        "text": "to compute most measure , data ( i . e objective ) be normalize  .  normalization be scale the data between 0 and 1 .  why do we normalize ? todo2 , it be easy to compare two value with different range/ measurement if the value be normalize  . ",
        "id": 759
    },
    {
        "code": "\nfrom cryptocompy import price\nimport pandas as pd\nxrp = price.get_historical_data('XRP', 'KRW', 'minute', e='Bithumb',aggregate=1, limit=2000)\nbtc = ...\neth = ...\ndf_XRP = pd.DataFrame(XRP)\ndf_ETH = ...\ndf_BTC = ...\ndf_combined = pd.concat([ETH.time,df_XRP.XRP,df_ETH.ETH,df_BTC.BTC],axis=1)\nprint(df_combined.head())\ndf_combined.to_csv(filename, sep='\\t', header=False, index=False)",
        "text": "quiz 3    -  crawl crypto data ( xrp , btc , eth ) from bithumb    -  use dataframe to make neat data",
        "id": 760
    },
    {
        "code": "def testGenerator(images, rows, cols, epoch, start):\n    noise = np.random.normal(0,1, [images,100])\n    out = gModel.predict(noise)\n    fig = plt.figure(figsize=(8,8))\n    for i in range(images):\n        plt.subplot(rows,cols,i+1)\n        plt_digit((out[i,:]*2 +1)/2)\n        \n    fig.savefig(str(start+epoch)+'.png')",
        "text": "function to test generator it also save the intermediate result",
        "id": 761
    },
    {
        "code": "vect = TfidfVectorizer(min_df=7,max_df=0.90,ngram_range=(2,5),analyzer='char_wb').fit(X_train)\nX_train_transform = vect.transform(X_train)\nX_val_transform = vect.transform(X_val)",
        "text": "build the feature use tf   -   idf vectorizer",
        "id": 762
    },
    {
        "code": "s=a[:,1:3]\ns\nc\ns[:]=10\ns\na",
        "text": "slice an array return a view of it ,",
        "id": 763
    },
    {
        "code": "from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state=42)\ndt = dt.fit(X_train, y_train)",
        "text": "fit a decision tree classifier with no set limit on maximum depth , feature , or leaf   -  determine how many node be present and what the depth of this ( very large ) tree be   -  use this tree , measure the prediction error in the train and test data set  .  what do you think be go on here base on the difference in prediction error ?",
        "id": 764
    },
    {
        "code": "def sentence_to_seq(sentence, vocab_to_int):\n    \n    lower_sentence = sentence.lower()\n    \n    sentence_ids = [vocab_to_int[word] if word in vocab_to_int else vocab_to_int['<UNK>'] for word in sentence.split()]\n    return sentence_ids",
        "text": "sentence to sequence fee a sentence into the model for translation , first need to preprocess it  .  implement the function sentence _ to _ seq ( )  to preprocess new sentence   -  convert the sentence to lowercase   -   convert word into id use vocab _ to _ int   -   convert word not in the vocabulary , to the     word id  . ",
        "id": 765
    },
    {
        "code": "dimensions = { 'longitude': {'range': (149.06, 149.17)},   \n               'latitude':  {'range': (-35.33, -35.27)},\n               'time':      { 'range': ((1992, 1, 1), (1997, 1, 1))} }\nlake_str = \"Lake_Burley_Griffin\"        \nlake_dispname = \"Lake Burley Griffin\"   \nwater_mask_thr = 90.0     \nwater_mask_buffer = 1.5   \nmin_valid_pix_thr = 10.0  \nsave_basedir = '/g/data/jr4/vis_data_v1.0/'   \nWQ_type = \"WQ_(B2+B3)/2\"  # Type of WQ info generated by this code",
        "text": "user input here be the input parameter need from the user ,",
        "id": 766
    },
    {
        "code": "import json\nwith open('c://users/evgeniy.pahnuk/Desktop/Coursera/Classification/week2/important_words.json', 'r') as f: \n    important_words = json.load(f)\nimportant_words = [str(s) for s in important_words]\ndef remove_punctuation(text):\n    import string\n    new_text = text\n    for i in string.punctuation:\n        new_text = new_text.str.replace(i, '')\n    return new_text\nproducts['review_clean'] = remove_punctuation(products['review'])\nfor word in important_words:\n    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))\nproducts",
        "text": "apply text clean on the review data",
        "id": 767
    },
    {
        "code": "\nser_counts = df_raw_data_amt[df_raw_data_amt['s_model_type'] == '04_weeks']['id'].value_counts() -1\nfig_count_num_expl_vars = ser_counts.value_counts().sort_index().plot(kind='bar', title = 'Number of historical flights'\n                                                                      + '\\nfor 4 week model type')\nfig_count_num_expl_vars.set_xlabel(\"Num Historical Flights\")\nfig_count_num_expl_vars.set_ylabel(\"Num Target Cases\")\nprint(fig_count_num_expl_vars)",
        "text": "four week model type",
        "id": 768
    },
    {
        "code": "my_lists + ['new intem']\nmy_lists\nmy_lists = my_lists + ['permanent add']\nmy_lists\nmy_lists * 2\nmy_lists",
        "text": "we can also use + to concatenate list",
        "id": 769
    },
    {
        "code": "\ntest_data = data[-21*24:]\ndata = data[:-21*24]\ntarget_fields = ['cnt', 'casual', 'registered']\nfeatures, targets = data.drop(target_fields, axis=1), data[target_fields]\ntest_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]\ntrain_features, train_targets = features[:-60*24], targets[:-60*24]\nval_features, val_targets = features[-60*24:], targets[-60*24:]",
        "text": "split the data into train , test , and validation set",
        "id": 770
    },
    {
        "code": "valgen = ImageDataGenerator(preprocessing_function=preprocess_function)\nval_flow = valgen.flow_from_directory(\n    validation_folder, batch_size=batch_size, target_size=(224, 224),\n    shuffle=False, class_mode='binary')\nall_correct = []\nfor i, (X, y) in zip(range(val_flow.n // batch_size), val_flow):\n    predictions = model.predict(X).ravel()\n    correct = list((predictions > 0.5) == y)\n    all_correct.extend(correct)\n    print(\"Processed %d images\" % len(all_correct))\nprint(\"Validation accuracy: %0.4f\" % np.mean(all_correct))",
        "text": "let 's compute the validation score on the full validation set ,",
        "id": 771
    },
    {
        "code": "fortune = pd.read_csv(\"c:/Pallav/pandas/fortune1000.csv\" ,index_col = \"Rank\")\nsectors = fortune.groupby(\"Sector\")\nfortune.head()\nsectors.max()\nsectors.min()  \nsectors.sum()\nsectors.get_group(\"Media\")[\"Revenue\"].sum()\nsectors.get_group(\"Media\")[\"Profits\"].sum()\nsectors.mean()\nsectors[\"Revenue\"].sum()\nsectors[\"Employees\"].sum()\nsectors[\"Profits\"].max()\nsectors[\"Profits\"].min()\nsectors[\"Employees\"].mean()\nsectors[[\"Revenue\", \"Profits\"]].sum().head()",
        "text": "method on the groupby object and dataframe column",
        "id": 772
    },
    {
        "code": "from h2o.estimators.deepwater import H2ODeepWaterEstimator\nmodel_mnist_mymodel_tf = H2ODeepWaterEstimator(epochs=80,\n                                            network_definition_file=network_def_path,\n                                            backend=\"tensorflow\",\n                                            image_shape=[28,28],\n                                            channels=1, model_id=\"model_mnist_mymodel_tf\")\nmodel_mnist_mymodel_tf.train(x=[\"uri\"], y=\"label\", training_frame=mnist_training, validation_frame=mnist_testing)\nmodel_mnist_mymodel_tf.show()",
        "text": "build deep water tensorflow model",
        "id": 773
    },
    {
        "code": "\nvar_portion = eig_val/eig_val.sum()\nprint ('Portion of variance in First PC:', var_portion[0])\nprint ('\\nPortion of variance in Second PC:', var_portion[1])\nprint ('\\nPortion of variance in Third PC:', var_portion[2])\nprint ('\\nPortion of variance in Fourth PC:', var_portion[3])\nprint ('\\nWe can observe that 92.46% of variance in data is contained in the first PC and other components have very')\nprint ('less portion of variance.')\nprint ('Hence, We can conclude that only 1 true component is needed to represent the data.')",
        "text": "$ \\textbf { question 1 part 2 }   -  \\textbf { portion of variance in each of the pc } $",
        "id": 774
    },
    {
        "code": "graph_v = v.plot(chart=cart, mapping=Phi, chart_domain=spher, nb_values=11, scale=0.2)\ngraph_v\nshow(graph_v, viewer='tachyon')",
        "text": "a 3d view of the vector field $ v $ be obtain via the embed $ \\phi $ ,",
        "id": 775
    },
    {
        "code": "plt.plot(np.cumsum(sig**2)/np.sum(sig**2))",
        "text": "out of curiosity , let 's plot the spectrum to get a sense of how independent the feature be  . ",
        "id": 776
    },
    {
        "code": "election88['vote_bush'] = np.around((election88.electionresult*election88.vote_total).values)\nelection88.head(5)",
        "text": "with the vote data , compute the number of people who do and do n't vote for bush by state  . ",
        "id": 777
    },
    {
        "code": "products['contains_perfect'] = products['perfect'].apply(lambda s : +1 if s >= 1 else 0)\nproducts['contains_perfect'].sum()",
        "text": "now , write some code to compute the number of product review that contain the word   -  perfect  -  first create a column call contains _ perfect  which be set to 1 if the count of the word   -  perfect  -  ( store in column   -  perfect  -  ) be > = 1   -  sum the number of 1 in the column contains _ perfect   . ",
        "id": 778
    },
    {
        "code": "\nf, axarr = plt.subplots(5, 8, figsize=(17, 10))\nrow = 0\ncol = 0\nfor index, frame in enumerate(bench_video[13]):\n    if index in [8, 16, 24, 32, 40]:\n        row += 1\n        col = 0\n    axarr[row, col].imshow(np.squeeze(frame, axis=2), cmap='gray')\n    col += 1",
        "text": "the sequence of frame of a sample video of *jogging*",
        "id": 779
    },
    {
        "code": "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['u']])\nprint(tfidf_transformer.idf_[bow_transformer.vocabulary_['university']])\nmessages_tfidf = tfidf_transformer.transform(messages_bow)\nprint(messages_tfidf.shape)",
        "text": "we ll go ahead and check what be the idf ( inverse document frequency ) of the word  u  ? of word  university  ?",
        "id": 780
    },
    {
        "code": "print((TP + TN) / float(TP + TN + FP + FN))\nprint(metrics.accuracy_score(y_test, model.predict(x_test)))",
        "text": "metric compute from a confusion matrix classification accuracy , overall , how often be the classifier correct ?",
        "id": 781
    },
    {
        "code": "a_website = requests.get(\"http://google.com\")\ndata = BeautifulSoup(a_website.content)",
        "text": "first , we request the webpage and extract the content ,",
        "id": 782
    },
    {
        "code": "def solve_image(eval_obj, niter, x):\n    for i in range(niter):\n        x, min_val, info = fmin_l_bfgs_b(eval_obj.loss, x0=x.flatten(), \n                                        fprime=eval_obj.grads, maxfun=20)\n        x = np.clip(x, -127, 127)\n        print('Current loss value:', np.mean(min_val))\n        \n        \n        imsave(f'{path}/results/res_at_iteration_{i}.png', deproc(x.copy(), shp)[0])\n        \n    return x",
        "text": "now we re go to optimize this loss function with a deterministic approach to optimization that us a line search , which we can implement with sklearn 's fmin _ l _ bfgs _ b  function  . ",
        "id": 783
    },
    {
        "code": "train_data = train_data.drop(['Name', 'PassengerId'], axis=1)\ntest_data = test_data.drop(['Name'], axis=1)\nfull_data = [train_data, test_data]\ntrain_data.shape, test_data.shape",
        "text": "now we can safely drop the name feature from train and test datasets  .  we also do not need the passengerid feature in the train dataset  . ",
        "id": 784
    },
    {
        "code": "S = pd.Series([0,2,4,6,8,10])\nS\ndf['F'] = S\ndf\nS = pd.Series([0,2,4,6,8,10], index=pd.date_range('20170101', periods=6))\nS\ndf['F'] = S\ndf",
        "text": "set a new column automatically align the data by the index",
        "id": 785
    },
    {
        "code": "from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\ndef rsquare_meansquare_error(train_y, test_y, train_X, test_X, test, best_model):\n    y_train_pred = best_model.predict(train_X)\n    y_test_pred = best_model.predict(test_X)\n    \n    print ('MSE ' + test + ' train data: %.2f, test data: %.2f' % (\n        mean_squared_error(train_y, y_train_pred),\n        mean_squared_error(test_y, y_test_pred)))\n    \n    print('R^2 ' + test + ' train data: %.2f, test data: %.2f' % (\n        r2_score(train_y, y_train_pred),\n        r2_score(test_y, y_test_pred)))\nrsquare_meansquare_error(y_train, y_test, X_train, X_test, \"OLS\", lr)",
        "text": "evaluate the performance of your ols model on the train and on the test set",
        "id": 786
    },
    {
        "code": "df['name']=df['name'].map(capitalizer)\ndf['coverage']=df['coverage'].map(np.square)\ndf.head()",
        "text": "map the capitalizer lambda function over each element in the series name   -   map ( ) apply an operation over each element of a series",
        "id": 787
    },
    {
        "code": "(1-truth_matrix)*prediction_matrix",
        "text": "set prediction for label that be present to zero",
        "id": 788
    },
    {
        "code": "\n(df['duration'][df.duration > 90].count()) / df.duration.dropna().count()\n# perc_one['duration']",
        "text": "what 's the probability that a movie wa long than an hour and a half ?",
        "id": 789
    },
    {
        "code": "df = pd.DataFrame([[ 1.4, np.nan], [7.1, -4.5], [np.nan, np.nan], [0.75, -1.3]], index =['a', 'b', 'c', 'd'], columns =['one', 'two'])\ndf\ndf.sum()",
        "text": "summarize and compute descriptive statistic",
        "id": 790
    },
    {
        "code": "nums=[0,1,2,3,4,5]\nsquares=[x**2 for x in nums]\nprint (squares)\neven_squares=[x**2 for x in nums if x%2==0]\nprint(even_squares)",
        "text": "list comprehension , when program , frequently we want to transform one type of data into another  .  a a simple example , consider the fillowing code that compute square number ,",
        "id": 791
    },
    {
        "code": "diabetes.groupby('frame')['hdl'].describe().unstack()\nsns.factorplot(x='pclass', y = 'survived', data=titanic)\nfig, ax = plt.subplots(figsize = (8,6))\nsns.boxplot(x = \"frame\", y = \"hdl\", data = diabetes, order = [\"small\",\"medium\",\"large\"], ax=ax);\nsns.factorplot(x = \"frame\", y = \"hdl\", data = diabetes, kind=\"point\", size = 5, order = [\"small\",\"medium\",\"large\"]);\nsns.set_context('notebook', font_scale=2)\nfig = sns.FacetGrid(diabetes, hue = \"frame\", aspect = 3, size = 6)\nfig.map(sns.kdeplot, \"hdl\", shade = True)\nfig.add_legend();",
        "text": "one categorical vs .  a numerical variable",
        "id": 792
    },
    {
        "code": "\ndf[\"Item ID\"].unique()\nunique_items = len(df[\"Item ID\"].unique())\navg_price = df[\"Price\"].mean()\nn_purchases = df[\"Price\"].count()\ntotal_rev = df[\"Price\"].sum()\npurch_table = pd.DataFrame({\n    \"Number of Unique Items\":[unique_items],\\\n    \"Average Price\": [avg_price],\\\n    \"Number of Purchases\": [n_purchases],\\\n    \"Total Revenue\": [total_rev]\n})\norg_purch_table = purch_table[[\"Number of Unique Items\",\"Average Price\",\"Number of Purchases\",\"Total Revenue\"]]\norg_purch_table",
        "text": "purchase analysis ( total ) * number of unique item * average purchase price * total number of purchase * total revenue",
        "id": 793
    },
    {
        "code": "from sklearn.pipeline import Pipeline\npreprocess_pipeline = Pipeline([\n    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n])\nX_train_transformed = preprocess_pipeline.fit_transform(X_train)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nlog_clf = LogisticRegression()\nscore = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\nscore.mean()",
        "text": "we be now ready to train our first spam classifier  .  let 's transform the whole dataset ,",
        "id": 794
    },
    {
        "code": "desc_leader = pd.DataFrame({'manager':['Clara','Sanja'],\n                           'description':['Bioinformatics','Biology']})\nmany_to_one = pd.merge(df3,desc_leader)\nmany_to_one",
        "text": "< h3 class =  green  > many   -   to   -   one join example",
        "id": 795
    },
    {
        "code": "def x_bigrams(tokens, x):\n    \n    return ls_bigram\nbigrams = x_bigrams(tok, 20)\nassert_is_instance(bigrams, list)\nassert_true(all(isinstance(b, tuple) for b in bigrams))\nassert_true(len(bigrams), 20)\nassert_equal(bigrams, [(\"'Til\", 'Recently'), (\"'To\", 'whoever'),\n                       ('Anybody', 'armed'), ('Attila', 'raised'),\n                       ('Badon', 'Hill'), ('Bon', 'magne'), ('Chapter', 'Two'),\n                       ('Clark', 'Gable'), ('Divine', 'Providence'),\n                       ('Great', 'scott'), ('Most', 'kind'),\n                       ('Olfin', 'Bedwere'), ('Recently', 'Said'),\n                       ('Thou', 'hast'), ('Thy', 'mer'), ('Too', 'late'),\n                       ('Uther', 'Pendragon'), ('absolutely', 'necessary'),\n                       ('advancing', 'behaviour'),\n                       ('anarcho-syndicalist', 'commune')])",
        "text": "collocation , bigram here you will make a function that will use nltk to grab the x best bi   -   gram , where x be a positive integer  .  you should be use pointwise mutual information in order to do this  . ",
        "id": 796
    },
    {
        "code": "soup.findAll('h5', class_ = \"title\")\nexample_tag =  soup.findAll('h5', class_ = \"title\")[0]\nexample_tag\nexample_tag.next\nexample_tag.next.strip()\ntitles = [tag.next.strip() for tag in soup.findAll('h5', class_ = \"title\")]\ntitles",
        "text": "title of each song 1 .  inspect the element 2 .  identify the html tag and class 3 .  use soup . findall  to make a list of all relevant tag 4 .  pull off an example case 5 .  use soup/string method to pull out the title 6 .  use a list comprehension to process all tag",
        "id": 797
    },
    {
        "code": "acidlistminusoleic = ['palmitic', 'palmitoleic', 'stearic', 'linoleic', 'linolenic', 'arachidic', 'eicosenoic']\nax = region_groupby.aggregate(mean)[acidlistminusoleic].plot(kind = \"barh\", stacked = True)\nax.set_yticklabels(rvals);",
        "text": "you can see that oleic dominate , and doe n't let u see much about the other acid  .  remove it and let 's draw bar plot again  . ",
        "id": 798
    },
    {
        "code": "df = pd.read_csv('sprint.csv')\ndf.head()\np = figure()\nsource = ColumnDataSource(df)\np = figure(x_axis_label = 'Year',y_axis_label='Time',tools='box_select')\np.circle('Year','Time',source=source,selection_color='red', nonselection_alpha=0.1)\noutput_file('selection_glyph.html')\nshow(p)",
        "text": "selection and non   -   selection glyph",
        "id": 799
    },
    {
        "code": "pt.axis(\"equal\")\npt.contour(xmesh, ymesh, fmesh, 50)",
        "text": "then a a  contour plot  ,",
        "id": 800
    },
    {
        "code": "alpha = 3\nbeta = 4\nprint(subtract_and_increment(a=alpha, b=beta))\nprint(subtract_and_increment(b=beta, a=alpha))",
        "text": "name argument it can be easy to make a mistake in the input order  .  this can lead to a bug  .  we can reduce this risk by give input a *named* argument  .  name argument also enhance program readability  .  when we use name argument , the order of input doe not matter  . ",
        "id": 801
    },
    {
        "code": "dog = model['dog']\nprint(dog.shape)\nprint(dog[:10])\ndinesh = model['Dinesh']\nprint(dinesh.shape)\nprint(dinesh[:10])",
        "text": "now check the shape of the word present in the pretrained word2vec model dictionary",
        "id": 802
    },
    {
        "code": "\ncosts = np.squeeze(d['costs'])\nplt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\nplt.show()",
        "text": "let 's also plot the cost function and the gradient  . ",
        "id": 803
    },
    {
        "code": "P_ace_of_spades = ...\nP_heart_given_first_card_was_ace_of_spade = ...\nP_first_card_ace_of_spade_and_second_card_is_heart = ...\nprint(P_first_card_ace_of_spade_and_second_card_is_heart)",
        "text": "suppose you take out two card from a standard pack of card one after another , without replace the first card  .  what be probability that the first card be the ace of spade , and the second card be a heart ?",
        "id": 804
    },
    {
        "code": "est = TensorFlow(source_directory=script_folder,\n                 script_params={'--data-folder': ds.path('mnist').as_mount()},\n                 compute_target=compute_target,\n                 conda_packages=['keras', 'matplotlib'],\n                 entry_script='keras_mnist.py', \n                 use_gpu=True)",
        "text": "next , we will create a new estimator without the above parameter since they will be pass in late by hyperdrive configuration  .  note we still need to keep the data   -   folder  parameter since that 's not a hyperparamter we will sweep  . ",
        "id": 805
    },
    {
        "code": "features = ['air_pressure', 'air_temp', 'avg_wind_direction', 'avg_wind_speed', 'max_wind_direction', \n        'max_wind_speed','relative_humidity']\nselect_df = sampled_df[features]\nselect_df.columns",
        "text": "< p style=  font   -   family , arial , font   -   size,1 . 75em , color , purple , font   -   style , bold  >   select feature we want to use for cluster",
        "id": 806
    },
    {
        "code": "\nmse = metrics.mean_squared_error(y_test, y_prediction)\nrmse = np.sqrt(mse)\nprint(rmse)\naccuracy_list = cross_val_score(credit_linreg, X, y, cv=10, scoring='mean_squared_error')\nprint(accuracy_list)\nmse_list_positive = -accuracy_list\nrmse_list = np.sqrt(mse_list_positive)\nprint(rmse_list)\nprint(rmse_list.mean())",
        "text": "que 7   -   now , use 10   -   fold cross   -   validation to evaluate the performance of a linear regression in predict the balance  .  thus , rather than split the dataset into test and train , use cross   -   validation to evaluate the regression performance  .  what be the rmse when you use cross validation ?",
        "id": 807
    },
    {
        "code": "df_elem.drop(['Accountability and Assistance Level', 'Accountability and Assistance Description', \n              'District_Accountability and Assistance Description', \n              'District_Accountability and Assistance Level', 'AP_Test Takers',\n             'AP_Tests Taken', 'Progress and Performance Index (PPI) - High Needs Students', \n             'District_Progress and Performance Index (PPI) - All Students', \n             'District_Progress and Performance Index (PPI) - High Needs Students'], axis=1, inplace=True)\ndf_elem.drop(['7_Enrollment', '8_Enrollment', '9_Enrollment', '10_Enrollment', '11_Enrollment', '12_Enrollment'], axis=1, inplace=True)\ndf_elem.dropna(thresh=300, axis=1, inplace=True)\ndf_elem.isnull().sum()",
        "text": "drop column with descriptive value and lot of miss information ,",
        "id": 808
    },
    {
        "code": "id_ft_all = id_ft + id_ft2\nfullt_all = fullt + fullt2\nprint(str(len(id_ft)))\nprint(str(len(id_ft2)))\nprint(str(len(id_ft_all)))\nprint(str(len(fullt)))\nprint(str(len(fullt2)))\nprint(str(len(fullt_all)))\nkeys_fullt = id_ft_all\nvals_fullt = fullt_all\ndict_fullt = dict(zip(keys_fullt,vals_fullt))",
        "text": "combine datasets 1 and 2",
        "id": 809
    },
    {
        "code": "sknl = knl\nsknl = lp.split_iname(sknl,\n        \"i\", 16, outer_tag=\"g.1\", inner_tag=\"l.1\")\nsknl = lp.split_iname(sknl,\n        \"j\", 16, outer_tag=\"g.0\", inner_tag=\"l.0\")\nsknl = lp.add_prefetch(sknl, \"u\",\n    [\"i_inner\", \"j_inner\"],\n    fetch_bounding_box=True)\nevt, (result,) = sknl(queue, u=u, n=n)",
        "text": "how about some data reuse ?",
        "id": 810
    },
    {
        "code": "scan = CTScan(np.asarray(candidates.iloc[negatives[600]])[0], \\\n              np.asarray(candidates.iloc[negatives[600]])[1:-1])\nscan.read_mhd_image()\nx, y, z = scan.get_voxel_coords()\nimage = scan.get_image()\ndx, dy, dz = scan.get_resolution()\nx0, y0, z0 = scan.get_origin()",
        "text": "check if my class work",
        "id": 811
    },
    {
        "code": "\ngradient_attack = foolbox.attacks.FGSM(fmodel)\ngradient_adversarial_image = gradient_attack(target_image[:, :, ::-1], label)",
        "text": "apply an attack to target image",
        "id": 812
    },
    {
        "code": "data = input(\"Please enter comma seprated numbers values : \")\nlist = data.split(\",\")\nprint('Generated List : ',list)",
        "text": "write a program which accept a sequence of comma   -   separate number from console and generate a list  . ",
        "id": 813
    },
    {
        "code": "H_d = entropy([.025,.025,.05,.9])\nH_d\nassert(H_d < H_original)",
        "text": "todo _  _  , change the probability to decrease the entropy",
        "id": 814
    },
    {
        "code": "from collections import Counter\ncounts = Counter(words)\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\nreviews_ints = []\nfor each in reviews:\n    reviews_ints.append([vocab_to_int[word] for word in each.split()])",
        "text": "encode the word the embed lookup require that we pas in integer to our network  .  the easy way to do this be to create dictionary that map the word in the vocabulary to integer  .  then we can convert each of our review into integer so they can be pass into the network  . ",
        "id": 815
    },
    {
        "code": "\ngenerate_learningcurves(sparse=True,gX_train=gX_train[:,[0,3,1,4,2,5]], gy_train=gy_train,\n                        alg=SVR(kernel='linear'), alg_name=\"Support Vector Machines - Top 6 Features\")",
        "text": "rerun learn curve with top 6 feature",
        "id": 816
    },
    {
        "code": "test_ids = []\nfor ID in df_test_1['Id'].values:\n    test_ids.append(ID)\ndef write_predictions(predictions, ids, outfile):\n    with open(outfile,\"w+\") as f:\n        \n        f.write(\"Id,Prediction\\n\")\n        for i, history_id in enumerate(ids):\n            f.write(\"%s,%d\\n\" % (history_id, predictions[i]))\nxgb_Test = xgb_fit.predict(X_test)\nwrite_predictions(xgb_Test,test_ids,\"xgb_f4.csv\")",
        "text": "part iii write to file",
        "id": 817
    },
    {
        "code": "df = json_normalize(json_decode, 'mjtheme_namecode', ['countryname'])\ndf = df.groupby(['code','name'])['countryname'].count().reset_index()\ndf.sort_values(['countryname'], ascending=False).head(10)",
        "text": "the top 10 major project theme ( use column mjtheme _ namecode  )",
        "id": 818
    },
    {
        "code": "\ntf_idf_vocabulary_title, tf_idf_matrix_title = tf_idf_extractor(dataset['title'], min_df=0.005)\nprint(\"matrix dimension: {} documents * {} features\".format(tf_idf_matrix_title.shape[0], tf_idf_matrix_title.shape[1]))\nprint(\"total number of elements: {}\".format(tf_idf_matrix_title.shape[0] * tf_idf_matrix_title.shape[1]))\nprint(\"non-zero values: {}\".format(tf_idf_matrix_title.nnz))\nprint(\"density: {}\".format(density(tf_idf_matrix_title)))\nprint(\"{0:.2f} % of the matrix are non-zero elements\".format(density(tf_idf_matrix_title)*100))",
        "text": "extract feature from title we will perform feature extraction from the title use tf _ idf and 3   -   ngrams token",
        "id": 819
    },
    {
        "code": "from operator import attrgetter\nprint (sorted(users, key=attrgetter('user_id')))",
        "text": "instead of use lambda , an alternative approach be to use operator . attrgetter ( )",
        "id": 820
    },
    {
        "code": "svd_model5 = TruncatedSVD(n_components=50, random_state=0)\ntrain_x5 = svd_model5.fit_transform(tfidf_train_x5)\ntest_x5 = svd_model5.transform(tfidf_test_x5)",
        "text": "dimensionality reduction of train and test data use latent semantic index ( lsi )",
        "id": 821
    },
    {
        "code": "print(lm.intercept_)\nprint(lm.coef_)\ncdf=pd.DataFrame(lm.coef_,X.columns,columns=['Coeff'])",
        "text": "evaluation    -  evaluate the model by check out it coefficient and how we can interpret them",
        "id": 822
    },
    {
        "code": "df[df['Grad.Rate'] > 100 ]\ndf['Grad.Rate']['Cazenovia College'] = 100\ndf[df['Grad.Rate'] > 100]",
        "text": "there seem to be a private school with a graduation rate of high than 100 %   - ",
        "id": 823
    },
    {
        "code": "def fit_model_and_score_tree(train_features, train_response, val_features, val_response):\n    return score_model(fit_tree(train_features, train_response),train_features, train_response, val_features, val_response)",
        "text": "you should be able to use your same score function a above to compute your model score  .  write a function that fit a tree model to your train set and return the model 's score for both the train set and the validation set  . ",
        "id": 824
    },
    {
        "code": "iris.shape\niris\niris.describe()\niris.species.value_counts()\niris.isnull().sum()",
        "text": "gather some basic information about the data  . ",
        "id": 825
    },
    {
        "code": "arr = np.arange(9).reshape((3,3))\narr",
        "text": "flatten    -  reshape from a high dimensional to one dimensional order be call flatten",
        "id": 826
    },
    {
        "code": "import numpy as np\nimport matplotlib.pyplot as plt\nnp.set_printoptions(precision=2)  \nn_features = 50\nn_samples = 1000\nidx = np.arange(n_features)\ncoefs = ((-1) ** idx) * np.exp(-idx / 10.)\ncoefs[20:] = 0.\nplt.stem(coefs)\nplt.title(\"Parameters / Coefficients\")\nplt.show()",
        "text": "part 0 , introduction we ll start by generate sparse vector and simulate data    -  get sparse coefficient",
        "id": 827
    },
    {
        "code": "open(\"read_write_3.txt\", \"w\").write(\"\"\"Hello World!\n- From reading exercise 3\nThis is a file we have created ourselves!\nGo see in Home that the file now exists and has this content\n\"\"\")\nprint(open(\"read_write_3.txt\").read())",
        "text": "file write by line prediction , ( double click here and enter your prediction before run the next cell )",
        "id": 828
    },
    {
        "code": "\naggregateresults = []\nfor n in range(0,10000):\n    cheaters = 0\n    for student in groundtruth:\n        cheaters += survey(student,groundtruth[student])\n    realcheaters = (cheaters - 125) * 2 \n    aggregateresults.append(int(realcheaters))\ngraphaggres = [int(res/2) for res in aggregateresults]\nbincount = max(graphaggres) - min(graphaggres)\nplt.hist(graphaggres, bins=bincount, histtype='stepfilled')\nplt.title(\"Corrected results\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\");\nnp.mean(aggregateresults)\nnp.median(aggregateresults)",
        "text": "okay , so how well doe it work ? okay , so we get an answer not too far off from what we know grind truth to be ( when i run the simulation , i get 98 )  .  but how well doe this work out in aggregate ? what would happen if we run our survey , say , 10,000 time ?",
        "id": 829
    },
    {
        "code": "arr = np.random.rand(10)\nprint(np.sort(arr))",
        "text": "exercise create a random vector of size 10 and sort it",
        "id": 830
    },
    {
        "code": "\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(units=16, \n                         activation='relu', \n                         kernel_regularizer=regularizers.l2(0.01),\n                         input_shape=(number_of_features,)))\nnetwork.add(layers.Dense(units=16, \n                         kernel_regularizer=regularizers.l2(0.01),\n                         activation='relu'))\nnetwork.add(layers.Dense(units=1, activation='sigmoid'))",
        "text": "weight regularization in kera , we can add a weight regularization by include use include kernel _ regularizer=regularizers . l2 ( 0 . 01 ) a late  .  in this example , 0 . 01 determine how much we penalize high parameter value  . ",
        "id": 831
    },
    {
        "code": "import numpy as np\neip_x = np.array([[1,2], [3,4]])\nprint(eip_x)    \n            \nprint(eip_x.T)  \n            \neip_v = np.array([1,2,3])\nprint(eip_v)    \nprint(eip_v.T)  # Prints \"[1 2 3]\"",
        "text": "apart from compute mathematical function use array , we frequently need to reshape or otherwise manipulate data in array  .  the simple example of this type of operation be transpose a matrix , to transpose a matrix , simply use the t attribute of an array object ,",
        "id": 832
    },
    {
        "code": "ef2 = ef.copy()\nef2['MOB']=['9632685440','859485948','5495894859']\nprint(ef2)\nef2.MOB.isin(['9632685440'])\nef2[['AGE','SALARY']]",
        "text": "use isin method of filter",
        "id": 833
    },
    {
        "code": "cast1950 = cast[cast.year // 10 == 195]\ncast1950 = cast1950[cast1950.n == 1]\ncast1950.groupby(['year', 'type']).size()",
        "text": "< div class=  alert alert   -   success  >   exercise   , how many lead ( n=1 ) role be available to actor , and how many to actress , in each year of the 1950s ?",
        "id": 834
    },
    {
        "code": "layer_conv2, weights_conv2 = \\\n    new_conv_layer(input=layer_conv1,\n                   num_input_channels=num_filters1,\n                   filter_size=filter_size2,\n                   num_filters=num_filters2,\n                   use_pooling=True)\nlayer_conv2\nlayer_conv3, weights_conv3 = \\\n    new_conv_layer(input=layer_conv2,\n                   num_input_channels=num_filters2,\n                   filter_size=filter_size3,\n                   num_filters=num_filters3,\n                   use_pooling=True)\nlayer_conv3",
        "text": "convolutional layer 2 and 3 create the second and third convolutional layer , which take a input the output from the first and second convolutional layer respectively  .  the number of input channel correspond to the number of filter in the previous convolutional layer  . ",
        "id": 835
    },
    {
        "code": "def emb_sent_bow(inp):\n    emb = TimeDistributed(Embedding(vocab_size, emb_dim))(inp) \n                                                               \n    return Lambda(lambda x: K.sum(x, axis=2))(emb)    # the Lambda layer adds up all embedding (in shape [m, story_maxsents=10, emb_dim=20])",
        "text": "we use   -  [ timedistributed ] ( <url> / )   -  here to apply the embed to every element of the sequence , then the   lambda   layer add them up",
        "id": 836
    },
    {
        "code": "def distance_two_features(title0, title1, x_feature, y_feature):\n    row0 = row_for_title(title0)\n    row1 = row_for_title(title1)\n    return (np.sqrt(((row0.item(x_feature) - row1.item(x_feature)) ** 2) \n                            + ((row0.item(y_feature) - row1.item(y_feature)) ** 2)))\nfor song in make_array(\"Lookin' for Love\", \"Insane In The Brain\"):\n    song_distance = distance_two_features(song, \"In Your Eyes\", \"like\", \"love\")\n    print(song, 'distance:\\t', song_distance)\n_ = ok.grade(\"q2_1_2\")",
        "text": "complete the function distance _ two _ features  that compute the euclidean distance between any two song , use two feature  .  the last two line call your function to show that *lookin  for love* be close to *in your eyes* than *insane in the brain  - ",
        "id": 837
    },
    {
        "code": "pd.crosstab(index=df['Weekday'], columns=df['TripType']).idxmax()",
        "text": "most common weekday for each triptype",
        "id": 838
    },
    {
        "code": "m=0\nmnthMean = [] \nmnthStd = []\nmnths = np.array([31,29,31, 30, 31, 30, 31, 31, 30, 31,30,31])  \nmonthArr = [i for i in range(0,12)]\nfor i in monthArr:\n    monthDat = data['tmax'][m:m+mnths[i]]\n    mnthMean.append(np.mean(monthDat))\n    mnthStd.append(np.std(monthDat))\n    m = m+mnths[i]\n \nprint(\"DONE\")",
        "text": "< a id=  mmean  >    -  monthly mean temperature   for our bottom plot you will use the calculate monthly average of the daily maximum temperature  .  since each month ha a different number of day the code iterate over an array to calculate the mean temperature for each month  . ",
        "id": 839
    },
    {
        "code": "squares[0]  \nsquares[-1]\nsquares[-3:]  # Slicing returns a new list.",
        "text": "like string ( and all other build   -   in [ sequence ] ( <url> ) type ) , list can be index and slice ,",
        "id": 840
    },
    {
        "code": "\nimport datetime as dt\ntoday = \"2016-02-31\"\nmydt = dt.datetime.strptime(today, '%Y-%m-%d')",
        "text": "it be impossible for february in any year can have more than 29 day  .  instead of use regular expression to validate date , you can also use python 's datatime  module  .  if a give date string can not be convert to a python date object , then the date would n't be valid  . ",
        "id": 841
    },
    {
        "code": "gauss.mean\nprint(gauss.mean.name)\nprint(gauss.mean.value)\nprint(gauss.mean.default)\nmodels.Gaussian1D(amplitude=1, stddev=1)",
        "text": "parameter be object with attribute ,",
        "id": 842
    },
    {
        "code": "df['size'] = df['size'].map(size_maping)\nX = df.values\nX = X[:,:-1]\ncolor_le = LabelEncoder()\nX[:,0] = color_le.fit_transform(X[:,0])\nX\nfrom sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(categorical_features = [0]) \nohe.fit_transform(X).toarray()\npd.get_dummies(df[['color','size','price']])",
        "text": "one   -   hot encode on nominal feature",
        "id": 843
    },
    {
        "code": "\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=1000))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\n#               metrics=['accuracy'])\n# Option #3 - Stochastic gradient descent optimizer.\n# All parameter gradients clipped to max of 1\nsgd = keras.optimizers.SGD(lr=0.01, \n                     decay=1e-6, \n                     momentum=0.9, \n                     nesterov=True,\n                     clipvalue=1.\n                    )\nmodel.compile(loss='mean_squared_error',\n# model.compile(loss='mean_squared_error', \n              optimizer=sgd,\n              metrics=['acc']\n#               metrics=['mae', 'acc']\n#               metrics=[metrics.mae, metrics.categorical_accuracy]\n             )",
        "text": "build the model architecture follow this guide <url>   -  build a model here use sequential  .  feel free to experiment with different layer and size  .  also , experiment add dropout to reduce overfitting  . ",
        "id": 844
    },
    {
        "code": "plt.plot(days, morale_true, lw=7., c='gold', alpha=0.5, label='true function')\nplt.plot(days, Amod.predict(days[:, np.newaxis]), lw=7., c='darkred', alpha=0.5, label='model')\nplt.scatter(students['A']['days'], students['A']['morale'],\n           s=70, c='darkred', label='student A', alpha=0.7)\nplt.xlabel('days', fontsize=16)\nplt.ylabel('morale', fontsize=16)\nplt.title('Morale over time\\n', fontsize=20)\nplt.xlim([0, 85])\nplt.legend(loc='upper left')",
        "text": "plot the model relationship between day and morale ,   - ",
        "id": 845
    },
    {
        "code": "\ntest_coords = nx.get_node_attributes(test_graph, 'xyz')\nx_vals = [x[0] for x in test_coords.values()]\ny_vals = [x[1] for x in test_coords.values()]\nz_vals = [x[2] for x in test_coords.values()]\ncolor_dict = {\n    'H': 'red',\n    'O': 'blue',\n}\nsize_dict = {\n    'H': 25,\n    'O': 60,\n}\nc_vals = [color_dict[x[0]] for x in test_coords.keys()]\ns_vals = [size_dict[x[0]] for x in test_coords.keys()]",
        "text": "state of the graph the graph ha a bunch of node that have no edge inbetween them  .  the node do however , have their x , y , z position attach to them  .  we should at least be able to generate a scatter plot  . ",
        "id": 846
    },
    {
        "code": "import copy\ntest_labels_copy = copy.copy(test_labels)\nnp.random.shuffle(test_labels_copy)\nhits_array = np.array(test_labels) == np.array(test_labels_copy)\nfloat(np.sum(hits_array)) / len(test_labels)",
        "text": "this approach reach an accuracy of   -  80 %  .  with a balance binary classification problem , the accuracy reach by a purely random classifier would be 50 %  .  but in this case it  s close to 19 % , so the result seem pretty good , at least when compare to a random baseline ,",
        "id": 847
    },
    {
        "code": "print(TP / float(TP + FP))\nprint(metrics.precision_score(y_test, model.predict(x_test)))",
        "text": "precision , when a positive value be predict , how often be the prediction correct ? how  precise  be the classifier when predict positive instance ?",
        "id": 848
    },
    {
        "code": "Lil_artists = Lil_data['artists']['items']\nfor artist in Lil_artists:\n    print(artist['name'], artist['popularity'])",
        "text": "with  lil wayne  and  lil kim  there be a lot of  lil  musician  .  do a search and print a list of 50 that be playable in the usa ( or the country of your choice ) , along with their popularity score   - ",
        "id": 849
    },
    {
        "code": "plt.imshow(grid, cmap='Greys', origin='lower')\nplt.plot(x_init[1], x_init[0], 'ro')\nfor (v1, v2) in rrt.edges:\n    plt.plot([v1[1], v2[1]], [v1[0], v2[0]], 'y-')\nplt.show()",
        "text": "now let 's plot the generate rrt  . ",
        "id": 850
    },
    {
        "code": "cast1950 = cast[cast['year'] // 10 == 195]\ncast1950 = cast1950[cast1950['n'] == 1]\ncast1950.groupby(['year', 'type']).size()",
        "text": "< div class=  alert alert   -   success  >   exercise   ,     how many lead ( n=1 ) role be available to actor , and how many to actress , in each year of the 1950s ?",
        "id": 851
    },
    {
        "code": "venues_both[venues_both.betweenness > 0].sort('betweenness_delta', ascending=True)[['name_x', 'betweenness_may', 'betweenness', 'betweenness_delta']][:10]",
        "text": "top 10 location with the large decrease in betweenness centrality",
        "id": 852
    },
    {
        "code": "def switch_guess(guesses, goatdoors):\n    result = np.zeros(guesses.size)\n    switch = {(0, 1): 2, (0, 2): 1, (1, 0): 2, (1, 2): 0, (2, 0): 1, (2, 1): 0}\n    for i in [0, 1, 2]:\n        for j in [0, 1, 2]:\n            mask = (guesses == i) & (goatdoors == j)\n            if not mask.any():\n                continue\n            result = np.where(mask, np.ones_like(result) * switch[(i, j)], result)\n    return result",
        "text": "write a function , switch _ guess , that represent the strategy of always switch a guess after the goat door be open  . ",
        "id": 853
    },
    {
        "code": "obj = pd.Series(['a', 'b', 'c', 'd'])\nobj\nobj.index  #this is the default index",
        "text": "< a name=  series  >        -  series   -   a series be a one   -   dimensional array   -   like object contain homogenously type element   -  each element ha an associate data label , call it index  .  by default , the index consist of ordinary array index , i . e  .  consecutive integer start from zero  . ",
        "id": 854
    },
    {
        "code": "from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nscores = []\ndepths = []\nfor depth in range(1,101):\n    dt=DecisionTreeClassifier(max_depth=depth)\n    dt.fit(X_train, y_train)\n    scores.append(dt.score(X_test,y_test))\n    depths.append(depth)\ndepth_best = depths[np.argmax(scores)]\nprint(depth_best)\nfpr,tpr,roc_auc, thresholds = generate_auc(X,y,DecisionTreeClassifier,max_depth=depth_best,criterion='entropy')\ngenerate_ROCplot(fpr,tpr,'LR',roc_auc)",
        "text": "find the optimal decision tree depth and compute the auc roc  . ",
        "id": 855
    },
    {
        "code": "entities = {\n    \"business\": (sampled_yelp_business_df, \"business_id\"),\n    \"reviews\": (sampled_yelp_review_df, \"review_id\"),\n    \"users\": (sampled_yelp_user_df, \"user_id\"),\n    \"checkins\": (sampled_yelp_checkin_df, \"checkin_id\")\n}\nrelationships = [\n    (\"business\", \"business_id\", \"reviews\", \"business_id\"),\n    (\"users\", \"user_id\", \"reviews\", \"user_id\"),\n    (\"business\", \"business_id\", \"checkins\", \"business_id\")\n]\ncutoff_times = [[_, cutoff_time] for _ in sampled_business_ids]\ncutoff_times = pd.DataFrame(cutoff_times, columns = ['business_id', 'cutoff_time'])\nfeature_matrix, feature_definitions = ft.dfs(entities = entities,\n    relationships = relationships,\n    target_entity = \"business\",\n    cutoff_time = cutoff_times)\nfeature_matrix, features = ft.encode_features(feature_matrix, feature_definitions)",
        "text": "deep feature synthesis with feature tool",
        "id": 856
    },
    {
        "code": "c = cast\nc = c[c.name == 'Frank Oz']\ng = c.groupby(['character']).size()\ng[g > 1].sort_values()",
        "text": "< div class=  alert alert   -   success  >   exercise   , list each of the character that frank oz ha portray at least twice  . ",
        "id": 857
    },
    {
        "code": "from pysal.contrib import shapely_ext\nfrom points.process import PoissonPointProcess as csr\nimport pysal as ps\nfrom points.window import as_window\nimport pysal_examples\nva = ps.open(pysal_examples.get_path(\"vautm17n.shp\"))\npolys = [shp for shp in va]\nstate = shapely_ext.cascaded_union(polys)",
        "text": "another example we apply the above centragraphy statistic and visualization to 2 simulate random datasets  . ",
        "id": 858
    },
    {
        "code": "\nax = j.plot_periodogram(0.5, kappa_units=True)\nax = jf.plot_periodogram(0.5, axes=ax, kappa_units=True)\nax = jf.plot_cepstral_spectrum(axes=ax, kappa_units=True)\nax[0].axvline(x = jf.Nyquist_f_THz, ls='--', c='r')\nax[1].axvline(x = jf.Nyquist_f_THz, ls='--', c='r')\nplt.xlim([0., 50.])\nax[1].set_ylim([12,18])\nax[0].legend(['original', 'resampled', 'cepstrum-filtered'])\nax[1].legend(['original', 'resampled', 'cepstrum-filtered']);",
        "text": "you can now visualize the filter psd   - ",
        "id": 859
    },
    {
        "code": "import numpy as np\nnp.arange(7)\nnp.arange(10,19)\nnp.arange(10,19) - 10 + 1\nlen(np.arange(10,19))\n(np.arange(10,19)).size\nnp.arange(10,20,2)\nnp.arange(20,step= 5)\nnp.arange(0,25,step = 5)",
        "text": "intrinsic numpy array creation use numpy 's method",
        "id": 860
    },
    {
        "code": "X = df.drop(\"party\", axis=1)\ny = df.party\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=4444)",
        "text": "split the data into a test and train set  . ",
        "id": 861
    },
    {
        "code": "threshold = 1.0\nweights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\nclipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\nclip_weights = tf.assign(weights, clipped_weights)\nweights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\nclipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\nclip_weights2 = tf.assign(weights2, clipped_weights2)\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()",
        "text": "next , let 's get a handle on the first hide layer 's weight and create an operation that will compute the clip weight use the clip _ by _ norm ( ) function  .  then we create an assignment operation to assign the clip weight to the weight variable ,",
        "id": 862
    },
    {
        "code": "target = 'Outcome'\nfeatures = ['TwoPointPct_Team', 'ThreePointPct_Team', 'FreeThrowPct_Team', 'OffensiveRebounds_Team', 'DefensiveRebounds_Team', 'TwoPointPct_Opponent', 'ThreePointPct_Opponent', 'FreeThrowPct_Opponent', 'OffensiveRebounds_Opponent', 'DefensiveRebounds_Opponent', 'AvgRank', 'OpponentAvgRank', 'WinPct', 'OpponentWinPct', 'WinPctDiff', 'AvgPointsFor', 'AvgPointsAgainst', 'AvgNetPointsFor', 'OpponentAvgPointsFor', 'OpponentAvgPointsAgainst', 'OpponentAvgNetPointsFor', 'SeedDiff', 'TourWins', 'OpponentTourWins', 'TourWinsDiff']\nX_train2 = df_approach2_training[features]\ny_train2 = df_approach2_training[target]\nX_dev2  = df_approach2_dev[features]\ny_dev2  = df_approach2_dev[target]\nX_train2_combined = df_approach2Combined_training[features]\ny_train2_combined = df_approach2Combined_training[target]\nX_dev2_combined  = df_approach2Combined_dev[features]\ny_dev2_combined  = df_approach2Combined_dev[target]",
        "text": "create train and dev set",
        "id": 863
    },
    {
        "code": "mnb_eval_target = pd.DataFrame(data=mnb_predictions_on_eval, columns=[\"Predictions\"])\nmnb_final_eval_data = pd.concat([eval_raw, mnb_eval_target], axis = 1)",
        "text": "now that we have the prediction for the evaluation data we need to merge these prediction with the original eval data",
        "id": 864
    },
    {
        "code": "X_train, X_test, y_train, y_test = train_test_split(\n    X.as_matrix(), y, random_state=42)",
        "text": "split off train and test data",
        "id": 865
    },
    {
        "code": "\nimage_test[0:3]['label']\nraw_pixel_model.predict(image_test[0:3])",
        "text": "make prediction with the simple model base on raw pixel",
        "id": 866
    },
    {
        "code": "x[0] = \"ai buzzword!\"\nx[0]",
        "text": "you can also directly change the value of an index ,",
        "id": 867
    },
    {
        "code": "df3 = json.load(open('data/world_bank_projects.json'))\ndf3 = json_normalize(df3,'mjtheme_namecode',['id'])\ndf3 = df3.groupby(['code', 'name']).size().sort_values(ascending=False)\ndf3.head(10)",
        "text": "find the top 10 major project theme ( use column mjtheme _ namecode  )",
        "id": 868
    },
    {
        "code": "s = c[ : , 1:3]\ns[:] = 10\nc",
        "text": "slice an array return a view of it  . ",
        "id": 869
    },
    {
        "code": "merged_dataset2 = pd.merge(GT17, GT20, on='2011 Seal Beach shooting: (Worldwide)', \n                           left_index=True, right_index=True, how='outer')\nmerged_dataset2.head()\nax_merge2 = merged_dataset2.plot()\nax_merge2.legend(bbox_to_anchor=(1.05, 1), loc=2)",
        "text": "gt17 and gt20 can be merge directly  . ",
        "id": 870
    },
    {
        "code": "image_size = 64\nnum_channels = 1\ndef reshape(dataset, labels):\n    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n    labels = np.array(newarray).astype(np.float32)\n    return dataset, labels\ntrain_dataset, train_labels = reshape(train_dataset, train_labels)\nprint('Training set', train_dataset.shape, train_labels.shape)\ntrain_labels = train_data[\"species\"]\ntarget_names = list(le.classes_)",
        "text": "principal component analysis section     data reshape",
        "id": 871
    },
    {
        "code": "def person_type(x):\n  if x <=16:\n    return 'C'\n  elif x <= 48:\n    return 'A'\n  elif x <= 90:\n    return 'S'\n  else:\n    return 'U'",
        "text": "we do not have any prior information about how the age for child wa define in those year  .  the above information be helpful a we can categorize the people a child , adult and senior by look the peak  .  let u define function to categorize the age into these three category with age of 16 and 48 a the define separator from the graph  . ",
        "id": 872
    },
    {
        "code": "print(\"Predictor coefficients:\")\nfor col, coeff in zip(predictor_cols, ridge.coef_):\n    print(\"{}: {}\".format(col, coeff))\nprint(\"Intercept: {}\".format(ridge.intercept_))",
        "text": "ridge coefficient   -   intercept for each of the predictor a determine by the ridge weight vector  . ",
        "id": 873
    },
    {
        "code": "products = products[products['rating'] != 3]",
        "text": "extract sentiment 3 .  we will ignore all review with rat = 3 , since they tend to have a neutral sentiment  . ",
        "id": 874
    },
    {
        "code": "mapping = {\n    \"Asian/Pacific Islander\": 15159516 + 674625,\n    \"Black\": 40250635,\n    \"Native American/Native Alaskan\": 3739506,\n    \"Hispanic\": 44618105,\n    \"White\": 197318956\n}\nrace_per_hundredk = {}\nfor key in race_counts:\n    race_per_hundredk[key] = 100000*(race_counts[key]/mapping[key])\nrace_per_hundredk",
        "text": "rate of gun death per 100,000 people   -   by race",
        "id": 875
    },
    {
        "code": "validation_data.topk('predictions', k=5, reverse=True)\nvalidation_data.sort('predictions', ascending=False)['grade'].tail(5)",
        "text": "quiz question  -  , what grade be the top 5 loan ?   -  19   -  let u repeat this excercise to find the top 5 loan ( in the   -  validation _ data  -  ) with the   -  lowest probability  -  of be predict a a   -  safe loan  -  ,",
        "id": 876
    },
    {
        "code": "def wrap_int64_list(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))",
        "text": "helper   -   function for wrap a list of integer so it can be save to the tfrecord file  . ",
        "id": 877
    },
    {
        "code": "def testGenerator(images, rows, cols, epoch, start):\n    noise = np.random.normal(0,1, [images,100])\n    out = gModel.predict(noise)\n    fig = plt.figure(figsize=(8,8))\n    for i in range(images):\n        plt.subplot(rows,cols,i+1)\n        plt_face((out[i,:]*2 +1)/2)\n        \n    fig.savefig(str(start+epoch)+'.png')",
        "text": "function to test generator it also save the intermediate result",
        "id": 878
    },
    {
        "code": "\nks = scipy.stats.ks_2samp(df.ageM[::200], df.ageF[::200])\nprint (ks)",
        "text": "now redo the test with a subsample of the data , take only 1 ride every of 200",
        "id": 879
    },
    {
        "code": "negative_mse = rnd_search.best_score_\nrmse = np.sqrt(-negative_mse)\nrmse",
        "text": "the best model achieve the follow score ( evaluate use 5   -   fold cross validation ) ,",
        "id": 880
    },
    {
        "code": "grouped = df.groupby('default')\ngrouped.boxplot(column='balance')\ngrouped.boxplot(column='income')\nplt.figure()\nplt.scatter(df.loc[df['default'] == 1, 'balance'] , df.loc[df['default'] == 1, 'income'] , color='r')\nplt.scatter(df.loc[df['default'] == 0, 'balance'] , df.loc[df['default'] == 0, 'income'])",
        "text": "make boxplots of all of the variable and a scatter plot of of ( balance , income ) with the color of the outcome of default  label red if yes and blue if no  . ",
        "id": 881
    },
    {
        "code": "train_features, dev_test_features, train_labels, dev_test_labels = \\\n  sklearn.model_selection.train_test_split(features, labels, train_size=0.8)",
        "text": "we split our data into train and test set ,",
        "id": 882
    },
    {
        "code": "reviews[\"sentiment\"] = reviews[\"overall\"].apply(lambda score: \"positive\" if score > 3 else \"negative\")\nreviews[\"usefulScore\"] = (reviews[\"helpful\"]/reviews[\"total\"]).apply(lambda n: \"useful\" if n > 0.8 else \"useless\")\nreviews.head(5)\nreviews[reviews.overall == 5].head(5)\nreviews[reviews.overall == 1].head(5)\nreviews.shape",
        "text": "sentiment column depict the numeric score of be positive or negative usefulscore column depict the boolean value of total number of votes _  _ ",
        "id": 883
    },
    {
        "code": "\ntests = np.random.binomial(1, 0.5, size=(int(1e6),5))\n(np.sum(tests,axis=1) == 1).mean()",
        "text": "five fair coin flip produce exactly one head",
        "id": 884
    },
    {
        "code": "import cmath\ncmath.sin(a)\ncmath.cos(a)",
        "text": "to perform additional complex   -   value function such a sine , cosine , or square root , use the *cmath* module",
        "id": 885
    },
    {
        "code": "regions = pd.DataFrame({'population': population,\n                       'area': area})\nregions\nregions.index\nregions.columns\nregions.area\nregions.area",
        "text": "now that we have this along with the population series from before , we can use a dictionary to construct a single two   -   dimensional object contain this information ,",
        "id": 886
    },
    {
        "code": "res=requests.get('http://61.61.224.144:8000/')\nrequests\nprint(requests.get)\nprint(type(requests))\nprint(type(res))\nprint(type(res.text))\nprint(res.url)\nres.status_code\nres.text\nfrom bs4 import BeautifulSoup\nsoup=BeautifulSoup(res.text,'html.parser')\nprint(soup.title.string)\nprint(type(soup))\nprint(soup.img)\nsoup.select(\"strong\")\nsoup.select(\"img[class='itemcov']\")\nprint(soup.select(\"img[src='lenna.png']\"))\nprint(soup.select(\"img[class='itemcov']\")[1])",
        "text": "the request module provide a function call get  that creats a response object",
        "id": 887
    },
    {
        "code": "\ntask = \"\"\ntodolist = []\nwhile(task != \"x\"):\n    task = input(\"Enter a task, press (x) to exit\"). lower()\n    if task != \"x\":\n        todolist.append(task)\n        \n    print(todolist)\nprint(\"You are done, here is your final list\")",
        "text": "write a program for the user to add a todo list",
        "id": 888
    },
    {
        "code": "smaller = int(input('What is your smaller number? '))\nlarger = int(input('What is your larger number? '))\nproduct = 1\nfor i in range(smaller, larger+1):\n    print(i)\n    product = product*i\nprint(product)",
        "text": "suppose you want to ask the user for two number and multiply all number between those number ( inclusive )  .  how would you do it ?",
        "id": 889
    },
    {
        "code": "loans[loans['credit.policy']==1]['fico'].hist(alpha=0.5,color='blue',bins=30,label='Credit.Policy=1')\nloans[loans['credit.policy']==0]['fico'].hist(alpha=0.5,color='red',bins=30,label='Credit.Policy=0')\nplt.legend()\nplt.xlabel('FICO')",
        "text": "exploratory data analysis let 's do some data visualization  .  we ll use seaborn and panda build   -   in plot capability   -  create a histogram of two fico distribution on top of each other , one for each credit . policy outcome   - ",
        "id": 890
    },
    {
        "code": "def model_inputs(image_width, image_height, image_channels, z_dim):\n    \n    inputs_real = tf.placeholder(tf.float32, (None, image_width, image_height, image_channels), name='inputs_real')\n    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name=\"input_z\")\n    learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n    return inputs_real, inputs_z, learning_rate\n\"\"\"\ndef model_inputs(real_dim, z_dim):\n    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name=\"input_real\")\n    \n    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name=\"input_z\")\n    return inputs_real, inputs_z\n\"\"\"",
        "text": "input create tf placeholder for the neural network   -   real input image placeholder with rank 4 use image _ width  , image _ height  , and image _ channels    -  z input placeholder with rank 2 use z _ dim    -  learn rate placeholder with rank 0 .  return the placeholder in a tuple",
        "id": 891
    },
    {
        "code": "Y, S, N = bad_data(111, 18, p_global)\n(S_hat, p_local) = eog_regress(Y, {}, p_global)\nsparklines(S, Y[p_global['eeg_chans']], S_hat, '')\nplt.show()",
        "text": "qualatative assessment ( bad ) here we sample many signal and see whether or not visually our denoised signal be close to our latent signal than the noisy signal be  .  we expect this to fail here since the signal be not indepdndent from the noise  . ",
        "id": 892
    },
    {
        "code": "poets_dict = {\"name\": \"Forough Farrokhzad\", \\\n            \"year of birth\": 1935, \\\n            \"year of death\": 1967, \\\n            \"place of birth\": \"Iran\", \\\n            \"language\": \"Persian\", \\\n            \"works\": [\"Remembrance of a Day\",\"Unison\",\"The Shower of Your Hair\",\"Portrait of Forough\"]}",
        "text": "the key have to be   -  unique  -  and   -  immutable  -  the usual suspect be string and integer   -  the value can be anything , include list , and even other dictionary",
        "id": 893
    },
    {
        "code": "'hello'.upper()\norders = pd.read_table('http://bit.ly/chiporders')\norders.head()\norders[orders.item_name.str.contains('Chicken')].head()\norders.choice_description.str.replace('[', '').str.replace(']', '').head()\norders.choice_description.str.replace('[\\[\\]]', '').head()",
        "text": "how do i use string method in panda ?",
        "id": 894
    },
    {
        "code": "myset = set()    \nmyset.add(1)     \nprint(type(myset),myset)\nmyset",
        "text": "set a set be an unordered collection of unique element and they can be create use the set ( ) function a follow ,",
        "id": 895
    },
    {
        "code": "model = sm.OLS(asl_dtg['DTG'], x_values)\nresults = model.fit()\nresults.summary()\nplt.scatter(asl_dtg['Log ASL'], asl_dtg['DTG'])\nplt.plot(asl_dtg['Log ASL'], results.predict(x_values))",
        "text": "dtg with feature log asl",
        "id": 896
    },
    {
        "code": "a=5.0/6.0\nprint(a)\nprint (type(a))",
        "text": "we can obtain the type of a variable , and use boolean comparison to test these type  . ",
        "id": 897
    },
    {
        "code": "\ndef fib(n):\n \n if n==1 or n==2: \n    return 1\n if n == 0:\n    return 0\n return fib(n-1) + fib(n-2) \nprint (fib(10))",
        "text": "exercise 9 write a recursive python program to compute the result of fibonacci series  .  this be what a fibonacci series be <url> make sure you invoke this function in your main program and display the result",
        "id": 898
    },
    {
        "code": "spectrum = wave.make_spectrum()\nspectrum.plot(linewidth=1)\nthinkplot.config(xlabel='frequency (Hz)',\n                 ylabel='amplitude',\n                 legend=False)",
        "text": "and here 's the spectrum ,",
        "id": 899
    },
    {
        "code": "g = \"global\"\ndef outer(p='param'):\n    l = \"local\"\n    def inner():\n        print( g, p, l)\n    inner()\nouter()\ninner()\nouter.inner()",
        "text": "legb rule local function be bind by the legb rule  .  local , enclose , global , build   -   in  .  local function be not member of the contain function in any way  .  it be just a local name bind  . ",
        "id": 900
    },
    {
        "code": "def add_and_maybe_multiply(a, b, c = None):\n    result = a + b\n    if c is not None:\n        result = result * c\n    return result\ntype(None)",
        "text": "none be also a common default value for function argument",
        "id": 901
    },
    {
        "code": "param_grid = {'max_depth':[1, 2, 3, 4, 5]}\nclf = DecisionTreeClassifier()\nprint('before feature selection:')\nDecTreeModel = tuningPara_crossValid(clf, param_grid, X_train, X_test, y_train, y_test)",
        "text": "decision tree , parameter tune   -   cross validation",
        "id": 902
    },
    {
        "code": "\ndata = np.loadtxt('../data/data1.txt', delimiter=',')\nX = data[:, 0, np.newaxis]  \nY = data[:, 1, np.newaxis]  \nfigureId = 1\ndef plotData(x, Y):\n    global figureId\n    plt.figure(figureId)\n    plt.grid()\n    plt.xlabel('Population of City in 10,000s')\n    plt.ylabel('Profit in $10,000s')\n    plt.plot(X, Y, 'r+')\n    plt.draw()\n    figureId += 1\nplotData(X, Y)\nplt.show()",
        "text": "load and visualize data",
        "id": 903
    },
    {
        "code": "def normalize(problem, points):\n  \n  meta = problem.objectives\n  all_objs = []\n  for point in points:\n    objs = []\n    for i, o in enumerate(problem.evaluate(point)):\n      low, high = meta[i].low, meta[i].high\n      \n      if high==low: objs.append(0);continue;\n      objs.append((o-low/high-low))\n    all_objs.append(objs)\n  return all_objs",
        "text": "to compute most measure , data ( i . e objective ) be normalize  .  normalization be scale the data between 0 and 1 .  why do we normalize ? todo2 , so that all the data be in the same range and can be compare  . ",
        "id": 904
    },
    {
        "code": "np.random.rand(5)\nnp.random.rand(5,5)\nnp.random.randn(2)\nnp.random.randn(4,4)\nnp.random.randint(1,100,10)\narr = np.arange(25)\narr\nranarr = np.random.randint(0,50,10)\nranarr",
        "text": "random , numpy also ha lot of way to create random number array ,   -   rand , create an array of the give shape and populate it with random sample from a uniform distribution over [ 0 , 1 )  . ",
        "id": 905
    },
    {
        "code": "plt.figure(figsize=(8,5))\nplt.boxplot([heterogeneity.values(), heterogeneity_smart.values()], vert=False)\nplt.yticks([1, 2], ['k-means', 'k-means++'])\nplt.rcParams.update({'font.size': 16})\nplt.tight_layout()",
        "text": "let 's compare the set of cluster heterogeneity we get from our 7 restart of k   -   mean use random initialization compare to the 7 restart of k   -   mean use k   -   means++ a a smart initialization  .  the follow code produce a [ box plot ] ( <url> ) for each of these method , indicate the spread of value produce by each method  . ",
        "id": 906
    },
    {
        "code": "\nfrom sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\nprint(len(train_set), \"train +\", len(test_set), \"test\")\nhousing = train_set.copy()",
        "text": "create the train and test datasets",
        "id": 907
    },
    {
        "code": "c=DecisionTreeClassifier(max_depth=7).fit(X_train,y_train)\nprint(c.score(X_train,y_train))\nprint(c.score(X_test,y_test))",
        "text": "so , we can choose max _ depth a 7 or 8",
        "id": 908
    },
    {
        "code": "lines = sc.parallelize([\"coffe panda\", \"happy panda\", \"happiest pandas party\"])\nwords = lines.flatMap(lambda line: line.split(\" \"))\nwords.first()\nfor line in words.take(7):\n    print (line)\nwords2 = lines.map(lambda line: line.split())\nfor line in words2.take(3):\n    print (line)",
        "text": "sometimes we want to produce multiple output element for each input element  .  the operation to do this be call flatmap ( )   -  img src=img2 . png  >",
        "id": 909
    },
    {
        "code": "help(random.randint)\nimport random\ndef get_random_integer():\n    return random.randint(20,50)\ntest_random_integer(get_random_integer)",
        "text": "exercise use the [  random  ] ( <url> ) library to generate a uniform random integer $ x $ such that $ 20 \\le x \\le 50 $ with equal probability  .  the function should take no argument   -  hint ,   -  read the documentation link to above  . ",
        "id": 910
    },
    {
        "code": "movie_id['gross_margin'] = movie_id['revenue'] - movie_id['budget']\nmovie_id[['title','gross_margin']].head()",
        "text": "for each movie , compute the gross margin ( difference between revenue and budget )",
        "id": 911
    },
    {
        "code": "\nw = complex(0.1, 0.4)\nn = 10\nmy_list = generate_sequence(w, n)\nplot_complex_list(my_list, style=\"points\")",
        "text": "now , we can simply execute the follow line ,",
        "id": 912
    },
    {
        "code": "print(\"thing 1\")\nprint(\"thing 1\", \"thing 2\")\nprint(\"thing 1\", \"thing 2\", \"thing 3\")",
        "text": "python provide two way of pass argument to a function 1   -  positional  -  argument 1   -  keyword  -  argument positional argument come first , follow by keyword argument  .  here in the help for  print  we have a positional argument  value   .  actually we can have a variable number of value (  value ,   -  ,  )  . ",
        "id": 913
    },
    {
        "code": "elton_john = people[people['name'] == 'Elton John']\npaul_mc_cartney = people[people['name'] == 'Paul McCartney']\ngraphlab.distances.cosine(elton_john['tfidf'][0], paul_mc_cartney['tfidf'][0])",
        "text": "the cosine distance between elton john 's and paul mccartney 's article ( represent with tf   -   idf ) fall within which range ?",
        "id": 914
    },
    {
        "code": "a=5.0/6.0\nprint(a)\nprint(type(a))\nisinstance(a,float)\nisinstance(a,int)",
        "text": "we can obtain the type of a variable , and use boolean comparison tontest these type  . ",
        "id": 915
    },
    {
        "code": "sal['JobTitle']\ndef chief_string(title):\n    if 'chief' in title.lower():\n        return True\n    else:\n        return False\nsum(sal['JobTitle'].apply(lambda x: chief_string(x)))",
        "text": "how many people have the word chief in their job title ? ( this be pretty tricky )",
        "id": 916
    },
    {
        "code": "start_time = time.time()\ndef isPrime(num):\n    i=2\n    while i <= int(math.sqrt(num)):\n        if num/i == num//i:\n            return False\n        i += 1\n    return True\nprime_num = 2\nprime_index = 0\nwhile prime_index < 10001:\n    if isPrime(prime_num): \n        \n        prime_index += 1\n    prime_num += 1\nprint(prime_num-1)\nprint(\"Processing time: {:0.4f} seconds\".format(time.time() - start_time))",
        "text": "by list the first six prime number , 2 , 3 , 5 , 7 , 11 , and 13 , we can see that the 6th prime be 13 .  what be the 10 001st prime number ?",
        "id": 917
    },
    {
        "code": "full_model = model5()\nfull_model.fit(X, y, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1)",
        "text": "train a full model on the entire give dataset this include generate image",
        "id": 918
    },
    {
        "code": "plot_validation_curve(estimator=RandomForestRegressor(), X=gX_train[:, rf_predictors], y=gy_train, \n                      param_name='min_samples_split', param_range=[1,5,10,15,16,17,18,19,20,21,22,23,24,25,30,50,70,90,100,150,300], \n                      scoring='r2', plot_title='Random Forests', x_label='min_samples_split', y_label='r2 score', \n                      n_jobs=-1)",
        "text": "the best cv score be obtain when max _ depth = 11 .  we will use max depth value near 11 for the hyperparameter grid search  . ",
        "id": 919
    },
    {
        "code": "Ngeq3 = GreaterEq(N, num(3))",
        "text": "were there at least 3 sock choose ?   - ",
        "id": 920
    },
    {
        "code": "num = int(input(\"Enter a number: \"))        \nisDivisible = False;\ni=2;\nwhile i < num:\n    if num % i == 0:\n        isDivisible = True;\n        print (\"{} is divisible by {}\".format(num,i) )\n    i += 1;\nif isDivisible:\n    print(\"{} is NOT a Prime number\".format(num))\nelse:\n    print(\"{} is a Prime number\".format(num))",
        "text": "python program to check give number be prime number or not",
        "id": 921
    },
    {
        "code": "import random\nprint([random.choice([b for b in range(0, 101, 2)]) for i in range(5)])",
        "text": "please write a program to randomly generate a list with 5 even number between 100 and 200 inclusive  . ",
        "id": 922
    },
    {
        "code": "df = pd.read_csv('labels.csv')\npath = 'for_train'\nif os.path.exists(path):\n    shutil.rmtree(path)\nfor i, (fname, breed) in df.iterrows():\n    path2 = '%s/%s' % (path, breed)\n    if not os.path.exists(path2):\n        os.makedirs(path2)\n    os.symlink('../../train/%s.jpg' % fname, '%s/%s.jpg' % (path2, fname))",
        "text": "preprocessing the train data set",
        "id": 923
    },
    {
        "code": "\na\na = a + a\na",
        "text": "yes  .  python allow you to write over assign variable name  .  we can also use the variable themselves when do the reassignment  .  here be an example of what i mean ,",
        "id": 924
    },
    {
        "code": "def gradient(theta, X, y):\n    theta = np.matrix(theta)\n    X = np.matrix(X)\n    y = np.matrix(y)\n    parameters = int(theta.ravel().shape[1])\n    grad = np.zeros(parameters)\n    error = sigmoid(X * theta.T) - y\n    for i in range(parameters):\n        term = np.multiply(error, X[:,i])\n        grad[i] = np.sum(term) / len(X)\n    return grad",
        "text": "the gradient descent algorithm for logistic regression",
        "id": 925
    },
    {
        "code": "dic = {\"treatment\":[\"A\",\"A\",\"B\",\"B\"],\"gender\":[\"F\",\"M\",\"F\",\"M\"],\"response\":[10,45,5,9],\"age\":[15,4,72,65]}\ndf = pd.DataFrame(dic)\ndf\ndf.pivot(index=\"treatment\",columns = \"gender\",values=\"response\")",
        "text": "pivot data frame * pivot , reshape tool",
        "id": 926
    },
    {
        "code": "df_interacting_proteins_APID = pd.concat([b,d,f])\ndf_interacting_proteins_APID.reset_index(inplace=True)\ndf_interacting_proteins_APID.drop(\"index\", inplace = True, axis = 1)",
        "text": "we merge these three interations in a single dataframe call df _ interacting _ proteins _ apid",
        "id": 927
    },
    {
        "code": "addresses = df_train.groupby('Address').groups\nprint('Total training locations: {} out of {} entries'.format(len(addresses.keys()),len(df_train.index)))\naddresses = df_test.groupby('Address').groups\nprint('Total test locations: {} out of {} entries'.format(len(addresses.keys()),len(df_test.index)))",
        "text": "lot of overlap  .  let 's look at unique location",
        "id": 928
    },
    {
        "code": "def select_features(X, y, random_state, kernel='linear', C=1.0, num_attributes=3):\n    \n            \n    return model, columns, ranking\nrfe, rfe_columns, rfe_ranking = select_features(X, y, check_random_state(0))\nassert_is_instance(rfe, RFE)\nassert_is_instance(rfe_ranking, np.ndarray)\nassert_is_instance(rfe.estimator, svm.SVC)\nassert_equal(rfe.estimator.kernel, 'linear')\nassert_equal(rfe.estimator.C, 1)\nassert_equal(rfe.n_features_, 3)\nassert_array_equal(rfe_columns, ['CRSDepTime', 'AirTime', 'Distance'])\nassert_array_equal(rfe_ranking, [3, 5, 2, 1, 4, 1, 1])",
        "text": "feature selection   -   use [ recursive feature elimination ] ( <url> ) to determine the most important feature",
        "id": 929
    },
    {
        "code": "FeatureEval = model.coef_\ntopFeature = np.argsort(FeatureEval)[::-1][:10]\nprint (Features[topFeature])",
        "text": "top 10 important feature in linear regression model",
        "id": 930
    },
    {
        "code": "\ntest_image = cv2.imread('./test_images/test3.jpg')\ntest_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\nundistorted_test_image = undistort(test_image, mtx, dist)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\nfig.subplots_adjust(hspace = .2, wspace=.05)\nax1.imshow(test_image)\nax1.set_title('Original Image', fontsize=20)\nax2.imshow(undistorted_test_image)\nax2.set_title('Undistorted Image', fontsize=20)",
        "text": "apply distortion correction to raw image",
        "id": 931
    },
    {
        "code": "addresses = df_train.groupby('Address').groups\nprint('Total locations: {}'.format(len(addresses.keys())))",
        "text": "lot of overlap  .  let 's look at the unique location  . ",
        "id": 932
    },
    {
        "code": "countries['density'] = countries['population']*1000000 / countries['area']\ncountries",
        "text": "add a new column to the dataframe be very simple ,",
        "id": 933
    },
    {
        "code": "record = ('Dave', 'dave@example.com', '773-555-1212', '847-555-1212')\nname, email, *phone_numbers = record\nname\nemail\nphone_numbers",
        "text": "a another use case , suppose you have user record that consist of a name and email address , follow by an arbitrary number of phone number  . ",
        "id": 934
    },
    {
        "code": "df = pd.concat([df, pd.get_dummies(df[\"label\"], prefix = \"cluster\")], axis = 1)\ndf.head()\nmodel = LinearRegression()\nX = df[[\"x1\", \"cluster_0\", \"cluster_1\", \"cluster_2\"]]\ny = df[\"x2\"]\nmodel.fit(X, y)\nprint(model.score(X, y))\nplt.scatter(df[\"x1\"], df[\"x2\"], c = df[\"color\"])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.scatter(df[\"x1\"], model.predict(X), color = \"black\")\nplt.show()",
        "text": "add cluster label back to the data frame and fit a linear model",
        "id": 935
    },
    {
        "code": "\nI_rate=0.03 \nI_pop=1 \nS_pop=100 \nN_pop=I_pop+S_pop \nRec_rate=0.001 \npackage=(I_rate,Rec_rate,S_pop,N_pop) \ntime=0 \nprint(rate_infected(I_pop,package,time)) #printing the output",
        "text": "for example , if the rate of infection ( beta ) be 0 . 03 , the infect population ( i ) be 1 , the susceptible population be 100 , the total population be 101 , and the recovery rate ( gamma ) be 0 . 001 , then the rate of change in the infect population will be ,",
        "id": 936
    },
    {
        "code": "plot_position(system.results)\nsavefig('chap09-fig01.pdf')",
        "text": "here 's what it look like  . ",
        "id": 937
    },
    {
        "code": "M = np.matrix(A)\nv = np.matrix(v1).T \nM\nv\nM * M\nM * v",
        "text": "we can cast the array object to the type matrix   .  this change the behavior of the standard arithmetic operator + ,   -   , * to use matrix algebra  . ",
        "id": 938
    },
    {
        "code": "interests_by_user_id = defaultdict(list)\nfor user_id, interest in interests:\n    interests_by_user_id[user_id].append(interest)",
        "text": "index of user by interest",
        "id": 939
    },
    {
        "code": "from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer(max_features=1500)\nX=cv.fit_transform(corpus).toarray()\nY=Data.iloc[:,1].values",
        "text": "create the bag of word model",
        "id": 940
    },
    {
        "code": "dog = word_vector_model['dog']\nprint(\"General Shape of word -> dog\",dog.shape)\nprint(dog[:10])",
        "text": "now check the shape of the word present in the pretrained word2vec model dictionary",
        "id": 941
    },
    {
        "code": "feature_list = ['bedrooms',  \n                'bathrooms',  \n                'sqft_living',  \n                'sqft_lot',  \n                'floors',\n                'waterfront',  \n                'view',  \n                'condition',  \n                'grade',  \n                'sqft_above',  \n                'sqft_basement',\n                'yr_built',  \n                'yr_renovated',  \n                'lat',  \n                'long',  \n                'sqft_living15',  \n                'sqft_lot15']\nfeatures_train, output_train = get_np_data(train, feature_list, 'price')\nfeatures_test, output_test = get_np_data(test, feature_list, 'price')\nfeatures_valid, output_valid = get_np_data(validation, feature_list, 'price')",
        "text": "extract feature and normalize use all of the numerical input list in feature _ list  , transform the train , test , and validation sframes into numpy array ,",
        "id": 942
    },
    {
        "code": "plt.plot(np.cumsum(sig**2)/np.sum(sig**2))\nplt.show()",
        "text": "out of curiosity , let 's plot the spectrum to get a sense of how independent the feature be  . ",
        "id": 943
    },
    {
        "code": "landing_page = []\ngoal_one = []\nwith open('task_data/Goal1CompletionLocation_Goal1Completions.json') as data_file:    \n    data = json.load(data_file)\nlist_of_dict = data['reports'][0]['data']['rows']\nfor dictionary in list_of_dict:\n    landing_page.append(dictionary['dimensions'][0])\n    goal_one.append(int(dictionary['metrics'][0]['values'][0]))\ngoals_df = pd.DataFrame(data = {'page': landing_page, 'goal_one': goal_one})\ngoals_df#.head()",
        "text": "work on first json file",
        "id": 944
    },
    {
        "code": "predict_train = training.map(lambda x: (x[0],x[1]))\npredictall_train = model.predictAll(predict_train).map(lambda  x: ((x[0],x[1]),x[2]))\npredictall_train.take(5)",
        "text": "< font color = purple size = 4 >   prediction on train data to check the model accuracy",
        "id": 945
    },
    {
        "code": "output_file = \"../data/cleaned_coal_data.csv\"\nimport numpy as np\nimport pandas as pd\ndf = pd.read_excel(\"../data/coalpublic2013.xls\", header=2, index_col='MSHA ID')\ndf['Company Type'].unique()\ndf['Company Type'].replace(to_replace='Indepedent Producer Operator',\n                           value='Independent Producer Operator',\n                           inplace=True).head()\ndf.rename(columns=lambda x:x.replace(\" \",\"_\"),inplace=True)\ndf = df[df['Production_(short_tons)']>0]\nlen(df)\ndf['log_production'] = np.log(df['Production_(short_tons)'])\ndf.to_csv(output_file)",
        "text": "data clean by   -  clean up the data   -   remove 0 production coal mine",
        "id": 946
    },
    {
        "code": "magic_cards = pd.read_csv(\"MagicDatasets/Magic_Pandas_DF\")\nmagic_cards.info()\nmagic_cards = magic_cards.drop(magic_cards.columns[0], axis=1)",
        "text": "data pull from kaggle dataset  . ",
        "id": 947
    },
    {
        "code": "\nfeature_cols = [x for x in data.columns if x not in 'color']\nfrom sklearn.model_selection import StratifiedShuffleSplit\nstrat_shuff_split = StratifiedShuffleSplit(n_splits=1, test_size=1000, random_state=42)\ntrain_idx, test_idx = next(strat_shuff_split.split(data[feature_cols], data['color']))\nX_train = data.loc[train_idx, feature_cols]\ny_train = data.loc[train_idx, 'color']\nX_test = data.loc[test_idx, feature_cols]\ny_test = data.loc[test_idx, 'color']",
        "text": "use stratifiedshufflesplit  to split data into train and test set that be stratify by wine quality  .  if possible , preserve the index of the split for question 5 below   -  check the percent composition of each quality level for both the train and test data set  . ",
        "id": 948
    },
    {
        "code": "\ntitanic.drop(['ticket','boat','body','home.dest'], axis=1, inplace=True)\ntitanic.info()",
        "text": "get rid of column ( variable , feature ) that be not useful and row that contain no data  . ",
        "id": 949
    },
    {
        "code": "ustotaldata[ustotaldata.Source==\"Total\"].sort_values(by='co2diff').tail()\nustotaldata[ustotaldata.Source==\"Total\"][\"co2\"].diff(1).plot(color=\"green\")\nplt.title('Plot of year-over-year differences in CO2 emissions for All sources')",
        "text": "identify the year with large year   -   over   -   year increase in co2 emission",
        "id": 950
    },
    {
        "code": "\nX = tf.placeholder(tf.float32, shape=[None, n_inputs])\ny = tf.placeholder(tf.int64)",
        "text": "placeholder for input and label",
        "id": 951
    },
    {
        "code": "signal = thinkdsp.UncorrelatedGaussianNoise()\nwave = signal.make_wave(duration=0.49, framerate=11025)\nwave.plot(linewidth=1)\nthinkplot.config(xlabel='time',\n                 ylabel='amplitude',\n                 legend=False)",
        "text": "an alternative to uu noise be uncorrelated gaussian ( ug noise )  . ",
        "id": 952
    },
    {
        "code": "\nbatch_size = 64\nmodel.fit(train_X, train_Y, batch_size=batch_size, epochs=EPOCHS, validation_split=0.2, verbose=1, shuffle=True)\nloss, acc = model.evaluate(test_X, test_Y, batch_size=batch_size, verbose=1)\nprint('')\nprint('Got %.2f%% accuracy' % (acc * 100.))",
        "text": "test time now it 's time to actually test the network  .  do n't change any of the parameter here except for the batch size  .  get above   -  65 %   - ",
        "id": 953
    },
    {
        "code": "aql = pd.melt(airquality, id_vars=['Month', 'Day'],\n              var_name='climate_variable',\n              value_name='climate_value')\naql.head()",
        "text": "r code ,   -    r aql   -  melt ( airquality , id . vars = c (  month  ,  day  ) , variable . name =  climate _ variable  , value . name =  climate _ value  ) head ( aql )     -  python equivalent ,   - ",
        "id": 954
    },
    {
        "code": "x = np.random.random_sample(100)*100\ny = x + np.random.normal(np.random.normal(0,15), 30, size=100) + 100\nplt.figure(figsize=(10,8))\nplt.scatter(x, y, s=70, c='steelblue')\nplt.show()",
        "text": "run gradient descent on regression data first let make some x and y variable like we do yesterday  . ",
        "id": 955
    },
    {
        "code": "\na  = tensor(destroy(N), qeye(2))\nsm = tensor(qeye(N), destroy(2))\nna = sm.dag() * sm  \nnc = a.dag() * a    \nH0 = wc * a.dag() * a + wa * sm.dag() * sm\nif use_rwa:\n    H1 = (a.dag() * sm + a * sm.dag())\nelse:\n    H1 = (a.dag() + a) * (sm + sm.dag())\nH1 = H0 + g * H1\nfull_energy, full_eigenstate = H.eigenstates()\nH",
        "text": "setup the operator and the hamiltonian",
        "id": 956
    },
    {
        "code": "first_pipeline = Pipeline([\n    ('pca', PCA()),\n    ('skb', SelectKBest(k=40)),\n    ('rf', RandomForestClassifier())\n])\nfirst_pipe_params = {\n    'rf__n_estimators':[10,100],\n    'rf__max_depth':[10,40,None]\n}",
        "text": "decide on a pipeline for data to pas through",
        "id": 957
    },
    {
        "code": "unempl = pd.Series([6.0, 6.0, 6.1], index=[2, 3, 4])\ndf_3['unempl'] = unempl\ndf_3",
        "text": "assign a series to a column ( note if assign a list or array , the length must match the dataframe , unlike a series ) ,",
        "id": 958
    },
    {
        "code": "from __future__ import division\ndef c2f(Celsius):\n    Fahrenheit = Celsius*9/5 + 32\n    return(Fahrenheit)\ndef f2c(Fahrenheit):\n    Celsius = 5*(Fahrenheit-32) / 9\n    return(Celsius)\n[c2f(0), c2f(100), f2c(32), f2c(212)]",
        "text": "suppose we want to convert between c ( celsius ) and f ( fahrenheit ) , use the equation 9*c = 5* ( f   -   32 )   -   write function c2f  and f2c   -   do all computation in float point for this problem",
        "id": 959
    },
    {
        "code": "bs_medians = np.array(bs_medians)\nplt.plot(N_bs, np.mean(bs_medians, axis=1))\nplt.xscale('log')\nplt.xlabel('Sample size N')\nplt.ylabel('Mean of bootstraps')\nplt.plot(N_bs, np.abs(np.mean(bs_medians, axis=1) - 1), label='|Mean bootstrap - 1|')\nplt.plot(N_bs, np.std(bs_medians, axis=1), label='Bootstrap std dev')\nplt.plot(N_bs, standard_median_rms, label='Usual formula')\nplt.xscale('log')\nplt.xlabel('Sample size N')\nplt.ylabel('Std dev of bootstraps')\nplt.legend();",
        "text": "we can plot the mean from each ,",
        "id": 960
    },
    {
        "code": "X_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train_norm = X_train / 255\nX_test_norm = X_test / 255\nX_train_dense = X_train_dense.astype('float32')\nX_test_dense = X_test_dense.astype('float32')\nX_train_dense = X_train_dense / 255\nX_test_dense = X_test_dense / 255",
        "text": "normalize image data greyscale pixel data be store a integer value between 0   -   255  .    neural network expect input between 0   -   1 , so divide each pixel by 255 to normalize it  .  also change the",
        "id": 961
    },
    {
        "code": "\nprint(KeyPpl.dtypes)\nfor col in KeyPpl.columns[1:]:\n    print(KeyPpl[col].unique())\nprint(KeyPpl[KeyPpl.columns[1:]].sum())\nprint(pd.value_counts(KeyPpl[KeyPpl.columns[1:]].sum(axis=1)))\nprint(KeyPpl.shape)\nKeyPpl_mod = KeyPpl[KeyPpl.columns[1:]] \nKeyPpl_mod.name = \"KeyPpl\"",
        "text": "keyppl ( nothing much to change )",
        "id": 962
    },
    {
        "code": "img = cv2.imread(\"test_images/straight_lines1.jpg\")\ndst = cv2.undistort(img, mtx, dist, None, mtx)\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\nimg = bgr_to_rgb(img)\nax1.imshow(img)\nax1.set_title('Original Image', fontsize=30)\ndst = bgr_to_rgb(dst)\nax2.imshow(dst)\nax2.set_title('Undistorted Image', fontsize=30)",
        "text": "apply a distortion correction to raw image",
        "id": 963
    },
    {
        "code": "\nX = tf.placeholder(tf.float32, shape=[None, 3])\nY = tf.placeholder(tf.float32, shape=[None, 1])",
        "text": "placeholder for a tensor that will be always feed  . ",
        "id": 964
    },
    {
        "code": "logits = LeNetTrafficSign(x, n_classes)\nprint(logits)\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\nloss_operation = tf.reduce_mean(cross_entropy)\noptimizer = tf.train.AdamOptimizer(learning_rate = rate)\ntraining_operation = optimizer.minimize(loss_operation)",
        "text": "train pipeline create a train pipeline that us the model to classify traffic signdata  . ",
        "id": 965
    },
    {
        "code": "selected_words_model.evaluate(test_data)\nsentiment_model.evaluate(test_data)\nselected_words_model.evaluate(test_data, metric='roc_curve')\nselected_words_model.show(view='Evaluation')",
        "text": "compare the accuracy of different sentiment analysis model ,",
        "id": 966
    },
    {
        "code": "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveRunConfig, PrimaryMetricGoal\nfrom azureml.train.hyperdrive import choice, loguniform\nps = RandomParameterSampling(\n    {\n        '--batch-size': choice(25, 50, 100),\n        '--first-layer-neurons': choice(10, 50, 200, 300, 500),\n        '--second-layer-neurons': choice(10, 50, 200, 500),\n        '--learning-rate': loguniform(-6, -1)\n    }\n)",
        "text": "intelligent hyperparameter tune we have train the model with one set of hyperparameters , now let 's how we can do hyperparameter tune by launch multiple run on the cluster  .  first let 's define the parameter space use random sample  . ",
        "id": 967
    },
    {
        "code": "df['DECISION_DATE'] = df['DECISION_DATE'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y').date())\ndf['CASE_SUBMITTED'] = df['CASE_SUBMITTED'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y').date())",
        "text": "convert the date into datetime object",
        "id": 968
    },
    {
        "code": "\nX = tf.placeholder(tf.float32, shape=[None, 1])\nY = tf.placeholder(tf.float32, shape=[None, 1])",
        "text": "placeholder for a tensor that will be always feed  . ",
        "id": 969
    },
    {
        "code": "clf=sklm.LinearRegression()\nclf.fit(xtrain, ytrain)\nscore=clf.score(xtest, ytest)\nprint(score)\nerrorplot(clf, xtest)",
        "text": "now we try fit various ml model",
        "id": 970
    },
    {
        "code": "\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=64)\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))\n# near state-of-the-art results",
        "text": "lstm for sequence classification in the imdb dataset",
        "id": 971
    },
    {
        "code": "lasso = Lasso(fit_intercept=True, alpha=0.01)\nlasso.fit(x_train, y_train)    \npred = lasso.predict(x_test)\nMAE = mean_squared_error(y_test, pred)\nprint('Mean absolute error on test data: %0.8f' % MAE,'using alpha = 0.01')",
        "text": "so we can see that the alpha value of 0 . 01 give the low mae on train data and we can use that to run the model on set aside test data  . ",
        "id": 972
    },
    {
        "code": "param_grid = {'min_samples_split': range(2,10),\n              'min_samples_leaf': range(1,10)}\nmodel_rf = GridSearchCV(ensemble.RandomForestClassifier(n_estimators=10), param_grid, cv=3, iid=False)\nmodel_rf.fit(X_train, y_train)\nbest_index = np.argmax(model_rf.cv_results_[\"mean_test_score\"])\nprint(\"Best parameter values:\", model_rf.cv_results_[\"params\"][best_index])\nprint(\"Best Mean cross-validated test accuracy:\", model_rf.cv_results_[\"mean_test_score\"][best_index])\nprint(\"Overall Mean test accuracy:\", model_rf.score(X_test, y_test))",
        "text": "let 's do another grid search to determine the best parameter ,",
        "id": 973
    },
    {
        "code": "\npmode_ACCtime, p_ACCtime, pll_ACCtime, pul_ACCtime, sigma2e_ACCtime = RunEM(resp_values_acc)",
        "text": "application of binary filter/smoother to acc data across time",
        "id": 974
    },
    {
        "code": "nona =  raw[pd.notnull(raw['shot_made_flag'])]\nnona.head()",
        "text": "drop nan we be gon na make a variable without nan for our exploratory analysis  . ",
        "id": 975
    },
    {
        "code": "for a in range(1,1001):\n    for b in range(1,1001):\n        c = 1000 - a - b\n        if a**2 + b**2 == c**2:\n            print(a,b,c)\n            print(a*b*c)",
        "text": "special pythagorean triplet a pythagorean triplet be a set of three natural number , a < b < c , for which , a2 + b2 = c2 for example , 32 + 42 = 9 + 16 = 25 = 52 .  there exist exactly one pythagorean triplet for which a + b + c = 1000 .  find the product abc   -  problem 9   -   solution",
        "id": 976
    },
    {
        "code": "plt.rcParams['figure.figsize'] = 16, 12\nplt.subplot(321)\npairPlot(QDA, 0, 1)\nplt.subplot(322)\npairPlot(QDA, 0, 2)\nplt.subplot(323)\npairPlot(QDA, 0, 3)\nplt.subplot(324)\npairPlot(QDA, 1, 2)\nplt.subplot(325)\npairPlot(QDA, 1, 3)\nplt.subplot(326)\npairPlot(QDA, 2, 3)\nplt.show()",
        "text": "below we visualize the decision boundary for each pair of feature  . ",
        "id": 977
    },
    {
        "code": "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveRunConfig, PrimaryMetricGoal\nfrom azureml.train.hyperdrive import choice, loguniform\nps = RandomParameterSampling(\n    {\n        '--batch-size': choice(25, 50, 100),\n        '--first-layer-neurons': choice(10, 50, 200, 300, 500),\n        '--second-layer-neurons': choice(10, 50, 200, 500)     \n    }\n)",
        "text": "intelligent hyperparameter tune we have train the model with one set of hyperparameters , now let 's how we can do hyperparameter tune by launch multiple run on the cluster  .  first let 's define the parameter space use random sample  . ",
        "id": 978
    },
    {
        "code": "if (rebuild_from_data_download == True):\n    for c in DATA_CLASSES:\n        make_dir(DATA_HOME_DIR + 'sample/train/' + c)\n        make_dir(DATA_HOME_DIR + 'sample/valid/' + c)\n        make_dir(DATA_HOME_DIR + 'sample/test/unknown')\n        make_dir(DATA_HOME_DIR + 'valid/' + c)\n        \n    make_dir(DATA_HOME_DIR + 'test/unknown')",
        "text": "create train , validation , test , and sample directory",
        "id": 979
    },
    {
        "code": "sentences_strings_ted = []\nfor line in input_text_noparens.split('\\n'):\n    m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n    sentences_strings_ted.extend(sent for sent in m.groupdict()['postcolon'].split('.') if sent)\nsentences_strings_ted[:5]\nm",
        "text": "now , let 's attempt to    -  remove speaker  name that occur at the begin of a line , by delete piece of the form   < up to 20 character > ,   , a show in this example  .  of course , this be an imperfect heuristic  . ",
        "id": 980
    },
    {
        "code": "def group_by_age(df, bins=None):\n    if bins is None:\n        bin_size = 5\n        _min, _max = int(df.age_years.min()), int(df.age_years.max())\n        bins = range(_min, _max + bin_size, 5)\n    return df.groupby(pd.cut(df.age_years, bins=bins))\ndata_by_age = churn_df.pipe(group_by_age)\nloss_by_age_df = data_by_age['predicted_loss'].sum().reset_index()\nloss_by_age_df['age_years'] = loss_by_age_df['age_years'].astype(str)\nloss_by_age_df.plot(x='age_years', y='predicted_loss', style='o')",
        "text": "loss by age group in this section , we calculate and plot the predict loss of revenue by age group  .  in our data set , age be an important feature in predict if a client will churn  .  we create a dataframe contain the cumulative predict loss by age group  . ",
        "id": 981
    },
    {
        "code": "apple_isnull = data[\"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Apple\"].isnull()\npumpkin_isnull = data[\"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Pumpkin\"].isnull()\npecan_isnull = data[\"Which type of pie is typically served at your Thanksgiving dinner? Please select all that apply. - Pecan\"].isnull()\nate_pies = ~(apple_isnull & pumpkin_isnull & pumpkin_isnull)\nate_pies.value_counts()",
        "text": "how many people eat apple , pecan , or pumpkin pie during thanksgiving dinner",
        "id": 982
    },
    {
        "code": "g = \"global\"\ndef outer(p='param'):\n    l = \"local\"\n    def inner():\n        print(g, p, l)\n    inner()\nouter()\ninner()\nouter.inner()",
        "text": "legb rule local function be bind by the legb rule  .  local enclose , global , build   -   in local function be not member of the contain function in any way  .  it be just a local name bind  . ",
        "id": 983
    },
    {
        "code": "reviews['province'].value_counts().head(20).plot.bar()\n(reviews['province'].value_counts().head(20)/len(reviews)).plot.bar()\nreviews['points'].value_counts().sort_index().plot.bar()",
        "text": "bar chart and categorical data",
        "id": 984
    },
    {
        "code": "b_list.append('foo')\nb_list\nb_list.remove('foo')\nb_list",
        "text": "element can be remove by value use remove , which locate the first such value and remove it from the list ,",
        "id": 985
    },
    {
        "code": "\nprojects_data['mjtheme_namecode'].head(3)\njsonData = json.load((open('data/world_bank_projects.json')))\nprojects_data_norm = json_normalize(jsonData, 'mjtheme_namecode', [['_id', '$oid'], 'countrycode','countryshortname'])\nprojects_data_norm\ndata = projects_data_norm.copy()\ndata = data.replace({'name': r'\\s*'}, np.nan)\ndata.head()\ndata.iloc[1]['name']\ndata.replace(r'\\s+',np.nan,regex=True).replace('',np.nan)",
        "text": "find the top 10 major project theme ( use column mjtheme _ namecode  )",
        "id": 986
    },
    {
        "code": "fig = plt.figure(figsize=(8,5))\nax = fig.gca()\nax.plot(days, morale_true, lw=7., c='gold', alpha=0.3, label='true function')\nax.scatter(students['A']['days'], students['A']['morale'],\n           s=70, c='darkred', label='student A', alpha=0.7)\nax.set_xlabel('days', fontsize=16)\nax.set_ylabel('morale', fontsize=16)\nax.set_title('Morale over time\\n', fontsize=20)\nax.set_xlim([0, 85])\nplt.legend(loc='upper left')\nplt.show()",
        "text": "< a id=student   -   a  >      -  student a 's morale over time    -  below we can plot student a 's morale at each day  .  the true function be also plot in yellow  . ",
        "id": 987
    },
    {
        "code": "unmatched_schools = schools.loc[schools['match_id'].isnull()]\nlen(unmatched_schools)\nunmatched_schools['cleaned_name']",
        "text": "check school with no budget row which school be not match with a budget row",
        "id": 988
    },
    {
        "code": "np.random.seed(0) \nsim1 = np.random.poisson(lam = t1_avg, size = 500)\nsim2 = np.random.poisson(lam = t2_avg, size = 500)\nassert len(sim1)==n\nassert len(sim2)==n\nassert sim1.dtype==np.dtype(int)\nassert sim2.dtype==np.dtype(int)\nassert np.abs(sim1.mean()-t1_avg) < 0.05\nassert np.abs(sim2.mean()-t2_avg) < 0.05",
        "text": "simulate  n  game for each team use a poisson distribution $ poi ( \\lambda ) $ with $ \\lambda $ choose appropriately for the team  .  store the number of goal for each team in a numpy array name sim1  and sim2  ,",
        "id": 989
    },
    {
        "code": "a=np.arange(12)**2\na\ni=np.array([1,1,3,8,5])\na[i]\nj=np.array([[3,4],[9,7]])\na[j]",
        "text": "index with array of index",
        "id": 990
    },
    {
        "code": "pops_change_1516 = pops_1516.merge(change_1516, on='id2')\npops_change_1516.shape \npops_change_1516.head(1)",
        "text": "now we need to merge the dataframe contain the population data with the dataframe contain the population change data  . ",
        "id": 991
    },
    {
        "code": "data['density'] = data['pop'] / data['area']\ndata",
        "text": "add a new column to a dataframe",
        "id": 992
    },
    {
        "code": "a=np.array([[1,2],[3,4],[5,6]]) \na\nv=[1,2,3]   \nv\nv=np.array([1,2,3]) \nv",
        "text": "numpy basic    -  vector and matrix",
        "id": 993
    },
    {
        "code": "multi_filter_dragonn_parameters = {\n    'seq_length': 500,\n    'num_filters': [15], \n    'conv_width': [45],\n    'pool_width': 45,\n    'dropout': 0.1}\nmulti_filter_dragonn = get_SequenceDNN(multi_filter_dragonn_parameters)\ntrain_SequenceDNN(multi_filter_dragonn, simulation_data)\nSequenceDNN_learning_curve(multi_filter_dragonn)",
        "text": "a multi   -   filter dragonn model next , we modify the model to have 15 convolutional filter instead of just one filter  .  will the model learn now ?",
        "id": 994
    },
    {
        "code": "import numpy as np\nn = [1, 2, 7, 21, 3, 1, 62, 3, 34, 12, 73, 44, 12, 11, 9]\nn_bin = []\nn_mean = np.mean(n)\nfor x in n:\n    if x >= n_mean:\n        n_bin.append(1)\n    else:\n        n_bin.append(0)\nprint(n_bin)\nn = [1, 2, 7, 21, 3, 1, 62, 3, 34, 12, 73, 44, 12, 11, 9]\nn_mean = np.mean(n)\nn_bin = []\nfor x in n:\n    if x >= n_mean:\n        n_bin.append(1)\n    else:\n        n_bin.append(0)",
        "text": "conditional logic in list comprehension list comprehension can be extend to cover more of the functionality of a for loop than just an operation over element  .  let 's say we want to  binarize  a variable base on whether the element be great or le than the mean over all element  .  the for loop could look something like this  . ",
        "id": 995
    },
    {
        "code": "\nviews = [50, 100, 200, 400, 800]\ny_a = [get_pdf(x, site_a[:view]) for view in views]\nlabels = ['Posterior After {} Views'.format(view) for view in views]\nplot_with_fill(x, y_prior, 'Prior')\nfor y, label in zip(y_a, labels):\n    plot_with_fill(x, y, label)\nplt.title('Site A')\nplt.xlabel('Click Through Rate')\nplt.xlim([0, 0.4]);",
        "text": "after 50 view , we re start to hone in on our prediction of *pa  -  overlay on the same graph the posterior after 50 view , 100 view , 200 view , 400 view and finally all 800 view  .  you should see a time progress that we get more certain of the true value of *pa  - ",
        "id": 996
    },
    {
        "code": "object_evidence=[total_neg_pos_evidence(i) for i in range(len(X_test))]\nobject_pos_evidence=[object_evidence[i][1] for i in range(len(X_test))]\nidx=np.argmax(object_pos_evidence)\nprint_info(idx)",
        "text": "the object that ha the large positive evidence",
        "id": 997
    },
    {
        "code": "import numpy as np\nniter = 1000\nEz_t = np.zeros((niter,sim.nx))\ntmax = niter * sim.dt\nprint(\"\\nRunning simulation up to t = {:g} ...\".format(tmax))\nwhile sim.t <= tmax:\n    print('n = {:d}, t = {:g}'.format(sim.n,sim.t), end = '\\r')\n    Ez_t[sim.n,:] = sim.emf.Ez\n    sim.iter()\nprint(\"\\nDone.\")",
        "text": "we run the simulation up to a fix number of iteration , control by the variable niter  , store the value of the em field $ e _ y $ and $ e _ z $ at every timestep so we can analyze them late ,",
        "id": 998
    },
    {
        "code": "import tarfile\ndataset_folder_path = 'flower_photos'\nclass DLProgress(tqdm):\n    last_block = 0\n    def hook(self, block_num=1, block_size=1, total_size=None):\n        self.total = total_size\n        self.update((block_num - self.last_block) * block_size)\n        self.last_block = block_num\n        \nif not isfile('flower_photos.tar.gz'):\n    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Flowers Dataset') as pbar:\n        urlretrieve(\n            'http://download.tensorflow.org/example_images/flower_photos.tgz',\n            'flower_photos.tar.gz',\n            pbar.hook)\n        print(\"total: \",pbar.total)\nif not isdir(dataset_folder_path):\n    with tarfile.open('flower_photos.tar.gz') as tar:\n        tar.extractall()\n        tar.close()",
        "text": "flower power here we ll be use vggnet to classify image of flower  .  to get the flower dataset , run the cell below  .  this dataset come from the [ tensorflow inception tutorial ] ( <url> )  . ",
        "id": 999
    },
    {
        "code": "_ = normalized_training_examples.hist(bins=20, figsize=(18, 12), xlabelsize=10)",
        "text": "task 3 , explore alternate normalization method   -  try alternate normalization for various feature to further improve performance   -  if you look closely at summary stats for your transform data , you may notice that linear scale some feature leaf them clump close to    -   1   .  for example , many feature have a median of    -   0 . 8  or so , rather than  0 . 0   . ",
        "id": 1000
    }
]