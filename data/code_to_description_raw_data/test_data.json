[
    {
        "code": "frame = DataFrame(np.arange(8).reshape((2,4)), index = ['three', 'one'],\n                  columns = ['d', 'a', 'b', 'c'])\nframe\nframe.sort_index() \nframe.sort_index(axis = 1) # columns",
        "text": "with dataframe , can sort by index on either axis ,",
        "id": 1
    },
    {
        "code": "Stress_Indicator_2011 = pd.read_excel('Data\\DATA Storebælt\\Stress\\DD2011.xlsx', header=None)\nStress_Indicator_2011 = Stress_Indicator_2011.T\nStress_Indicator_2011.columns = ['SG1', 'SG2','SG3','SG4','SG5','SG6','SG7','SG8','SG9','SG10','SG11','SG12','SG13','SG14','SG15']\nStress_Indicator_2011=Stress_Indicator_2011.drop(['SG10','SG11','SG12','SG13','SG14','SG15'], axis=1)\nStress_Indicator_2011.index = pd.date_range('2011-01-01', '2011-12-31', freq='d', )\n#Stress_Indicator_2011.head()",
        "text": "stress performance indicator   -   2011",
        "id": 2
    },
    {
        "code": "mean_per_column = housing_data_train_numeric.apply(lambda x: x.mean(),axis=0)\nnumeric_mean_filled = housing_data_train_numeric.fillna(mean_per_column,axis=0)\nhousing_data_train_categories.describe()",
        "text": "impute numeric data with the mean value",
        "id": 3
    },
    {
        "code": "import numpy as np\narr = np.arange(0,11)\narr\narr + arr\narr - arr\narr * arr\narr + 100\narr * 100\narr / arr\n1 / arr\narr ** 2\nnp.sqrt(arr)\nnp.exp(arr)\nnp.log(arr)",
        "text": "numpy operation array with array array with scalar universal array function",
        "id": 4
    },
    {
        "code": "def wrap_int64(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))",
        "text": "helper   -   function for wrap an integer so it can be save to the tfrecord file  . ",
        "id": 5
    },
    {
        "code": "\nm = 32\nmp = matrixProfile.stomp(signal_df.signal.values, m)\nprint(len(mp), len(mp[0]), len(mp[1]))\nmp_adj = np.append(mp[0],np.zeros(m-1)+np.nan)\nprint(len(mp_adj))",
        "text": "calculate output ( matrix profile )",
        "id": 6
    },
    {
        "code": "print (filename)\ndataIn.to_csv(filename, sep=',',index=False)\ndataIn.to_csv(filenamecopy, sep=',',index=False)\nprint ('Complete')",
        "text": "manual overwrite ( just for good measure )",
        "id": 7
    },
    {
        "code": "\nfrom pyspark.sql.functions import *\ndataDF.filter(dataDF.age > 20)\\\n      .select(concat(dataDF.first_name, lit(' '), dataDF.last_name).alias('full_name'), dataDF.occupation)\\\n      .show(10)",
        "text": "to make the expert cod style more readable , enclose the statement in parenthesis and put each method , transformation , or action on a separate line  . ",
        "id": 8
    },
    {
        "code": "bat_df[(bat_df['H'] >= 200) & (bat_df['HR'] >=30) & (bat_df['2B'] >= 40) & (bat_df['3B'] >= 10)]['playerID'].unique().size",
        "text": "how many player have hit 40 2bs , 10 3bs , 200 hit , and 30 hr ( inclusive ) in one season ?",
        "id": 9
    },
    {
        "code": "\ndef kmeans(data, k=2, centroids=None):\n    \n    if not centroids:\n        centroids = randomize_centroids(data,k)\n    old_centroids = centroids[:]\n    iterations = 0\n    while True:\n        iterations += 1\n        \n        clusters = [[] for i in range(k)]\n        \n        for datapoint in data:\n            \n            centroid_idx = find_closest_centroid(datapoint,centroids)\n            \n            \n            clusters[centroid_idx].append(datapoint)\n        \n        \n        old_centroids = centroids[:]\n        \n        \n        centroids = update_centroids(centroids,clusters)\n        \n        if check_converge(centroids, old_centroids, iterations, threshold=0):\n            break\n    return centroids",
        "text": "< div class=anchor  >      -  question 1 . 6 complete the k   -   mean algorithm scheleton below , with the function you write above  . ",
        "id": 10
    },
    {
        "code": "df = pd.read_csv(\"http://www.ats.ucla.edu/stat/data/binary.csv\")\nsample = df.head(50)\nsample",
        "text": "read data from csv file",
        "id": 11
    },
    {
        "code": "data_pandas['business_major'] = ['yes','no','yes','yes','yes','no','yes']\ndata_pandas['years_experience'] = [1,4,2,6,0,3,0]\ndata_pandas",
        "text": "we can also add column ( they should have the same number of row a the dataframe they be be add to  .  )",
        "id": 12
    },
    {
        "code": "\nvar_portion_std = eig_val_std/eig_val_std.sum()\nprint ('Portion of variance in First PC:', var_portion_std[0])\nprint ('\\nPortion of variance in Second PC:', var_portion_std[1])\nprint ('\\nPortion of variance in Third PC:', var_portion_std[2])\nprint ('\\nPortion of variance in Fourth PC:', var_portion_std[3])\nprint ('\\nWe can observe that 72.77% of variance in data is contained in the first PC, 23.03% of variance in data')\nprint ('is contained in the second PC and other components have very less portion of variance.')\nprint ('\\nHence, We can conclude that only 2 true components are needed to represent the data, together containing')\nprint ('95.8% of variance in data')",
        "text": "$ \\textbf { portion of the variance contain in each of the pc } $",
        "id": 13
    },
    {
        "code": "\ngenerate_learningcurves(gX_train=gX_train[:,[0,3,1,5,2,8,4,19,20,7]], gy_train=gy_train,\n                        alg=RandomForestRegressor(), alg_name=\"Random forests with top 12 features\")",
        "text": "rerun learn curve with top12 feature",
        "id": 14
    },
    {
        "code": "\nrfc = ensemble.RandomForestClassifier(bootstrap=False, min_samples_leaf=5, min_samples_split=2, n_estimators=20,\n                                     max_features='sqrt', max_depth=20, n_jobs=-1)\ncv_models(rfc, 5)",
        "text": "random forest classifier 2nd attempt run the model with randomizedsearchcv 's  best parameter   . ",
        "id": 15
    },
    {
        "code": "x = tf.placeholder(tf.float32, (None, 32, 32, 3), name='x')\ny = tf.placeholder(tf.int32, (None), name = 'y')\none_hot_y = tf.one_hot(y, 43)",
        "text": "feature and label train the network to classify the german traffic sign data  .   x  be a placeholder for a batch of input image  .   y  be a placeholder for a batch of output label  . ",
        "id": 16
    },
    {
        "code": "a = ENSAEPROJECT()\na.feature_correlation_removal = True\na.create_datasets()\nprint(a.X_train.shape, a.X_test.shape, a.X_train_cv.shape, a.X_valid_cv.shape)\na.set_model(\"gbc\")\na.do_cv = True\na.run_model()",
        "text": "gradient boost , no automatic feature selection , feature correlation removal",
        "id": 17
    },
    {
        "code": "from scipy.stats import norm\nfrom numpy import linspace\nfrom pylab import plot,show,hist,figure,title\nsample = norm.rvs(loc=0,scale=1,size=100) \npar = norm.fit(sample) \nprint (par)\nx = linspace(-5,5,100)\npdf_fitted = norm.pdf(x,loc=par[0],scale=par[1])\npdf = norm.pdf(x)\ntitle('Normal distribution')\nhist(sample,normed=1,alpha=.3)\nplot(x,pdf_fitted,'r-',x,pdf,'b-')\nshow()",
        "text": "example , histogram fit here be an example of an unbinned *max   -   likelihood* fit of a set of event to a gaussian pdf courtesy <url>",
        "id": 18
    },
    {
        "code": "red_wines_df = pd.read_csv('../data/winequality-red.csv', delimiter=';')\nwhite_wines_df = pd.read_csv('../data/winequality-white.csv', delimiter=';')\nred_wines_df.columns\nwhite_wines_df.columns\nred_wines_quality_df = red_wines_df.groupby('quality').mean()['fixed acidity'].reset_index()\nred_wines_quality_df.head()\nwhite_wines_quality_df = white_wines_df.groupby('quality').mean()['fixed acidity'].reset_index()\nwhite_wines_quality_df.head()\npd.merge(red_wines_quality_df, white_wines_quality_df, on=['quality'], suffixes=[' red', ' white'])",
        "text": "let 's read in a different data set , since we re look at combine multiple data source  . ",
        "id": 19
    },
    {
        "code": "\nu = np.sqrt(2)/(1+np.sqrt(3))\nv = np.sqrt(6)/(1+np.sqrt(3))",
        "text": "enter the gain of the two record to get the clean speech  .  note , the square root of a number $ a $ can be write a  np . sqrt ( a )   in ipython  . ",
        "id": 20
    },
    {
        "code": "temp=station_id[['Monthly_Avg', 'Summer_Avg', 'Winter_Avg']]\ntemp['station_id']=station_id.index\ntemp.index=station_id['BoroCT2010']\nct_shape_MonthlyAvg=ct_shape.join(temp)\nct_shape_MonthlyAvg.fillna(0,inplace=True)\nf, ax = plt.subplots(figsize=(10,10))\nct_shape_MonthlyAvg.plot(column='Monthly_Avg',colormap='YlGn',alpha=1,linewidth=0.1,ax=ax)\nplt.title(\"Monthly Ride Counts\")",
        "text": "add the monthly , summer , winter average ride to the ct shapefile",
        "id": 21
    },
    {
        "code": "[1,2,3] == [1,2,4]\n[1,2,3] < [1,2,4]\n[2,2,3] < [1,2,4]",
        "text": "we can do boolean test on list a well ,",
        "id": 22
    },
    {
        "code": "plot_corr(pd.concat([df_stage3[['song_hotttnesss']],df_stage3[df_stage3.columns[-17:]]], axis=1), \"datastory/figures/feature/correlationGenre.png\")",
        "text": "and let 's do the same thing for genre only  . ",
        "id": 23
    },
    {
        "code": "simple_dict = {\"first_name\": \"Steve\", \n               \"last_name\": \"Flurry\",\n               \"age\": 17,\n               \"age\": 22}\nsimple_dict\npd.Series(simple_dict)",
        "text": "create series from python dictionary",
        "id": 24
    },
    {
        "code": "G1 = matrix([[1.0, 0.0], [-1.0, -1.0]]).trans()\nh1 = matrix([3.0, -2.0])\nsolution = solvers.qp(P, q, G1, h1)\nprint(solution['x'])",
        "text": "if we delete second too , (   -  x _ 1 \\leq 0 $ )",
        "id": 25
    },
    {
        "code": "df_gb3 = pd.DataFrame(crime_data_2018_DF.groupby(['District', 'Hour']).size())\ndf_gb3.reset_index(inplace=True)\ndf_gb3.rename(columns={0:\"Crimes\"}, inplace=True)\ndf_h3 = df_gb3.pivot(\"District\", \"Hour\", \"Crimes\")\nfig, ax = plt.subplots()\nfig.set_size_inches(16, 4)\nax = sns.heatmap(df_h3, ax=ax, cmap= sns.cm.rocket_r)\nplt.savefig (\"../plot/12.heatmap_time_2018.png\")\nplt.title('Heat Map : Crime by District and Hour of day - 2018', fontsize=20, weight='bold')\nplt.show()",
        "text": "heatmap   -   crime by district   -   hour of the day , 2018",
        "id": 26
    },
    {
        "code": "def reset_graph():\n    if 'sess' in globals() and sess:\n        sess.close()\n    tf.reset_default_graph()\nreset_graph()\nhyperparam_dict = dict(\n    number_of_classes = len(unique_training_labels_resampled),\n    batch_size = 14,\n    back_prop_steps = 100,\n    lstm_layers = 2,\n    lstm_cell_units = 5,\n    drop_out = 0.9,\n    clipping_ratio = 2,\n    learning_rate = 8e-3,\n    training_iterations = 1000\n)\nX = tf.placeholder(tf.float32,\n                   [None, hyperparam_dict['back_prop_steps']],\n                   name = 'Features')\ny = tf.placeholder(tf.int64,\n                   [None], \n                   name='Labels')\nkeep_probability = tf.placeholder('float', name = 'drop_out')",
        "text": "define the hyperparameters organize in dictionary to allow easy log of value while search for best model parameter  . ",
        "id": 27
    },
    {
        "code": "sf['Country']\nsf[\"age\"].mean()",
        "text": "inspect some column of dataset",
        "id": 28
    },
    {
        "code": "sf['Country']\nsf['Country'].show()\ndef transform_country(country):\n    if country == \"USA\":\n        return \"United States\"\n    return country\ntransform_country(\"Brazil\")\ntransform_country(\"USA\")\nsf['Country'].apply(transform_country)\nsf['Country'] = sf['Country'].apply(transform_country)",
        "text": "use apply function to do advance transformation of our data",
        "id": 29
    },
    {
        "code": "cars.sum()\ncars.sum(axis=1)\ncars.median()\ncars.mean()\ncars.mpg.idxmax()\nmpg = cars.mpg\nmpg.idxmax()",
        "text": "look at summary statistic that decribe a variable 's numeric value",
        "id": 30
    },
    {
        "code": "from keras.layers import Bidirectional\nblstm = Sequential()\nblstm.add(Embedding(nb_words, e, input_length=MAX_SEQUENCE_LENGTH))\nblstm.add(Bidirectional(LSTM(128, dropout_U=0.2, dropout_W=0.2)))\nblstm.add(Dropout(0.2))\nblstm.add(Dense(1, activation='sigmoid'))\nblstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nblstm.summary()",
        "text": "bidirectional and stack lstm < img width=  90 %  src=  <url>  / >",
        "id": 31
    },
    {
        "code": "ufo.columns\nufo.rename(columns = {'Colors Reported':'Colors_reported','Shape Reported':'Shape_Reported'}, inplace=True)\nufo.columns\nufo_cols = ['city', 'colors reported', 'shape reported', 'state', 'time', 'location']\nufo.columns = ufo_cols\nufo.head()\ncol_names = ['city', 'colors reported', 'shape reported', 'state', 'time']\nufo = pd.read_csv('http://bit.ly/uforeports', names = col_names, header = 0)\nufo.head()\nufo.columns = ufo.columns.str.replace(' ','_')\nufo.columns",
        "text": "how to rename a column in a dataframe ?",
        "id": 32
    },
    {
        "code": "plt.title('Directors with the Highest Total Revenue')\ndf.groupby('director')['revenue'].sum().sort_values(ascending=False).head(10).plot(kind='bar', colormap='autumn')\nplt.show()",
        "text": "director with the high total revenue",
        "id": 33
    },
    {
        "code": "train_data, test_data = final_data.randomSplit([0.7, 0.3])\ntrain_fit = pipeline.fit(train_data)",
        "text": "split the data into train and test set in ratio 70,30",
        "id": 34
    },
    {
        "code": "\nresult = em.select_matcher([dt, rf, svm, ln, lg], table=H, \n        exclude_attrs=['_id', 'ltable_id', 'rtable_id', 'label'],\n        k=5,\n        target_attr='label', metric_to_select_matcher='f1', random_state=0)\nresult['cv_stats']\nresult['drill_down_cv_stats']['precision']\nresult['drill_down_cv_stats']['recall']\nresult['drill_down_cv_stats']['f1']",
        "text": "select the best matcher use cross   -   validation now , we select the best matcher use k   -   fold cross   -   validation  .  for the purpose of this guide , we use five fold cross validation and use precision  metric to select the best matcher  . ",
        "id": 35
    },
    {
        "code": "def get2Grams(payload_obj):\n    payload = str(payload_obj)\n    ngrams = []\n    for i in range(0,len(payload)-2):\n        ngrams.append(payload[i:i+2])\n    return ngrams\ntfidf_vectorizer_2grams = TfidfVectorizer(tokenizer=get2Grams)\ncount_vectorizer_2grams = CountVectorizer(min_df=1, tokenizer=get2Grams)",
        "text": "gram feature create a countvectorizer and tf   -   idfvectorizer that us 2   -   gram  . ",
        "id": 36
    },
    {
        "code": "\nparam_grid = {'C': np.logspace(-1, 1, 3), 'gamma': [.5, 1, 2, 3]}\ngrid_search = GridSearchCV(SVC(kernel='rbf', probability=True),\n                           param_grid =param_grid, cv=5,\n                           scoring='roc_auc',\n                           n_jobs=-1, error_score=0)\ngrid_search.fit(X_train_sc, y_train)\nsvm = grid_search\nprint(\"Best parameters: {}\".format(svm.best_params_))\nprint(\"Best cross-validation AUC: {:.4f}\".format(svm.best_score_))\nrocauc = draw_roc_curve(y_test, X_test_sc, svm, flag=1)",
        "text": "nonlinear model , svm with kernel",
        "id": 37
    },
    {
        "code": "def read_csv(filename):\n    data = open(filename).read().split('\\n')\n    string_list = data[1:]\n    final_list = []\n    for each in string_list:\n        int_fields = []\n        string_fields = each.split(',')\n        for val in string_fields:\n            integer = int(val)\n            int_fields.append(integer)\n        final_list.append(int_fields)\n        \n    return final_list\ncdc_list = read_csv(\"US_births_1994-2003_CDC_NCHS.csv\")\nprint(cdc_list[0:10])",
        "text": "convert to a list of list",
        "id": 38
    },
    {
        "code": "new_df[\"year\"] = new_df['Animal Birth'].apply(lambda birth: birth.year)\nnew_df[\"year\"].head()",
        "text": "create a new column call  year  that be the dog 's year of birth  -  the animal birth  column be a datetime , so you can get the year out of it with the code df [ animal birth  ]  . apply ( lambda birth , birth . year )   . ",
        "id": 39
    },
    {
        "code": "a=np.array([[1,2],[3,4],[5,6]]) \na\nv=[1,2,3]   \nv\nv=np.array([1,2,3]) \nv\nv=np.arange(1,2,0.1)\nv\nv.tolist()\nv=range(1,6)\nv\nv=np.linspace(1,2,11)\nv",
        "text": "numpy basic vector and matrix",
        "id": 40
    },
    {
        "code": "start = time.time()\nuser_sample = 0.05\npr = Evaluation.precision_recall_calculator(test_data, train_data, pm, is_model)\n(pm_avg_precision_list, pm_avg_recall_list, ism_avg_precision_list, ism_avg_recall_list) = pr.calculate_measures(user_sample)\nend = time.time()\nprint(end - start)",
        "text": "use the above precision recall calculator class to calculate the evaluation measure",
        "id": 41
    },
    {
        "code": "ts_data = pickle_helper.load('data/train_set_forecasting.pickle')\nplt.figure(figsize=(20,4))\nplt.plot(ts_data)\nplt.show()",
        "text": "for the forecast we be go to use page view data , very similar to the data use in the anomaly detection section  .  it be also page view data and contain 1 sample per hour  . ",
        "id": 42
    },
    {
        "code": "\nlength = 26.2 * u.meter\nprint(length) \ntype(length)\ntype(u.meter)\nlength\nlength.value\nlength.unit\nlength.info",
        "text": "basic how do we define a quantity and which part doe it have ?",
        "id": 43
    },
    {
        "code": "mnist = input_data.read_data_sets('data/', one_hot=True)\ntrainimg = mnist.train.images\ntrainlabel = mnist.train.labels\ntestimg = mnist.test.images\ntestlabel = mnist.test.labels\nprint (\"MNIST loaded!!!\")",
        "text": "download and extract mnist dataset",
        "id": 44
    },
    {
        "code": "bias_flt = 'OPEN'\nflt_BJ10 = 'B_JOHN_10'\nflt_VJ11 = 'V_JOHN_11'\nflt_RJ12 = 'R_JOHN_12'\nprint(\"Done\")",
        "text": "set the metadata value for the filter name ( metadata key flt _ id ) ,",
        "id": 45
    },
    {
        "code": "def get_numpy_data(dataframe, features, output):\n    dataframe['one'] = 1\n    features = ['one'] + features\n    feature_dataframe = dataframe[features]\n    feature_matrix = feature_dataframe.as_matrix()\n    output_dataframe = dataframe[output]\n    output_matrix = output_dataframe.as_matrix()\n    return (feature_matrix, output_matrix)\nfeature_matrix_train, sentiment_train = get_numpy_data(train_data, important_words, 'sentiment')\nfeature_matrix_valid, sentiment_valid = get_numpy_data(validation_data, important_words, 'sentiment')",
        "text": "convert data frame to multi   -   dimensional array",
        "id": 46
    },
    {
        "code": "def lexical_sentiment(doc, sid=None):\n    if sid is None: sid = SentimentIntensityAnalyzer()\n    label = int(sid.polarity_scores(doc)['compound'] > 0)\n    return label\nfor doc in testing_docs:\n    doc = \" \".join(doc[0])\n    label = lexical_sentiment(doc, sid)\n    print(doc[:100] + \"...\", label)",
        "text": "compare two approach first we can transform the sentiment score by the lexical approach into label by the follow rule , + positive sentiment , compound score > 0 + negative sentiment , compound score   -  0",
        "id": 47
    },
    {
        "code": "total_births = pd.pivot_table(all_names, \n                              values= 'births', \n                              index= 'name', \n                              aggfunc= sum)\ntotal_births.head()",
        "text": "explore total birth by name",
        "id": 48
    },
    {
        "code": "additional_info_one = final_nominations['Additional Info'].str.rstrip(\"'}\")\nadditional_info_two = additional_info_one.str.split(\" {'\")\nmovie_names = additional_info_two.str[0]\ncharacters = additional_info_two.str[1]\nfinal_nominations['Movie'] = movie_names\nfinal_nominations['Character'] = characters\nfinal_nominations.head()\nfinal_nominations.drop('Additional Info', axis=1, inplace=True)\nfinal_nominations.head()",
        "text": "clean up the additiona info column",
        "id": 49
    },
    {
        "code": "x_train, x_validate, x_test = np.split(X, [int(.6*len(X)), int(.8*len(X))])\ny_train, y_validate, y_test = np.split(Y, [int(.6*len(Y)), int(.8*len(Y))])\ny2_train, y2_validate, y2_test = np.split(Y2, [int(.6*len(Y2)), int(.8*len(Y2))])\nprint('x_train.shape=', x_train.shape, ' y_train.shape=', y_train.shapeape, ' y2_train.shape=', y2_train.shape)",
        "text": "split the data to train , validation and test * train = 60 % * validation = 20 % * test = 20 %",
        "id": 50
    },
    {
        "code": "pd.crosstab(index=table_inspections['02_x'],\n                          columns=[table_inspections['22_y']])",
        "text": "the most frequent violation group report in re   -   inspection be 10 , for restaurant with group 2 violation in initial inspection  . ",
        "id": 51
    },
    {
        "code": "d = {'cat' : 'cute', 'dog': 'furry'}\nprint(d['cat'])\nprint(d['dog'])\nd['fish'] = 'wet'\nprint(d['fish'])\nprint(d.get('monkey', 'N/A'))\nprint(d.get('fish', 'N/A'))\ndel d['fish']\nprint(d.get('fish', 'N/A'))",
        "text": "dictionary a dictionary store ( key , value ) pair , similar to a map in java or an object in javascript <url>",
        "id": 52
    },
    {
        "code": "doc_name = '5ede8912-59c9-4ba9-93df-c58cebb542b7'\ndoc = session.query(Document).filter(Document.name==doc_name).one()\nbrat.view(\"spouse/train\", doc)",
        "text": "launch brat interface in a new window once our collection be initialize , we can view specific document for annotation  .  the default mode be to generate a html link to a new brat browser window  .  click this link to connect to launch the annotator editor  . ",
        "id": 53
    },
    {
        "code": "my_number = 33\nif my_number % 2 == 0:\n    print(\"Even\")\nelse:\n    print(\"Odd\")",
        "text": "write a cell that create a variable and store an integer there  .  then have this cell look at the integer variable and print  even  if the number be even or  odd  if the number be odd  .  ( if you want to make your program more robust , you can have it print an error message if it receive a non   -   integer )",
        "id": 54
    },
    {
        "code": "def sentence_to_seq(sentence, vocab_to_int):\n    sentence = sentence.lower()\n    ids = [vocab_to_int.get(word,vocab_to_int['<UNK>']) for word in sentence.split(\" \")]\n    return ids\ntests.test_sentence_to_seq(sentence_to_seq)",
        "text": "sentence to sequence to fee a sentence into the model for translation , first need to preprocess it  .  this implement the function sentence _ to _ seq ( )  to preprocess new sentence   -  convert the sentence to lowercase   -   convert word into id use vocab _ to _ int   -   convert word not in the vocabulary , to the     word id  . ",
        "id": 55
    },
    {
        "code": "pd.crosstab(index=df['TripType'], columns=df['Weekday']).hist(figsize=(20,10))",
        "text": "most common triptype for each weekday",
        "id": 56
    },
    {
        "code": "fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(12,8))\nax.set_title('distribution of means as a function of sample size')\nax.set_xlabel('sample size')\nax.set_ylabel('ditribution of means');\nfor i in sample_sizes:\n    if i %50 ==0 and i < 1000:\n        plt.scatter([i]*200, sample_means[i], color='b', alpha=0.05);\nplt.xlim([0,1000])\nplt.ylim([0.25,0.75]);",
        "text": "the distribution be much tight at large sample size , and that you can have way low and way large mean at small sample size  .  indeed there be mean a small a 0 . 1 at a sample size of 10 , and a small a 0 . 3 at a sample size of 100  . ",
        "id": 57
    },
    {
        "code": "\ndef webData_to_DF(system_datafram, dict_of_webData):\n    for ea in dict_of_webData.keys():\n        usr_lines = list(filter(lambda a: a != 0, dict_of_webData[ea][1:]))\n        addProfile(system_datafram, ea, dict_of_webData[ea][0], usr_lines)",
        "text": "function to take data dict from web or text file and add it to exist system dataframe",
        "id": 58
    },
    {
        "code": "class Shape:\n    def area(self):\n        return 0\n        \nclass Square(Shape):\n    def __init__(self, length):\n        self.length = length\n    def area(self):\n        return self.length ** 2\nsquare = Square(2)\nprint(square.area())\nshape = Shape()\nprint(shape.area())",
        "text": "question define a class name shape and it subclass square  .  the square class ha an init function which take a length a argument  .  both class have a area function which can print the area of the shape where shape 's area be 0 by default  . ",
        "id": 59
    },
    {
        "code": "Image(\"./img/023_3_cluster.png\", width = 300, height = 300)\nImage(\"./img/023_3.png\", width = 1200, height = 1200)",
        "text": "we ve run the above line of code and save the result , so here we just load the save plot and image  . ",
        "id": 60
    },
    {
        "code": "\ndef resize_img(fpath, base):\n    img = Image.open(fpath)\n    rimg = img.resize((224,224))\n    img.close()\n    return rimg\nDATA_PATH='/home/bfortuner/workplace/data/imagenet_sample/'\nfnames = list(glob.iglob(DATA_PATH+'*/*.JPEG'))\nN = 5000",
        "text": "resize image   -  url>",
        "id": 61
    },
    {
        "code": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.05, random_state=42)\nprint (\"Training and testing split was successful.\")",
        "text": "split the data into test and traning set",
        "id": 62
    },
    {
        "code": "visualize_feature(no_show, 'Diabetes')\nvisualize_feature(no_show, 'Alcoholism')",
        "text": "about 10 % of the patient receive the scholarship and patient who do n't receive the scholarship tend to miss the appointment more  . ",
        "id": 63
    },
    {
        "code": "n = 10\nplt.figure(figsize=(20, 2))\nfor i in range(n):\n    ax = plt.subplot(1, n, i)\n    plt.imshow(x_test_noisy[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()",
        "text": "here 's what the noisy digit look like ,",
        "id": 64
    },
    {
        "code": "df = pd.read_table('pd-data/ufo.csv', sep=',')\ndf.head(2)\ndf.columns\ndf.rename(columns = {'Colors Reported': 'Colors_Reported'}, inplace=True)\ndf.columns\nnew_names = ['City', 'Colors_Reported', 'Shape_Reported', 'State', 'Time']\ndf.columns = new_names\ndf.columns\ndf = pd.read_table('pd-data/ufo.csv', sep=',', names=new_names, header=0)\ndf.columns\ndf.head()\ndf = pd.read_table('pd-data/ufo.csv', sep=',')\ndf.columns = df.columns.str.replace(' ', '_')\ndf.head(2)",
        "text": "how to rename column in a panda dataframe",
        "id": 65
    },
    {
        "code": "fig, subaxes = plt.subplots(2, 4, figsize=(15, 8), dpi=200)\nfor this_penalty, this_axis in zip(['l1','l2'], subaxes): \n    for this_C, subplot in zip([0.1, 1, 10, 200], this_axis):\n        title = 'penalty = {:s}, C = {:.2f}'.format(this_penalty, this_C)\n        clf = LogisticRegression(C=this_C,penalty=this_penalty,random_state=0).fit(X_train, y_train)\n        plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n                                                 X_test, y_test, title,\n                                                 subplot)\n        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)",
        "text": "logistic regression regularization , use c parameter and penalty  . ",
        "id": 66
    },
    {
        "code": "g = 9.8 \nm = 0.045 \nk = 10.4 \ntimestep = 0.01 \nk",
        "text": "define variable we define some variable here  .  it 's not very complicate to define a variable that 's just a number  .  if you want to avoid make me crazy , please use comment , mark by a hashtag , to indicate the unit and any other useful information  .  please define some constant that you will need , to calculate the oscillation of milli  . ",
        "id": 67
    },
    {
        "code": "sentence = \"This is a sentence; please slice it.\"\nsentence = 'This is a sentence; please slice it.'\nprint(sentence[0:4])\nprint(sentence[5:7])\nprint(sentence[8:9])\nprint(sentence[10:18])\nprint(sentence[20:26])\nprint(sentence[27:32])\nprint(sentence[33:35])",
        "text": "use slice to extract each word from",
        "id": 68
    },
    {
        "code": "def get_embed(input_data, vocab_size, embed_dim):\n    embed = tf.contrib.layers.embed_sequence(input_data, \n                                             vocab_size, \n                                             embed_dim)\n    return embed",
        "text": "word embed apply embed to input data use tensorflow  .  return the embed sequence  . ",
        "id": 69
    },
    {
        "code": "x = np.linspace(-1.4, 1.4, 50)\nplt.plot(x, x**2, \"r--\", label = \"Square function\")\nplt.plot(x, x**3, \"g-\", label = \"Cubic function\")\nplt.legend(loc = \"best\")\nplt.grid(True)",
        "text": "legend , the simple way to add a legend be to set a label on all line , then just call the legend  function  . ",
        "id": 70
    },
    {
        "code": "import random\nimport numpy as np\ndef createClusteredData(N, k):\n    random.seed(10)\n    pointsPerCluster = float(N)/k\n    X = []\n    for i in range(k):\n        incomeCentroid = random.uniform(20000.0, 200000.0)\n        ageCentroid = random.uniform(20.0, 70.0)\n        for j in range(int(pointsPerCluster)):\n            X.append([np.random.normal(incomeCentroid, 10000.0), np.random.normal(ageCentroid, 2.0)])\n    X = np.array(X)\n    return X",
        "text": "mean cluster example let 's make some fake data that include people cluster by income and age , randomly ,",
        "id": 71
    },
    {
        "code": "\nnumber_of_features = 1000\n(train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features)\ntokenizer = Tokenizer(num_words=number_of_features)\ntrain_features = tokenizer.sequences_to_matrix(train_data, mode='binary')\ntest_features = tokenizer.sequences_to_matrix(test_data, mode='binary')",
        "text": "load movie review text data",
        "id": 72
    },
    {
        "code": "\naxes=['ax'+str(i) for i in range(ndim)]\nfig, (axes) = plt.subplots(ndim, figsize=(10,60))\nplt.tight_layout()\nfor i in range(0,ndim):\n    if i<int(ndim/2):\n        axes[i].set(ylabel='d%i' % (i+1))\n    else:\n        axes[i].set(ylabel='c%i' % (i-5))\nfor i in range(0,ndim):\n    sns.tsplot(traces_cold[i],ax=axes[i])",
        "text": "let 's see what our chain look like by produce trace plot ,",
        "id": 73
    },
    {
        "code": "lm1 = LinearRegression(fit_intercept=False)\nlm1\nlm1.fit(X, bos.PRICE)\nlm1.intercept_\n#### https://stats.stackexchange.com/questions/102709/when-forcing-intercept-of-0-in-linear-regression-is-acceptable-advisable",
        "text": "exercise , how would you change the model to not fit an intercept term ? would you recommend not have an intercept ? why or why not ?",
        "id": 74
    },
    {
        "code": "pickle.dump(gbm, open(\"gbm.pickle.dat\", \"wb\"))\ntrain_preds = pd.DataFrame(gbm_train_preds)\ntest_preds = pd.DataFrame(gbm_test_preds)\ntrain_preds.columns = ['RESPONSE']\ntest_preds.column = ['RESPONSE']\ntrain.to_csv('LGBM Train.csv', sep=',')\ntrain_preds.to_csv('LGBM Train Preds.csv', sep=',')\ntest.to_csv('LGBM Test.csv', sep=',')\ntest_preds.to_csv('LGBM Test Preds.csv', sep=',')\nimportance.to_csv('LGBM Feature Importance.csv', index = False)\nslack_message(\"Files saved!\", 'channel')",
        "text": "save model file and write  . csv file to work directory save lightgbm model file for future reference  .  similar function to load previously save file be comment out below  .  then , write all file to the work directory",
        "id": 75
    },
    {
        "code": "df_pm25.head(1)\nstates.reset_index(inplace=True)\npm_st = states.merge(df_pm25, how='left', left_on='NAME', right_on ='State Name')\npm_st.head()\npm_st.groupby('NAME')['Arithmetic Mean'].mean().head()\nstates.set_index('NAME', inplace=True)\nstates['aver_air_quality'] = pm_st.groupby('NAME')['Arithmetic Mean'].mean()\nstates.head()\nax = states.plot(column='aver_air_quality', cmap='Greys', figsize=(20,20))\nax.axis('off')",
        "text": "make a map of average air quality of each state , with high quality be light red and low quality be dark red",
        "id": 76
    },
    {
        "code": "df['5 day'] = fiveday(df['Close'])\ndf['10 day'] = tenday(df['Close'])\ndf['20 day'] = twentyday(df['Close'])\ndf = df.dropna()\ndf.tail()",
        "text": "run daily close through fiveday , tenday , and twentday function  .  save series to new column in dataframe  . ",
        "id": 77
    },
    {
        "code": "ev = Evaluation(besthvpop_3, toolbox_3, logbook_3, actual, distance_treshold=10)\nev.plot()\nprint(ev)\nplt.savefig('.exports/avg_winter_3kWh.jpg', dpi=500)",
        "text": "average winter day , 3kwh battery",
        "id": 78
    },
    {
        "code": "ts = Series(np.random.randn(4),\n            index=pd.date_range('1/1/2000', periods=4, freq='M'))\nts\nts.shift(2)\nts.shift(-2)\nts / ts.shift(1) - 1\nts.shift(2, freq='M')\nts.shift(3, freq='D')\nts.shift(1, freq='3D')\nts.shift(1, freq='90T')",
        "text": "shift ( lead and lag ) data",
        "id": 79
    },
    {
        "code": "app_pivot['Percent with Application'] = app_pivot.Application / app_pivot.Total\napp_pivot",
        "text": "calculate another column call percent with application  , which be equal to application  divide by total   . ",
        "id": 80
    },
    {
        "code": "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 8))\nax1.imshow(data, norm=norm)\nax1.set_title('Data')\nax2.imshow(segm2, cmap=segm2.cmap())\nax2.set_title('Segmentation Image')\nfor aperture in apertures:\n    aperture.plot(color='white', lw=1.5, alpha=0.5, ax=ax1)\n    aperture.plot(color='white', lw=1.5, alpha=1.0, ax=ax2)",
        "text": "now plot the elliptical aperture on the data and the segmentation image  . ",
        "id": 81
    },
    {
        "code": "from scipy import io\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import np_utils\nfrom keras.wrappers.scikit_learn import KerasClassifier \nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score\npath = 'E:/[3] 수업/ME특론1/dataset/ME1.mat'\ndata = io.loadmat(path)\nnp.shape(data['data'])\nlabel = pd.read_csv('E:/[3] 수업/ME특론1/dataset/label.txt',header=None, engine='python')\nlabel.head()",
        "text": "convolutional neural network   -   raw data , no feature extraction   -   parietal , occipital region",
        "id": 82
    },
    {
        "code": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('.'))\n    training_accuracy = evaluate(X_train, y_train)\n    test_accuracy = evaluate(X_test, y_test)\n    validation_accuracy = evaluate(X_valid, y_valid)\n    print(\"Training Accuracy = {:.3f}\".format(training_accuracy))\n    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n    print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))",
        "text": "evaluate performance of the model",
        "id": 83
    },
    {
        "code": "par(mfrow=c(1,2))\nbarplot(margin.table(bed_vs_bath,1))\nbarplot(margin.table(bed_vs_bath,2))\nlibrary(pastecs)\noptions(scipen=999)\nstat.desc(housing_prices)",
        "text": "let 's plot barplots use the table command  .  essentially , a bar plot work like a table command  . ",
        "id": 84
    },
    {
        "code": "from bigdl.nn.layer import *\nfrom bigdl.nn.criterion import *\nimport numpy as np",
        "text": "we first need to import the necessary module  . ",
        "id": 85
    },
    {
        "code": "df.Militarystatus.value_counts()\ndf[df.Militarystatus.isnull()]\ndf['Militarystatus'].value_counts().plot(kind='bar')",
        "text": "for militarystatus",
        "id": 86
    },
    {
        "code": "class InferenceConfig(coco.CocoConfig):\n    \n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\nconfig = InferenceConfig()\nconfig.display()",
        "text": "configuration we ll be use a model train on the m   -   coco dataset  .  the configuration of this model be in the  cococonfig   class in  coco . py    .  for inferencing , modify the configuration a bite to fit the task  .  to do so , sub   -   class the  cococonfig   class and override the attribute you need to change  . ",
        "id": 87
    },
    {
        "code": "\ndaily_store_sales.expanding().mean().head()\ndata['Sales'].ewm(span=10).mean().head()",
        "text": "panda expand function in addition to rolling  function , panda provide expanding  , which , instead of use a window of n value , use all value up until that time  . ",
        "id": 88
    },
    {
        "code": "gs = ms.GridSearchCV(\n    estimator=ssvc,\n    param_grid={\n        'svr__C': [1e-15, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n        'svr__kernel': ['linear', 'rbf']\n    },\n    scoring='neg_mean_squared_error',\n    cv=split\n)\ngs.fit(X, y)\n-gs.best_score_\ngs.best_estimator_",
        "text": "determine optimal  kernel and value of  c  by cross   -   validation  . ",
        "id": 89
    },
    {
        "code": "a = np.random.random(10)\nb = np.random.random(10)\nclose = np.allclose(a, b)\nequal = np.array_equal(a, b)\nprint(a)\nprint(b)\nprint(close)\nprint(equal)",
        "text": "consider two random array a and b check if they be equal",
        "id": 90
    },
    {
        "code": "import time\ninput_size = 28 * 28 * 1\nnum_classes = 10\nhidden_size = 20\nnet = TwoLayerNet(input_size, hidden_size, num_classes)\nstart_time = time.clock()\nstats = net.train(X_train, y_train, X_val, y_val,\n            num_iters=1000, batch_size=200,\n            learning_rate=0.05, learning_rate_decay=0.95,\n            reg=0.1, verbose=True)\nend_time = time.clock()\nprint(\"Training took \", end_time - start_time, \" seconds\")\nval_acc = (net.predict(X_val) == y_val).mean()\nprint('Validation accuracy: ', val_acc)",
        "text": "train a network to train our network we will use sgd with momentum  .  in addition , we will adjust the learn rate with an exponential learn rate schedule a optimization proceed , after each epoch , we will reduce the learn rate by multiply it by a decay rate  .  ( this be already implement , just run the cell  .  )",
        "id": 91
    },
    {
        "code": "from sklearn.pipeline import make_pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import Binarizer\nmake_pipeline(Binarizer(), MultinomialNB())\npipe.steps[0]\npipe.named_steps['reduce_dim']\npipe.set_params(clf__C=10)\nfrom sklearn.model_selection import GridSearchCV\nparams = dict(reduce_dim__n_components=[2, 5, 10],\n              clf__C=[0.1, 10, 100])\ngrid_search = GridSearchCV(pipe, param_grid=params)\nfrom sklearn.linear_model import LogisticRegression\nparams = dict(reduce_dim=[None, PCA(5), PCA(10)],\n              clf=[SVC(), LogisticRegression()],\n              clf__C=[0.1, 10, 100])\ngrid_search = GridSearchCV(pipe, param_grid=params)",
        "text": "the utility function make _ pipeline be a shorthand for construct pipeline , it take a variable number of estimator and return a pipeline , fill in the name automatically ,",
        "id": 92
    },
    {
        "code": "p = np.arange(1000)\nplt.subplot(2,1,1)\nplt.plot( Ttest[:,:1],Ytest[:,:1], 'o', p )\nplt.subplot(2,1,2)\nplt.plot( Ttrain[:,:1],Ytrain[:,:1], 'ro', p )",
        "text": "the above graph describe the actual output for test data for appliance in blue v the predict output in orange",
        "id": 93
    },
    {
        "code": "def permutations(str):\n    if len(str) <= 1:\n        return [str]\n    else:\n        perms = []\n        for ch in permutations(str[:-1]):\n            for i in xrange(len(ch)+1):\n                perms.append(ch[:i] + str[-1] + ch[i:])\n        return set(perms)\npermutations(\"aac\")",
        "text": "permutation with dups , write a method to compute all permutation of a string whose character be not necessarily unique  .  the list of permutation should not have duplicate  . ",
        "id": 94
    },
    {
        "code": "titles = data['title'].fillna('')\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features = 1000, \n                             ngram_range=(1, 2), \n                             stop_words='english',\n                             binary=True)\nvectorizer.fit(titles)\nX = vectorizer.transform(titles)\nX",
        "text": "demo , use of the count vectorizer",
        "id": 95
    },
    {
        "code": "ts_log_moving_avg_diff = ts_log - moving_avg\nts_log_moving_avg_diff.head(12)",
        "text": "the red line show the roll mean  .  let subtract this from the original series  .  note that since we be take average of last 12 value , roll mean be not define for first 11 value  .  this can be observe a ,",
        "id": 96
    },
    {
        "code": "x = np.array([1,1,0,1,0,0,1,0])\ny = np.array([1,0,0,1,0,0,1,1])\nprint(JaccardCoefficient(x,y))\nprint(CosineSimilarity(x,y))",
        "text": "< img src=  images/similarity _ c . png  / >",
        "id": 97
    },
    {
        "code": "products['sentiment'] = products['rating'].apply(lambda rating : 1 if rating > 3 else -1)\nproducts",
        "text": "assign review with a rat of 4 or high to be *positive* review , while the one with rat of 2 or low be *negative  -  for the sentiment column , we use +1 for the positive class label and   -   1 for the negative class label  . ",
        "id": 98
    },
    {
        "code": "\n10 + 11\n\"Hello my name is \" + \"Rodrigo.\"\nprint(21.5 + 15.8)\nprint(\"Hello \" + \"my name is \" + \"Rodrigo.\")",
        "text": "module 1 section 4 , addition   -   error    -  addition , number and string *  _  _ math addition ,  _  _  number addition with + *  _  _ string concatenation ,  _  _  combine string with +",
        "id": 99
    },
    {
        "code": "def fit_model_and_score_tree(train_features, train_response, val_features, val_response):\n    \n    model = fit_tree(train_features, train_response)\n    return score_model(model, train_features, train_response, val_features, val_response)",
        "text": "you should be able to use your same score function a above to compute your model score  .  write a function that fit a tree model to your train set and return the model 's score for both the train set and the validation set  . ",
        "id": 100
    },
    {
        "code": "X_train[0].strip()\nvectorizer = pickle.load(open(RESOURCE_PATH['TFIDF_VECTORIZER'], 'rb'))\nX_train_tfidf, X_test_tfidf = vectorizer.transform(X_train), vectorizer.transform(X_test)",
        "text": "let u reuse the tf   -   idf vectorizer that we have already create above  .  it should not make a huge difference which data wa use to train it  . ",
        "id": 101
    },
    {
        "code": "len(in_spectrum), len(cumsum_filter2)\nthinkplot.preplot(2)\nout_wave.plot(label='cumsum')\nout_wave2 = (in_spectrum * cumsum_filter2).make_wave()\nout_wave2.plot(label='filter')\nthinkplot.config(legend=True, loc='lower right')",
        "text": "now we can compute the output wave use the convolution theorem , and compare the result ,",
        "id": 102
    },
    {
        "code": "\nmyForest = RandomForestClassifier(n_estimators=10)\nscores = cross_validation.cross_val_score(myForest, X, y, cv=5)\nprint(\"Scores:\", scores)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))",
        "text": "train and evaluate random forest",
        "id": 103
    },
    {
        "code": "symbol = \"BAM\"\nprint(sdb.merge(trades,\n            tkfilter(\"symbol='%s'\" % symbol)\n          ).nonempty())\nprint(sdb.merge(quotes,\n            tkr.filter(\"symbol='%s'\" % symbol)\n          ).nonempty())",
        "text": "look up trade by symbol string join with the auxiliary tkr array to look up data by ticker symbol name  .  here be example that count the number of trade and quote for bam   . ",
        "id": 104
    },
    {
        "code": "\npums_p.groupby(['PUMA'])['PWGTP'].agg(['size'])\nsf_pop = pums_p.PWGTP.sum()\nsf_pop",
        "text": "down to several thousand record now  .  how many record in each ?",
        "id": 105
    },
    {
        "code": "import plotly.offline as pyo\nimport plotly.graph_objs as go\nimport numpy as np\nnp.random.seed(42)\nrandom_x = np.random.randint(1,101,100)\nrandom_y = np.random.randint(1,101,100)\ndata = [go.Scatter(\n    x = random_x,\n    y = random_y,\n    mode = 'markers',\n)]\npyo.plot(data, filename='scatter1.html')",
        "text": "plotly scatter plot example 1",
        "id": 106
    },
    {
        "code": "\nresponse = sagemaker.describe_endpoint(EndpointName=endpoint_name)\nstatus = response['EndpointStatus']\nprint('EndpointStatus = {}'.format(status))\nsagemaker.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\nendpoint_response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\nstatus = endpoint_response['EndpointStatus']\nprint('Endpoint creation ended with EndpointStatus = {}'.format(status))\nif status != 'InService':\n    raise Exception('Endpoint creation failed.')",
        "text": "finally , now the endpoint can be create  .  it may take sometime to create the endpoint   - ",
        "id": 107
    },
    {
        "code": "gauss.mean\nprint(gauss.mean.name)\nprint(gauss.mean.value)",
        "text": "model parameter be object with attribute ,",
        "id": 108
    },
    {
        "code": "def do_test():\n    reader = create_reader(data['test']['file'], is_training=False)\n    evaluate(reader, z, 'intent')\ndo_test()\nz.classify.b.value",
        "text": "now we can measure the model accuracy by go through all the example in the test set and use the c . eval . evaluator method  . ",
        "id": 109
    },
    {
        "code": "clf = neighbors.KNeighborsClassifier(3, weights='distance')\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\naccuracy = np.mean(predictions == y_test)*100\nprint('The final accuracy is %.2f' % accuracy + '%')",
        "text": "final evaluation    -  k=3 give u the high accuracy , so we select it a our best model  .  now we can evaluate it on our train set and get our final accuracy rat  . ",
        "id": 110
    },
    {
        "code": "w = widgets.Dropdown(\n    options={'One': 1, 'Two': 2, 'Three': 3},\n    value=2,\n    description='Number:')\ndisplay(w)\nw.value",
        "text": "the follow be also valid ,",
        "id": 111
    },
    {
        "code": "whReleases['tokenized_text'] = whReleases['text'].apply(lambda x: nltk.word_tokenize(x))",
        "text": "now we have all the text in a dataframe we can look at a few thing  .  first let 's tokenize the text with the same tokenizer a we use before  .  we will just save the token a a list for now , no need to convert to text 's  . ",
        "id": 112
    },
    {
        "code": "rnd_batches = get_in_batches(val_path, batch_size=batch_size, shuffle=True)\nval_res = [model.evaluate_generator(rnd_batches, rnd_batches.samples) for i in range(3)]\nnp.round(val_res, 3)",
        "text": "validate the model performance on the val set run the evaluate generator return the cost and accuracy of the model  .  do it in a loop allow u to confirm that the performance be stable  .  result should be very similar for all run  .  this take  _ very _  long though",
        "id": 113
    },
    {
        "code": "plot_acf(pm25, lags=12)\nplot_pacf(pm25, lags=12)",
        "text": "plot the acf and pacf  . ",
        "id": 114
    },
    {
        "code": "from utils.layers import softmax_loss\nloss, _ = softmax_loss(scores, y_train)\nprint(\"loss = %.2f\" % loss)",
        "text": "softmax loss function , vectorized implementation",
        "id": 115
    },
    {
        "code": "predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n                                                  validation_targets[\"median_house_value_is_high\"], \n                                                  num_epochs=1, \n                                                  shuffle=False)\nvalidation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)\nvalidation_predictions = np.array([item['predictions'][0] for item in validation_predictions])\n_ = plt.hist(validation_predictions)",
        "text": "solution click below to display the solution  . ",
        "id": 116
    },
    {
        "code": "richpeople.plot(kind='scatter', x = 'age', y='networthusbillion', figsize=(10,10), alpha=0.3)",
        "text": "maybe plot their net worth v age ( scatterplot )",
        "id": 117
    },
    {
        "code": "jeopardy['Air Date'] = pd.to_datetime(jeopardy['Air Date'])\njeopardy.dtypes",
        "text": "normalize date column the air date column should also be a datetime , not a string , to enable you to work with it more easily  . ",
        "id": 118
    },
    {
        "code": "basic_plot2(c_totals=new_totals[:,0], sd=sd)",
        "text": "show the new mean flank by desire standard deviation ,",
        "id": 119
    },
    {
        "code": "print(np.__version__) \nprint(np.show_config()) \nprint(dir(np)) \n               \n               \n               \n               \n               \n               \nprint ()                \nprint(np.__file__)",
        "text": "print the numpy version and the configuration",
        "id": 120
    },
    {
        "code": "sample_safety_factor(source_diameter,0,dsource_position,\n                     actual_collimator_size,collimator_offset,dcollimator_offset,\n                     source_to_collimator, collimator_to_fp_end, \n                     field_plate_spacing/2.0 - field_plate_guard_ring_offset)",
        "text": "assume uncorrelated random variable , the typical close trajectory to the guard ring with measure collimator offset and collimator size and estimate uncertainty in the collimator offset , and estimate uncertainty in beam source alignment will be ,",
        "id": 121
    },
    {
        "code": "simulation.reporters = []\nprint ('Heating the system from 0 to 300K (increasing target_temperature in 6 steps of 50K)')\nsimulation.reporters.append(PDBReporter('TetraAspDimer_noIons_Heating.pdb', 100))\nsimulation.reporters.append(StateDataReporter(stdout, 500, step=True, potentialEnergy=True, temperature=True))\nfor target_temperature in range(0, 10, 2):\n    integrator.setTemperature(target_temperature)\n    simulation.step(1000)\n    print ('Done heating to', target_temperature, 'K.')",
        "text": "heat in 6 step of 50 k increment from 0 to 300 k",
        "id": 122
    },
    {
        "code": "\ndef kmeans(data, k=2, centroids=None):\n    \n    if not centroids:\n        centroids = randomize_centroids(data, k)\n    old_centroids = centroids[:]\n    iterations = 0\n    while True:\n        iterations += 1\n        \n        clusters = [[] for i in range(k)]\n        \n        for datapoint in data:\n            \n            centroid_idx = find_closest_centroid(datapoint, centroids)\n            \n            \n            clusters[centroid_idx].append(datapoint)\n        \n        \n        old_centroids = centroids\n        \n        \n        centroids = update_centroids(old_centroids, clusters)\n        \n        \n        if check_converge(centroids, old_centroids, iterations, threshold=0):\n            break\n    return centroids",
        "text": "< div class=anchor  >      -  question 1 . 6 complete the k   -   mean algorithm skeleton below , with the function you write above  . ",
        "id": 123
    },
    {
        "code": "X_aligned = normalize(shapeData(X_aligned, h=aligned_h, w=aligned_w))\ny_aligned = np_utils.to_categorical(y_aligned)\nfull_model_aligned = model5(h=aligned_h, w=aligned_w)\nfull_model_aligned.fit(X_aligned, y_aligned, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1)",
        "text": "train a full model on the align image derive from the entire give dataset this include generate image",
        "id": 124
    },
    {
        "code": "new_data = json.load(open('data/world_bank_projects.json'))\nnew_data = json_normalize(new_data, 'mjtheme_namecode')\nnew_data.groupby(['code','name']).size().sort_values(ascending = False).head(10)\nnew_data.name.value_counts().head(10)",
        "text": "find the top 10 major project theme ( use column mjtheme _ namecode  )",
        "id": 125
    },
    {
        "code": "print(\"Improved plot df Homeruns by year from 2011 as a violin chart.\")\n_ = sns.violinplot(y='Homeruns', x='yearID', data=df[(df.yearID > 2010) & (df.Homeruns != 0)], color = 'green')",
        "text": "improve the previous violin chart , * filter out record with homeruns of 0 , * use only one color of all violin  . ",
        "id": 126
    },
    {
        "code": "my_trump_tweet = create_tweet_from_dict(my_trump_dict)\ntype(my_trump_tweet)\nassert type(my_trump_tweet) == Tweet, \"Your object is not of type Tweet but {}.\".format(\ntype(my_trump_tweet))\nassert my_trump_tweet.text == my_trump_dict.get('text'), \"The content of the two elements does not match.\"",
        "text": "now , let 's test our new create _ tweet _ from _ dict ( )  function ,",
        "id": 127
    },
    {
        "code": "print(data.groupby([\"alchemy_category\"])[[\"label\"]].count())\np = sb.factorplot(x = \"alchemy_category\",\n              y = \"label\",\n              kind = \"bar\",\n              data = data,\n              size = 8,\n              estimator = len)\np.set_xticklabels(rotation = 45, horizontalalignment = \"right\")\nplt.show()",
        "text": "exercise , 5 .  how many article be there per category ?",
        "id": 128
    },
    {
        "code": "articles3[0] \nscraped_articles = []\nfor article in articles3:\n    current = {}\n    current[\"name\"] = article[\"name\"]\n    current[\"code\"] = article[\"articleCode\"]\n    current[\"oldprice\"] = article[\"priceInfo\"][\"formattedOldPrice\"]\n    current[\"price\"] = article[\"priceInfo\"][\"formattedPrice\"]\n    current[\"url\"] = article[\"webUrl\"]\n    scraped_articles.append(current)\ndf = pd.DataFrame(scraped_articles)\ndf.to_csv(\"scraped_articles.csv\", index=False)",
        "text": "download all on   -   sale woman 's top and save them to a csv  .  i want this csv to include   -   name   -   product code/number   -   old price   -   on   -   sale price   -   item 's url",
        "id": 129
    },
    {
        "code": "\ny_2016 = df_2016['Happiness Score']\nX_2016 = df_2016.drop(['Happiness Score','Happiness Rank','Country','Region','Lower Confidence Interval','Upper Confidence Interval'],axis=1)\npredict_2016=lm.predict(X_2016)",
        "text": "eliminate feature in 2016 dataset to fit generic model   -  apply prediction on dataset",
        "id": 130
    },
    {
        "code": "\nfrom urllib.request import urlretrieve\nimport pandas as pd\nurl = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\nurlretrieve(url, 'winequality-red.csv')\ndf = pd.read_csv('winequality-red.csv', sep=';')\nprint(df.head())\nimport pandas as pd\ndf = pd.read_csv(url, sep = ';')",
        "text": "import flat file from the web",
        "id": 131
    },
    {
        "code": "\ndef fib(n):\n    if n == 0: \n        return 0\n    elif n == 1: \n        return 1\n    else: \n        return fib(n-1)+fib(n-2)\nfib(7)\n### END CODE",
        "text": "exercise 9 write a recursive python program to compute the fibonnaci number for a give index  .  this be what a fibonacci series be <url> make sure you invoke this function in your main program and display the result",
        "id": 132
    },
    {
        "code": "t = (5, 10, 15)\ntype(t)\nt[0]\nfor item in t:\n    print(item * item)\nt.append(30)\ncarefree_list = [5, 10, 15, 20, 25]\ncarefree_list.append(30)\ncarefree_list\ncarefree_list[1] = \"Boris\"\ncarefree_list\nt[1] = \"Boris\"\nhello = [1, 2, 3]\nfoo = (1, 2, 3)\nimport sys\nsys.getsizeof(hello)\nsys.getsizeof(foo)",
        "text": "tuple ( not  toople  ,  tuple  rhyme with  supple  ) tuple be kind of like a strict list",
        "id": 133
    },
    {
        "code": "((odds_hat_Setosa >= 1) == (cs.Setosa == 1)).sum()\n((odds_hat_Setosa >= 1) == (cs.Setosa == 1)).mean()",
        "text": "now , use just the odds , let 's confirm the model accuracy",
        "id": 134
    },
    {
        "code": "myvec <- c(1,10,100,1000)\nnames(myvec) <- c(\"col1\",\"col2\",\"col3\",\"col4\") \nprint(myvec)\nprint(myvec[2])\nprint(myvec[c(2,3)])\nprint(myvec[c(-2,-3)])\nprint(myvec[c(\"col3\",\"col4\")])\nprint(myvec[myvec > 100])",
        "text": "to subset a vector , use  [ ]  a in python  .  however , the index in r start from one , not zero   -  you can also use vector , name , and relational operator for subsetting a vector  . ",
        "id": 135
    },
    {
        "code": "assert count_words_in_tweet(my_trump_dict) == 50, \"The solution should be 50 but your solution is {}.\".format(\n    count_words_in_tweet(my_trump_dict))\nmy_trump_dict.get('text')",
        "text": "again let 's test our function count _ words _ in _ tweet  ,",
        "id": 136
    },
    {
        "code": "train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()",
        "text": "let u drop parch , sibsp , and familysize feature in favor of isalone  .  patch,sibsp,family sizeの各機能をisaloneに譲りましょう . ",
        "id": 137
    },
    {
        "code": "model_ridge = linear_model.Ridge(alpha=6, fit_intercept=True, max_iter=10000)\nmodel_ridge.fit(x_train, y_train)\nmodel_lasso = linear_model.LassoCV(alphas = [1, 0.16, 0.1, 0.001, 0.0005]).fit(x_train, y_train)\nrmse_cv(model_lasso).mean()\ncv_ridge.min()",
        "text": "a value of alpha = 6 be about right base on the plot above  .  let  try out the lasso model  .  we will do a slightly different approach here and use the build in lasso cv to figure out the best alpha for u  .  for some reason the alpha in lasso cv be really the inverse or the alpha in ridge  . ",
        "id": 138
    },
    {
        "code": "\nmax_trees = 250\ncnt_jump = 25\npred_matrix_train = np.zeros((X_train.shape[0], max_trees))\npred_matrix_test = np.zeros((X_test.shape[0], max_trees))\npred_matrix_train, I, void = \\\n    rf.random_forest_predict(X_train, y_train, X_train, n_trees=max_trees,\n                                                max_depth=5)\npred_matrix_test = rf.random_forest_predict(X_train, y_train, X_test, n_trees=max_trees,\n                                                  max_depth=5)[0]\ntrain_scores = []\ntest_scores =  []\noob_scores =   []\nplot_error_vs_trees(pred_matrix_train, I, pred_matrix_test, max_trees, cnt_jump)",
        "text": "max depth of 5 tree",
        "id": 139
    },
    {
        "code": "pacific_storms.iloc[0]['record_identifier']\npacific_storms['record_identifier'].value_counts()",
        "text": "there be some empty string present ,",
        "id": 140
    },
    {
        "code": "\nfrom Bio import Entrez\nimport networkx as nx\nimport os\nDATADIR = os.path.join(os.getcwd(), \"..\", \"ClassPrep\")\nprint(os.path.exists(DATADIR))\nfrom IPython.display import Image, display\nimport nxdrawing as nxd\nimport gzip\nimport pickle",
        "text": "run the cell below if you need to install biopython",
        "id": 141
    },
    {
        "code": "cv = CountVectorizer(stop_words='english', max_df=.95, min_df=5)\nX = cv.fit_transform(df_revs.Content)\nX",
        "text": "create a document   -   term matrix",
        "id": 142
    },
    {
        "code": "rdd6.reduce(lambda x, y: x / y)",
        "text": "we be to reduce the data in a manner that we would like to *divide* the current result by the subsequent one , we would expect a value of 10",
        "id": 143
    },
    {
        "code": "filter(lambda x: x % 2 == 0,range(20))",
        "text": "filter ( ) be more commonly use with lambda function because we usually use filter for a quick job where we do n't want to write an entire function  .  let repeat the example above use a lambda expression ,",
        "id": 144
    },
    {
        "code": "clf=sklm.LinearRegression()\nclf.fit(xtrain, ytrain)\nscore=clf.score(xtest, ytest)\nprint(score)",
        "text": "now we try fit various ml model",
        "id": 145
    },
    {
        "code": "PlanAttributes_Data=PlanAttributes_Raw[['BusinessYear','StateCode','IssuerId','ImportDate','PlanId','MetalLevel',\n                                        'TEHBInnTier1FamilyMOOP','TEHBInnTier1IndividualMOOP']]\nPlanAttributes_Data.head(3)",
        "text": "pick the feature we will use ,",
        "id": 146
    },
    {
        "code": "from sklearn.model_selection import cross_val_score\nfolds = 5\nmax_neighbors = np.floor(X.shape[0] - X.shape[0]/5.)\nprint(max_neighbors)\ntest_acc = []\nfor i in range(1, int(max_neighbors)):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    test_acc.append(np.mean(cross_val_score(knn, X, y, cv=5)))\nfig, ax = plt.subplots(figsize=(8,6))\nax.plot(list(range(1, int(max_neighbors))), test_acc, lw=3.)\nplt.show()",
        "text": "fit knn across different value of k and plot the mean cross   -   validate accuracy with 5 fold  . ",
        "id": 147
    },
    {
        "code": "numerical_train, numerical_test, spent_train, spent_test = train_test_split(numerical_features['Length of Membership'], yearly_amount_spent['Yearly Amount Spent'], test_size=0.3, random_state=123)\nspent_train.head()",
        "text": "split the data into a train and test set  .  make a test set size 0 . 3 and random seed 123 ( so the result be consistent for all student )",
        "id": 148
    },
    {
        "code": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=1000.0, random_state=0)\nlr.fit(X_train_std, y_train)\nplot_decision_regions(X_combined_std, y_combined,\n                     classifier=lr, test_idx=range(105,150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()\nlr.predict_proba(X_test_std[0, :].reshape(1, -1))",
        "text": "train a logistic regression model with scikit   -   learn",
        "id": 149
    },
    {
        "code": "bias_dict = {\n    'b1': tf.Variable(tf.zeros([n_dense_1])), \n    'b2': tf.Variable(tf.zeros([n_dense_2])),\n    'b_out': tf.Variable(tf.zeros([n_classes]))\n}\nweight_dict = {\n    'W1': tf.get_variable('W1', [n_input, n_dense_1], initializer=weight_initializer),\n    'W2': tf.get_variable('W2', [n_dense_1, n_dense_2], initializer=weight_initializer),\n    'W_out': tf.get_variable('W_out', [n_dense_2, n_classes], initializer=weight_initializer)\n}",
        "text": "define dictionary for store weight and bias for each layer    -  and initialize",
        "id": 150
    },
    {
        "code": "for name in ('LOUISE', 'ALICE', 'LUCIE', 'ANNA', 'LÉA', 'EMMA', 'CAMILLE'):\n    plotname(name, 1)\nplt.xlim([1900,2015])\nplt.ylim([0,11000])\nplt.legend(loc='upper center')\nplt.xlabel('Year')\nplt.ylabel('Number of people')\nplt.title('Old female names becoming popular again (in Top 15 in 2015)\\n')\nplt.show()",
        "text": "old  female name become popular again ( top 15 in 2015 )    -  in term of absolute number of birth ,",
        "id": 151
    },
    {
        "code": "fig, axes = plt.subplots(nrows=4, ncols=5)\nk,i=0,0\nfor e in bins:\n    sex_xt = pd.crosstab(df_train['target'],df_train[e])\n    sex_xt_norm = sex_xt.apply(lambda x : x/np.sum(sex_xt,axis=1))\n    sex_xt_norm.plot(kind='bar', stacked=True, \n                     ax=axes[k,i],figsize=(15,15),\n                     sharex=False, sharey=True,\n                     title=e, legend=False )\n    i+=1\n    if i==5:\n        i=0\n        k+=1\nplt.tight_layout()",
        "text": "percentage dist of binary value by target value",
        "id": 152
    },
    {
        "code": "a = 2\nb = 11\nc = a + b",
        "text": "above , the variable  c  be use to store  the value 10   .  the function print  be use to display the value of a variable  .  ( we will learn what function be and how we use them late )  .  to compute $ c = a + b $ , where $ a = 2 $ and $ b = 11 $ ,",
        "id": 153
    },
    {
        "code": "\nrandom.seed(98103)\nn=30\nx=[random.random() for i in range(n)]\nx.sort()\nxseries=pd.Series(x)\nyseries=xseries.apply(lambda x: math.sin(4*x))\nrandom.seed(1)\nnoise=pd.Series([random.gauss(0,1.0/3.0) for i in range(n)])\nyseries=yseries+noise",
        "text": "the data we will generate a random set of x value  .  the y value will be sin ( 4x ) , with some noise add to it  . ",
        "id": 154
    },
    {
        "code": "def delays_text(line_name,xmlroot):\n    line_status_soup = MakeLine_ServiceStatusSoup(line_name,xmlroot)\n    delay_string = ''\n    delayline_name = line_name\n    dealyline_num = ''\n    for ea in FullLineList:\n        if ea[1] == delayline_name:\n            dealyline_num = ea[0]\n    del_text = ''\n    for del1 in line_status_soup.find_all('span', {'class': 'TitleDelay'}):\n        \n        \n        if len(del1.find_all('p')) > 0:\n            \n            delay_deets = del1.find_all('p')\n            \n            for dels in delay_deets:\n                del_text += dels.text + ' '\n            \n            \n        else:\n            del_text = ''.join(line_status_soup.find_all(text=True)[3:5]).strip()\n            \n            \n    return del_text",
        "text": "function to intake service status xml and a line name and return text of delay on that line , if applicable",
        "id": 155
    },
    {
        "code": "students = [ \n { 'Name':'bob','GPA':3.4, 'Ischool' : True },\n { 'Name':'sue','GPA':2.8, 'Ischool' : True },\n { 'Name':'kent','GPA':4.0, 'Ischool' : False }\n]\nprint(students)\ntype(students)\nstudents[-1]\ntype(students[-1])\nprint(students[-1])\nfor student in students:    \n    if student['Ischool']:  \n        print(student['Name'], student['GPA'])",
        "text": "watch me code 3 , list of dictionary",
        "id": 156
    },
    {
        "code": "from sklearn.linear_model import ElasticNet\nres = bench_estimators_relative(\n    GeneralizedLinearRegressor(solver=\"cd\", l1_ratio=0.5, start_params='guess'),\n    estimator_ref=ElasticNet(max_iter=100, l1_ratio=0.5),\n    metric=\"train_test_err\"                \n)\ndisplay(res)",
        "text": "mae for the reference implementation and the glm implementation on the train | test datasets",
        "id": 157
    },
    {
        "code": "\nstart = 25000\nstop = 35000\ntr_buf.parse(\"spi_trace.csv\",start,stop)\ntr_buf.decode(\"spi_trace.pd\",\n              options=':wordsize=8:cpol=0:cpha=0')",
        "text": "parse and decode transaction the trace buff object be able to parse the transaction into a   -  csv  file ( save into the same folder a this script )  .  then the trace buff object can also decode the transaction use the open   -   source sigrok  decoder  .  the decode file (   -  pd  ) be save into the same folder a this script  .  reference , <url>",
        "id": 158
    },
    {
        "code": "for i in xrange(len(train_datasets)):\n    pickle_file = train_datasets[i] \n    with open(pickle_file, 'rb') as f:\n        letter_set = pickle.load(f) \n        print('%s has %d data points.'%(train_datasets[i],len(letter_set)))",
        "text": "another check , we expect the data to be balance across class  .  verify that  . ",
        "id": 159
    },
    {
        "code": "X_complete.shape\nidx_complete\nidx_complete.shape\nX_incomplete.shape\nidx_incomplete\nidx_incomplete.shape\nfrom sklearn import model_selection\nX_train_complete, X_dev_complete, y_train_complete, y_dev_complete, train_complete_idx_local, dev_complete_idx_local = model_selection.train_test_split(X_complete, y_complete, range(len(X_complete)), test_size=200, shuffle=True, random_state=0)\n(X_train_complete.shape, X_dev_complete.shape, y_train_complete.shape, y_dev_complete.shape)\ntrain_complete_idx_local\ntrain_complete_idx_local[100] == 1410",
        "text": "split into train and development set",
        "id": 160
    },
    {
        "code": "my_trump_dict == trump_dict_list[0] and my_trump_dict == trump_dict_list[1]",
        "text": "similarly ,  a and b  test if  a  and  b  be true   .  only if all item be true  the expression will evaluate to true  a well  . ",
        "id": 161
    },
    {
        "code": "data.head()\ndata.tail(3)\ndata.shape\ndata.head(3)",
        "text": "exercise try out these command to see what they return ,   -   data . head ( )    -   data . tail ( 3 )    -   data . shape",
        "id": 162
    },
    {
        "code": "\nmale_df = (sample_data.loc[sample_data['Sex'] == 'male'])\ndisplay(male_df.describe())\nfig, axs = plt.subplots(1, 2, figsize=(16,8))\nmale_survived = male_df.groupby('Survived')\ncolor=['b','g']\nfor ind, grp in male_survived:\n    grp['Age'].plot(kind='hist', color=color[ind], ax = axs[ind])\naxs[0].set_xlabel('Deceased')\naxs[0].set_ylabel('Number of  Male Passengers')\naxs[1].set_xlabel('Survived')\naxs[1].set_ylabel('Number of Male Passengers')",
        "text": "explore the data contd    -  ( male only data )",
        "id": 163
    },
    {
        "code": "plt.plot(days, morale_true, linewidth=5, color='gold', alpha=0.5, label='true function')\nplt.xlabel('days', fontsize=16)\nplt.ylabel('morale', fontsize=16)\nplt.title('Morale over time', fontsize=20)\nplt.legend(loc='upper left')",
        "text": "we can plot out the  true function  of day predict morale below ,",
        "id": 164
    },
    {
        "code": "w = best_svm.W[:,:-1] \nw = w.reshape(10, 32, 32, 3)\nw_min, w_max = np.min(w), np.max(w)\nclasses = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nfor i in xrange(10):\n  plt.subplot(2, 5, i + 1)\n  \n  wimg = 255.0 * (w[i].squeeze() - w_min) / (w_max - w_min)\n  plt.imshow(wimg.astype('uint8'))\n  plt.axis('off')\n  plt.title(classes[i])",
        "text": "finally , we visualize the learn weight for each class  .  depend on your choice of learn rate and regularization strength , these may or may not be nice to look at ,",
        "id": 165
    },
    {
        "code": "train = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\ntrain, test = names(train, test)\ntrain, test = age_impute(train, test)\ntrain, test = cabin_num(train, test)\ntrain, test = cabin(train, test)\ntrain, test = embarked_impute(train, test)\ntrain, test = fam_size(train, test)\ntest['Fare'].fillna(train['Fare'].mean(), inplace = True)\n#train, test = drop(train, test)",
        "text": "have build our helper function , we can now execute them in order to build our dataset that will be use in the model , a",
        "id": 166
    },
    {
        "code": "sns.lmplot(x='Outstate', y='F.Undergrad', data=df, hue='Private',fit_reg=False)\ng=sns.FacetGrid(df, hue='Private',size=6,aspect=2)\ng = g.map(plt.hist,'Outstate',bins=20,alpha=0.7)",
        "text": "creating a scatterplot of f . undergrad versus outstate where the point be color by the private column   - ",
        "id": 167
    },
    {
        "code": "fl = Dataset('../../swift.dkrz.de/COREII_data/fesom.1951.oce.mean.nc')\nfl.variables['temp'].shape",
        "text": "load data for one year ,",
        "id": 168
    },
    {
        "code": "rnd_batches = get_in_batches(val_path, batch_size=batch_size, shuffle=True)\nval_res = [model.evaluate_generator(rnd_batches, 350) for i in range(5)]\nnp.round(val_res, 3)",
        "text": "validate the model performance on the val set run the evaluate generator return the cost and accuracy of the model  .  do it in a loop allow u to confirm that the performance be stable  .  result should be very similar for all run  .  this take  _ very _  long though ( a long a train or more  -  will have to look into this )",
        "id": 169
    },
    {
        "code": "tf.reset_default_graph()\nb = tf.placeholder(tf.float32, [None, 10, 32, 5])\nprint(get_shape(b))\nb = reshape(b, [0, [1, 2],3])\nprint(get_shape(b))",
        "text": "then collapse the dimension become very easy ,",
        "id": 170
    },
    {
        "code": "\nnfl_missing_values_count = nfl_data.isnull().sum()\nnfl_missing_values_count[0:10]",
        "text": "see how many miss data point we have  _  _  _  ok , now we know that we do have some miss value  .  let 's see how many we have in each column  . ",
        "id": 171
    },
    {
        "code": "\nr = f.rolling(50).var()['Volume'].plot()",
        "text": "be the variance of the trade volume relatively stable over time ?",
        "id": 172
    },
    {
        "code": "df_high_tm_low_we = df_sim[(df_sim[\"we_sim\"] < 0.4)]\ndf_high_tm_low_we.iloc[np.random.permutation(len(df_high_tm_low_we))]",
        "text": "high tm similarity , low we similarity",
        "id": 173
    },
    {
        "code": "train_results = np.zeros((1,2))\nfor i in range(n):\n    qw,er = test(data1[i]), test(data2[i])\n    train_results = np.r_[train_results, np.array([qw,er]).reshape((1,2))]\ntrain_results = train_results[1:]\nget_boxplots(train_results)\nnp.mean(train_results[:,0]>0),np.mean(train_results[:,1]<0)",
        "text": "get the result on the train set  .  we observe that the classification be very good in this case with an error rate of 13 % , even though the mean function $ \\mu _ 1 $ and $ \\mu _ 2 $ be quite close",
        "id": 174
    },
    {
        "code": "df['cluster'].value_counts()\ngrouped = df['cluster'].groupby(df['name']) \ngrouped.mean() #average rank (1 to 100) per cluster",
        "text": "number of people per cluster",
        "id": 175
    },
    {
        "code": "plt.rcParams['figure.figsize'] = 16, 12\nplt.subplot(321)\npairPlot(gnb, 0, 1)\nplt.subplot(322)\npairPlot(gnb, 0, 2)\nplt.subplot(323)\npairPlot(gnb, 0, 3)\nplt.subplot(324)\npairPlot(gnb, 1, 2)\nplt.subplot(325)\npairPlot(gnb, 1, 3)\nplt.subplot(326)\npairPlot(gnb, 2, 3)\nplt.show()",
        "text": "below we visualize the decision boundary for each pair of feature  . ",
        "id": 176
    },
    {
        "code": "lengthList = []\nfor word in wordList.split() :\n    lengthList.append(len(word))\nlengthList.sort()\nm=sum(lengthList)/len(lengthList)\nprint('The mean of the list is {m}'.format(m=m))\nprint('The median of the list is {median}'.format(median=lengthList[len(lengthList)//2]))\nprint('The mode of the list is {mode}'.format(mode = max(set(lengthList), key=lengthList.count)))",
        "text": "find out what be the average , median and mode for word length ( character length )  . ",
        "id": 177
    },
    {
        "code": "\naxes=['ax'+str(i) for i in range(ndim)]\nfig, (axes) = plt.subplots(ndim, figsize=(10,60))\nplt.tight_layout()\nfor i in range(0,ndim):\n    if i<int(ndim/2):\n        axes[i].set(ylabel='d%i' % (i+1))\n    else:\n        axes[i].set(ylabel='c%i' % (i+1))\nfor i in range(0,ndim):\n    sns.tsplot(traces_cold[i],ax=axes[i])",
        "text": "let 's see what our chain look like by produce trace plot ,",
        "id": 178
    },
    {
        "code": "atlantic_storms.iloc[0]['record_identifier']\natlantic_storms['record_identifier'].value_counts()",
        "text": "there be some empty string present ,",
        "id": 179
    },
    {
        "code": "import random\nrandom.seed(123)\nparameters = [random.randint(1000,2000) for i in range(6)]\nparameters",
        "text": "question parameter generate parameter , that appear in the question  .  since we would like to test python code , we use random generator with seed to generate test case  . ",
        "id": 180
    },
    {
        "code": "df_sim = comp.load_sim(config=config,\n                       log_energy_min=log_energy_min,\n                       log_energy_max=log_energy_max,\n                       test_size=0, \n                       verbose=True)\nfeature_list, feature_labels = comp.analysis.get_training_features()\nfeature_list\ndf_data = comp.load_data(config=config,\n                         log_energy_min=log_energy_min,\n                         log_energy_max=log_energy_max,\n                         n_jobs=15,\n                         verbose=True)",
        "text": "data preprocessing [ [ back to top ] (   -   top ) ] 1 .  load simulation/data dataframe and apply specify quality cut 2 .  extract desire feature from dataframe 3 .  get separate test and train datasets",
        "id": 181
    },
    {
        "code": "\nthinkplot.Scatter(heights, weights, alpha=0.01, s=5)\nfxs, fys = thinkstats2.FitLine(heights, inter, slope)\nthinkplot.Plot(fxs, 10**fys, color='red')\nthinkplot.Config(xlabel='Height (cm)', ylabel='Weight (kg)', legend=False)",
        "text": "make the same plot but apply the inverse transform to show weight on a linear ( not log ) scale  . ",
        "id": 182
    },
    {
        "code": "C, average_clustering(ws)",
        "text": "the cluster coefficient be a little high than in the data  . ",
        "id": 183
    },
    {
        "code": "for dataset in full_data:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\ntrain_data.tail()",
        "text": "let u replace age with ordinal base on these band  . ",
        "id": 184
    },
    {
        "code": "fig, ax = plt.subplots(figsize=(12,8))\nax.plot(np.arange(iters), cost, 'r')\nax.set_xlabel('Iterations')\nax.set_ylabel('Cost')\nax.set_title('Error vs. Training Epoch')",
        "text": "look pretty good  .  since the gradient decent function also output a vector with the cost at each train iteration , we can plot that a well  .  notice that the cost always decrease   -   this be an example of a convex optimization problem  . ",
        "id": 185
    },
    {
        "code": "a = np.array([3,24,30,47,43,7,47,13,44,39])\nprint(\"Range : \"+ str(Range(a)))\nprint(\"Variance : \"+ str(variance(a)))\nprint(\"Mean : \"+ str(mean(a)))\nprint(\"STD : \"+ str(standard_devaition(a)))",
        "text": "< img src=  images/range _ variance . png  / >",
        "id": 186
    },
    {
        "code": "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, tree_predicted)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, tree_predicted)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, tree_predicted)))\nprint('F1: {:.2f}'.format(f1_score(y_test, tree_predicted)))\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, tree_predicted, target_names=['not 1', '1']))",
        "text": "evaluation metric for binary classification",
        "id": 187
    },
    {
        "code": "x = numList\ny = numList\np = plt.plot(x,y)",
        "text": "now let 's do a quick simple plot use the list we define early  . ",
        "id": 188
    },
    {
        "code": "d = 13 \nS_W = np.zeros((d, d))\nfor label, mv in zip(range(1, 4), mean_vecs):\n    class_scatter = np.zeros((d, d))  \n    for row in X_train_std[y_train == label]:\n        row, mv = row.reshape(d, 1), mv.reshape(d, 1)  \n        class_scatter += (row - mv).dot((row - mv).T)\n    S_W += class_scatter                          \nprint('Within-class scatter matrix: %sx%s' % (S_W.shape[0], S_W.shape[1]))",
        "text": "compute the within   -   class scatter matrix ,",
        "id": 189
    },
    {
        "code": "predictions = predict_point_by_point(model, X_test)    \nprint('Training duration (s) : ', time.time() - global_start_time)\nplot_results(predictions, y_test)",
        "text": "predict for 1 step and compare with true value",
        "id": 190
    },
    {
        "code": "from random import random\nfrom collections import Counter\ncounter = Counter()\ndef flip(p):\n    return random() < p\ndef play(i):\n    counter[i] += 1\n    p = actual_probs[i] / 100\n    if flip(p):\n        return 'W'\n    else:\n        return 'L'",
        "text": "the follow function simulate play one machine once  . ",
        "id": 191
    },
    {
        "code": "neutral = 0\npositive = 0\nnegative = 0\nfor i in range (0, len(new_list2)):\n    if new_list2[i][\"manual_sentiment\"] == \"neutral\":\n        neutral +=1\n    elif new_list2[i][\"manual_sentiment\"] == \"positive\":\n        positive +=1\n    else:\n        negative +=1\nprint (neutral) \nprint (positive)\nprint (negative)",
        "text": "check amount of sentiment data after clean",
        "id": 192
    },
    {
        "code": "data.Age[data.Pclass == 3].plot(kind='kde')    \ndata.Age[data.Pclass == 2].plot(kind='kde')\ndata.Age[data.Pclass == 1].plot(kind='kde')\nplt.xlabel(\"Age\")    \nplt.title(\"Age Distribution, within classes\")\nplt.legend(('3rd Class', '2nd Class','1st Class'))",
        "text": "analysis by age and class",
        "id": 193
    },
    {
        "code": "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\ndf.head()\ntrain, test = df[df['is_train']==True], df[df['is_train']==False]",
        "text": "create the train and test data",
        "id": 194
    },
    {
        "code": "site_group=xrfdf.groupby([\"Site\",\"Sample_ID\",\"Particle_Size\",\"Sample_Depth\"])",
        "text": "group the dataframe by site , date then depth",
        "id": 195
    },
    {
        "code": "def generate_ngrams(text, n):\n    words = text.split()\n    output = []  \n    for i in range(len(words)-n+1):\n        output.append(words[i:i+n])\n    return output\ngenerate_ngrams('this is a sample text', 2)",
        "text": "gram a feature a combination of n word together be call n   -   gram  .  n gram ( n > 1 ) be generally more informative a compare to word ( unigrams ) a feature  .  also , bigram ( n = 2 ) be consider a the most important feature of all the others  .  the follow code generate bigram of a text  . ",
        "id": 196
    },
    {
        "code": "thinkplot.cdf(cdf_age, label='age', complement=True)\ndecorate(title='Distribution of age', \n         xlabel='Age (years)', \n         ylabel='Complementary CDF, log scale',\n         xscale='log',\n         yscale='log')",
        "text": "here 's the complementary cdf on a log   -   log scale  .  interpretation ,",
        "id": 197
    },
    {
        "code": "x = set()\nx\nx.add(1)\nx",
        "text": "set be an unordered collection of unique element  .  we can construct them by use the set ( ) function  . ",
        "id": 198
    },
    {
        "code": "\nimport unittest\nclass AlmostEqualTest(unittest.TestCase):\n    def testEqual(self):\n        self.assertEqual(1.1, 3.3 - 2.2)\n    def testAlmostEqual(self):\n        self.assertAlmostEqual(1.1, 3.3 - 2.2, places=1)\n    def testNotAlmostEqual(self):\n        self.assertNotAlmostEqual(1.1, 3.3 - 2.0, places=1)",
        "text": "almost equal ? in addition to strict equality , it be possible to test for near equality of float point number use assertalmostequal ( ) and assertnotalmostequal ( )  . ",
        "id": 199
    },
    {
        "code": "df.dropna(axis=1)\ndf.dropna(axis=1, how='any')      # but 'any' is the default value...",
        "text": "drop any column that have miss data",
        "id": 200
    },
    {
        "code": "from IPython.display import HTML\nHTML(\"\"\"\n<video width=\"640\" height=\"360\" controls>\n  <source src=\"{0}\">\n</video>\n\"\"\".format('project_out.mp4'))",
        "text": "display video file in jupyter notebook",
        "id": 201
    },
    {
        "code": "import numpy as np\nimport problem_unittests as tests\nfrom collections import Counter\ndef create_lookup_tables(text):\n    counts = Counter(text)\n    vocab = sorted(counts, key = counts.get, reverse = True)\n    vocab_to_int = {word : i for i, word in enumerate(vocab)}\n    int_to_vocab = dict(enumerate(vocab))\n    return (vocab_to_int, int_to_vocab)\ntests.test_create_lookup_tables(create_lookup_tables)",
        "text": "implement preprocessing function    -  lookup table to transform the word to id , we need to create two dictionary ,   -   dictionary to go from the word to an id , we ll call vocab _ to _ int   -   dictionary to go from the id to word , we ll call int _ to _ vocab",
        "id": 202
    },
    {
        "code": "def pick_word(probabilities, int_to_vocab):\n    return int_to_vocab[np.argmax(probabilities)]\ntests.test_pick_word(pick_word)",
        "text": "choose word the pick _ word ( )  function be to select the next word use probabilities   . ",
        "id": 203
    },
    {
        "code": "def delta_one(user_list):\n    delta_list = []\n    delta = 0\n    counter = 0\n    answer = False\n    for nums in range(0,len(user_list)-1):\n        delta = user_list[nums + 1] - user_list[nums]\n        delta_list.append(delta)\n        \n    while counter < len(delta_list) - 1:\n        if delta_list[counter] == delta_list[counter + 1]:\n            answer = True\n        else:\n            answer = False\n            break\n        counter += 1\n    return answer\ndelta_one([-6, -3, 0, 3, 6, 9])",
        "text": "delta   -   one function       delta _ one ( list )     this function will return a boolean value of true or false  .  if true , that mean this list of number ha a delta   -   one constant  .  if false , this list doe not have a delta   -   one constant",
        "id": 204
    },
    {
        "code": "titles = data['title'].fillna('')\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(max_features = 1000, \n                             ngram_range=(1, 2), \n                             stop_words='english',\n                             binary=True)\nvectorizer.fit(titles)\nX = vectorizer.transform(titles)",
        "text": "demo , use of the count vectorizer",
        "id": 205
    },
    {
        "code": "\nX_train, X_val, y_train, y_val = train_test_split(train_transformed, y, test_size = 0.2, random_state = 0)\nprint(\"X_train : \" + str(X_train.shape))\nprint(\"X_val : \" + str(X_val.shape))\nprint(\"y_train : \" + str(y_train.shape))\nprint(\"y_val : \" + str(y_val.shape))",
        "text": "split between train and validation set",
        "id": 206
    },
    {
        "code": "\nimport textwrap\ndef should_indent(line):\n    print('Indent {!r}?'.format(line))\n    return len(line.strip()) % 2 == 0\ndedented_text = textwrap.dedent(sample_text)\nwrapped = textwrap.fill(dedented_text, width=50)\nfinal = textwrap.indent(wrapped, 'EVEN ',\n                        predicate=should_indent)\nprint('\\nQuoted block:\\n')\nprint(final)",
        "text": "to control which line receive the new prefix , pas a callable a the predicate argument to indent ( )  .  the callable will be invoke for each line of text in turn and the prefix will be add for line where the return value be true  .  this example add the prefix even to line that contain an even number of character  . ",
        "id": 207
    },
    {
        "code": "param_test1 = {\n 'max_depth':range(4,10,2), \n 'min_child_weight':range(1,6,2) \n}\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate = 0.1, n_estimators=140, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n param_grid = param_test1, scoring='neg_log_loss',n_jobs=4,iid=False, cv=None)\ngsearch1.fit(X_train[predictors], X_train['target'])\ngsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_",
        "text": "tune max _ depth and min _ child _ weight",
        "id": 208
    },
    {
        "code": "from sklearn.cross_validation import train_test_split\ntraining, test_data = train_test_split(training, test_size = 0.3)\ntest_data, test_data_future = train_test_split(test_data, test_size = 0.2)\ntraining.to_csv(training_set)\ntest_data.to_csv(test_set)\ntest_data_future.to_csv(future_set)",
        "text": "create train set and test set",
        "id": 209
    },
    {
        "code": "c = cast\nc = c.groupby(['year', 'type']).size()\nc = c.unstack('type')\nc.plot()",
        "text": "< div class=  alert alert   -   success  >   exercise   , plot the number of actor role each year and the number of actress role each year over the history of film  . ",
        "id": 210
    },
    {
        "code": "import os\nfilename = input('What file do you want to add words to? ')\nsay = input('What do you want to say? ')\ndata = []\nif os.path.exists(filename):\n    with open(filename, 'a') as f:\n        f.write(say+'\\n')    \nelse:\n    f = open(filename, 'w')\n    f.write(say+'\\n')\nf.close()",
        "text": "suppose we want to ask the user for a file  .  if that file already exist , we want to add the user 's word to the file  .  if the file doe n't exist , we want to create a new one  .  we would need to import the o package if use a mac ,",
        "id": 211
    },
    {
        "code": "plot_df = pd.DataFrame(np.random.randn(1000,2),columns=['x','y'])\nplot_df['y'] = plot_df['y'].map(lambda x : x + 1)\nplot_df.head()\nplot_df.plot()\nplot_df.hist()",
        "text": "plot panda be equip with straightforward wrapper for quick plot of data * use plot to visualize the column of the dataframe define below * use hist to visualize the distribution of the data in the form of a histogram",
        "id": 212
    },
    {
        "code": "\nimport sys\nsys.path.append(\"../lib/app_utilities\")\nfrom bi_util import *\nfrom IPython.display import display, HTML\nCONN = create_connection('webreport_06_15_2018')\ntypes = count_bi_types(CONN)\ntypes[types['num_types'] > 0][:10]\n# all of the db software where we have 1 or more installs (for any type/permutation)\n# installs[installs['num_installs'] > 0]\n# bi_software_histo ('Python%', conn=CONN)\n#bi_software_histo ('%Crystal Reports%', conn=CONN)",
        "text": "business intelligence ( bi ) tool analysis use bigfix webreports data 06/15/2018 snapshot    -  about notebook develop around bigfix dataset to understand bi software at the agency it be back by a postgresql database  .  see readme . md on build the database  .  upload data be restrict to window and macos machine ( server   -   desktop include )",
        "id": 213
    },
    {
        "code": "least_popular = songs.groupby(key_columns='artist', operations={'total_count': gl.aggregate.SUM('listen_count')}).sort('total_count', ascending=True)\nleast_popular",
        "text": "find least popular artist by number of listen",
        "id": 214
    },
    {
        "code": "import osmnx as ox\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nimport time\nfrom scipy.sparse import csr_matrix\nfrom scipy.spatial.distance import squareform, pdist\nfrom sklearn.cluster import DBSCAN\nox.config(use_cache=True, log_console=True)\neps = 300 \nminpts = 10 \npseudo_minpts = 1 \n                  \nn_firms = 1e6\nn_clusters = 10\nplace = 'Oakland, California, USA'",
        "text": "cluster 1,000,000 point along a spatial network cluster a set of fake firm base on their network distance from each other  . ",
        "id": 215
    },
    {
        "code": "dates = [today - timedelta(weeks=x) for x in range(4)]\ndates\ndates.sort()\nfor date in dates: \n    filename = \"b5km_baa-max-7d_{:%Y%m%d}.nc\".format(date)\n    url = \"{}/{:%Y}\".format(base_url, date)\n    print(\"downloading {}/{}\".format(url, filename))\n    cmd = 'curl --silent {}/{} -o {}/{}'.format(url, filename, opath, filename) \n    r = call(cmd, shell=True)\n    if r != 0:\n        print(\"something went wrong with the download of {}/{}\".format(url, filename))",
        "text": "build the list of date , start 4 week from now",
        "id": 216
    },
    {
        "code": "model = Sequential()\nmodel.add(Dense(n_hidden_1, activation='relu',  input_shape=(n_input,), name = \"Dense_1\"))\nmodel.add(Dense(n_hidden_2, activation='relu', name = \"Dense_2\"))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='SGD',\n              metrics=['accuracy'])\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=training_epochs,\n                    verbose=1, \n                    validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])",
        "text": "make the model simple in a simple way often when we write out layer we put the activation function in the layer itself rather than on it own",
        "id": 217
    },
    {
        "code": "a=np.arange(12)**2\ni=np.array([1,1,3,8,5])\na[i]\na\nj=np.array([[3,4],[9,7]]) \na[j]",
        "text": "fancy index and index trick    -  index with array of index",
        "id": 218
    },
    {
        "code": "most_popular = songs.groupby(key_columns='artist', operations={'total_count': gl.aggregate.SUM('listen_count')}).sort('total_count', ascending=False)\nmost_popular",
        "text": "find most popular artist by number of listen",
        "id": 219
    },
    {
        "code": "left = pd.DataFrame({'key': ['foo', 'foo'], 'lval': [1, 2]})\nright = pd.DataFrame({'key': ['foo', 'foo'], 'rval': [4, 5]})\nleft\nright\npd.merge(left, right, on='key')",
        "text": "join   sql style merge  .  see the database style join ( <url> )",
        "id": 220
    },
    {
        "code": "toxicity_annotations_unanimous_df = pd.read_table('toxicity_annotations_unanimous.tsv')\ntoxicity_annotations_comments_unanimous_df = pd.read_table('toxicity_annotated_comments_unanimous.tsv')\ntoxicity_annotations_df = pd.read_table('toxicity_annotations.tsv')\ntoxicity_annotations_comments_df = pd.read_table('toxicity_annotated_comments.tsv')",
        "text": "read in the data a panda dataframe",
        "id": 221
    },
    {
        "code": "\nx_pos = range(n_classes)\nlabel_list = y_train.tolist()\nsign_count = [label_list.count(x) for x in range(n_classes)]\nplt.bar(x_pos, sign_count, width=0.8, align='center')\nplt.ylabel('Sample Count')\nplt.xlabel('Sample Class')\nplt.savefig('./examples/visualization.jpg')\nplt.show()",
        "text": "plot the count of each sign in train set  . ",
        "id": 222
    },
    {
        "code": "graphlab.canvas.set_target(\"ipynb\")\nimage_train['label'].sketch_summary()",
        "text": "compute summary statistic of the data , sketch summary be technique for compute summary statistic of data very quickly  .  use the train data , compute the sketch summary of the  label  column and interpret the result  .  what  s the least common category in the train data ? save this result to answer the quiz at the end  . ",
        "id": 223
    },
    {
        "code": "\nreference_df = pd.read_pickle(save_load_path+'/mss_df.pkl')\nreference_df = reference_df.filter(items = ['track','title'])\nreference_df.head()\nreference_df[reference_df['track'].isin(foreign_songs)]",
        "text": "review foreign song detect by song title",
        "id": 224
    },
    {
        "code": "Z = np.arange(11)\nprint(Z)\nZ[(3 < Z) & (Z <= 8)] *= -1\nprint(Z)",
        "text": "give a 1d array , negate all element which be between 3 and 8 , in place  . ",
        "id": 225
    },
    {
        "code": "def tokenize(text_str):\n    \n    tokens = word_tokenize(text_str)\n    return tokens\ntok = tokenize(monty)\nassert_is_instance(tok,list)\nassert_true(all(isinstance(t, str) for t in tok))\nassert_equal(len(tok), 16450)\nassert_equal(tok[:10], ['SCENE', '1', ':', '[', 'wind', ']', '[', 'clop', 'clop', 'clop'])\nassert_equal(tok[51:55], ['King', 'of', 'the', 'Britons'])\nassert_equal(tok[507:511], ['African', 'swallows', 'are', 'non-migratory'])",
        "text": "tokenize in this function , you will tokenize the give input text  .  the function word _ tokenize ( ) might prove helpful in this instance  . ",
        "id": 226
    },
    {
        "code": "mnist      = input_data.read_data_sets('data/', one_hot=True)\ntrainimg   = mnist.train.images\ntrainlabel = mnist.train.labels\ntestimg    = mnist.test.images\ntestlabel  = mnist.test.labels\nprint (\"MNIST loaded\")",
        "text": "download and extract mnist dataset",
        "id": 227
    },
    {
        "code": "avg_funding_round_acquired = companies[companies.status == 'acquired'].average_funding_round.dropna()\navg_funding_round_closed = companies[companies.status == 'closed'].average_funding_round.dropna()\navg_funding_round_ipo = companies[companies.status == 'ipo'].average_funding_round.dropna()",
        "text": "then we have to separate average fund round for company that close from average fund round for company that get acquire and for company that get list  . ",
        "id": 228
    },
    {
        "code": "data['salary'] = data.salary.astype('str')\ndata = data[~data.salary.str.contains('hour')]",
        "text": "lastly , we need to clean up salary data  .  1 .  some of the salary be not yearly but hourly , these will be useful to u for now 2 .  the salary be give a text and usually with range   -  filter out the salary that not yearly ( filter those that refer to hour )",
        "id": 229
    },
    {
        "code": "threshold_values = np.linspace(0.5, 1, num=100)\nprint(threshold_values)",
        "text": "precision   -   recall curve now , we will explore various different value of tresholds , compute the precision and recall score , and then plot the precision   -   recall curve  . ",
        "id": 230
    },
    {
        "code": "start_year = 1998\nend_year = 2015\nsounding_times = get_sounding_times(start_year,1,1,0,\n                                    end_year,1,1,23)\npres_levels = [1013, 950, 925, 900, 850, 800, 750, 700, \n               650, 600, 550, 500, 400, 300, 200, 100] \nprint(len(sounding_times))",
        "text": "here be where we input the time and pressure level to get sound from  . ",
        "id": 231
    },
    {
        "code": "for name in ('LOUIS', 'PAUL', 'JULES', 'GABRIEL', 'ARTHUR'):\n    plotname(name, 0)\nplt.xlim([1900,2015])\nplt.ylim([0,12000])\nplt.legend(loc='upper center')\nplt.xlabel('Year')\nplt.ylabel('Number of people')\nplt.title('Old male names becoming popular again (in Top 15 in 2015)\\n')\nplt.show()",
        "text": "old  male name become popular again ( top 15 in 2015 )    -  in term of absolute number of birth ,",
        "id": 232
    },
    {
        "code": "bq.Query('SELECT * FROM data LIMIT 5', data_sources={'data': drivedata}).results()",
        "text": "query the table now let 's verify that we can access the data  .  we will run a simple query to show the first 5 row  .  note how we specify the federate table by use just a name in the query , and then pass the table in use a data _ sources dictionary parameter  . ",
        "id": 233
    },
    {
        "code": "traindfdummy = pd.get_dummies(traindf,columns = ['Species','zipcode'])\ntestdfdummy = pd.get_dummies(testdf,columns = ['Species','zipcode'])",
        "text": "convert categorical variable to dummy variable",
        "id": 234
    },
    {
        "code": "lrg_model.save_weights(path + 'models/lrg_nmp.h5')\nlrg_model.evaluate(conv_val_feat, val_labels)",
        "text": "jh , when i submit the result of this model to kaggle , i get the best single model result of any show here ( rank 22nd on the leaderboard a at dec   -   6   -   2016 )  . ",
        "id": 235
    },
    {
        "code": "bands=pd.read_csv(\"C:/Lectures/DataVisualization/Project2/metal-bands-by-nation/metal_bands_2017.csv\",\n                  low_memory=False,encoding='latin-1')",
        "text": "read the csv file and store into data frame  . ",
        "id": 236
    },
    {
        "code": "clf = SVC(kernel='linear', decision_function_shape='ovo')\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\naccuracy = np.mean(predictions == y_test) * 100\nprint('The final accuracy is %.2f' % accuracy + '%')",
        "text": "final evaluation    -  the linear kernel give u the high accuracy , so we select it a our best model  .  now we can evaluate it on our train set and get our final accuracy rat  . ",
        "id": 237
    },
    {
        "code": "X, Y = np.ogrid[:total_rows, :total_cols]\nhalf_upper = X < center_row \nhalf_upper_sieve = np.logical_and(half_upper, circular_sieve)\nphoto_data = misc.imread('./wifire/sd-3layers.jpg')\nphoto_data[half_upper_sieve] = random.randint(200,255)\nplt.figure(figsize=(15,15))\nplt.imshow(photo_data)",
        "text": "< p style=  font   -   family , arial , font   -   size,1 . 75em , color ,   -   2462c0 , font   -   style , bold  >   you can further improve the mask , for example just get upper half disc",
        "id": 238
    },
    {
        "code": "alphas =  [x/10.0 for x in range(1,6)]\nexperimentKanon(alphas, 'a',3)",
        "text": "impact of $ \\alpha $ on $ k   -  anonimity",
        "id": 239
    },
    {
        "code": "a = {'x': 1, 'z': 3 }\nb = {'y': 2, 'z': 4 }\nmerged = dict(b)\nmerged.update(a)\nprint (merged)\nprint (merged['x'])\nprint (merged['y'])\nprint (merged['z'])",
        "text": "a a alternative to chainmap , you might consider merge dictionary together use the update ( ) method  . ",
        "id": 240
    },
    {
        "code": "from sklearn import tree\nclf = tree.DecisionTreeClassifier() \nclf = clf.fit(train_data, classes)\nnp.sum(clf.predict(test_data) == test_classes) / len(test_data)",
        "text": "out of the box decision tree",
        "id": 241
    },
    {
        "code": "import os\npwd = os.getcwd()\nprint(pwd)\npath = pwd + '\\\\imagenet-sample-train\\\\'\ndpath = pwd + '\\\\imagenet-sample-train\\\\'\nprint(path)\nprint(dpath)",
        "text": "data can be download from [ here ] ( <url> )  .  update path below to where you download data to  .  optionally use a 2nd path for fast ( e . g  .  ssd ) storage   -   set both to the same path if use aws  . ",
        "id": 242
    },
    {
        "code": "plt.scatter(\"YearOnly\", \"Horsepower\", alpha = 0.5, data = hp_year)\nplt.xlabel(\"Year\")\nplt.ylabel(\"Avg. Horsepower\")\nplt.grid(axis = 'both')\nplt.title(\"Avg. Horsepower vs. Year\")\nplt.show()",
        "text": "use matplotlib to create a line chart of average horsepower ( y ) versus year ( x ) , * put both line and point on your chart   -  use the data  keyword argument   -  label the x   -   and y   -   ax and plot title   -  use x and y ax gridlines  . ",
        "id": 243
    },
    {
        "code": "kw_list=['Data Science',\n        'Machine Learning',\n        'Artificial Intelligence',\n        'Internet of Things',\n        'Big Data']\nloc='US-MA'\ntimeframe=t_d['past7d']\ndef pull_loc(key_words,timeframe,loc='US-MA'):\n    if type(key_words) != list:\n        key_words = [key_words]\n    pytrends.build_payload(key_words,geo=loc,timeframe=timeframe)\n    loc_df = pytrends.interest_over_time()\n    return loc_df",
        "text": "function to loop through set of keywords give",
        "id": 244
    },
    {
        "code": "def rmsle(lp, la):\n    return np.sqrt(np.mean(np.square(lp - la)))\nfrom sklearn.preprocessing import Imputer\nimpute = Imputer()\ndef single_num(feature):\n    X_train = train0[feature].as_matrix().reshape(-1,1)\n    X_valid = val0[feature].as_matrix().reshape(-1,1)\n    X_tr = impute.fit_transform(X_train)\n    X_val = impute.fit_transform(X_valid)\n    tree = DecisionTreeRegressor(max_depth=5)\n    tree.fit(X_tr, y_train0)\n    y_pred = tree.predict(X_val)\n    return rmsle(y_pred, y_val0)",
        "text": "check out the predictive power of each variable for numerical variable , use linear regression or tree ( ? )",
        "id": 245
    },
    {
        "code": "hyp_to_Pdisk_cart_U = pol_to_Pdisk_cart * pol_to_hyp.inverse()\nhyp_to_Pdisk_cart_U\nhyp_to_Pdisk_cart_U.display()",
        "text": "still on $ u $ , the change of coordinate $ ( x , y ) \\rightarrow ( u , v ) $ be obtain by combine the change $ ( x , y ) \\rightarrow ( r , \\varphi ) $ with $ ( r , \\varphi ) \\rightarrow ( u , v ) $ ,",
        "id": 246
    },
    {
        "code": "df.index = df['Data']\ndf.drop('Data', axis=1 , inplace=True)\ndf.head()\nt = df.index.values\nx = df.iloc[:,0]\npl.plot(x)\npl.show",
        "text": "transform the  data  column to index",
        "id": 247
    },
    {
        "code": "a * 5              \na + 5              \na + b              \na / b              \nnp.exp(a)          \nnp.power(a,b)      \nnp.sin(a)          \nnp.cos(a)          \nnp.arctan2(y,x)    \nnp.arcsin(x)       \nnp.radians(a)      \nnp.degrees(a)      \nnp.var(a)          \nnp.std(a, axis=1)  # standard deviation",
        "text": "element   -   wise operation and math function",
        "id": 248
    },
    {
        "code": "venues_both.sort('betweenness_delta', ascending=False)[['name_x', 'betweenness_may', 'betweenness', 'betweenness_delta']][:10]",
        "text": "top 10 location with the large increase in betweenness centrality",
        "id": 249
    },
    {
        "code": "\ngenerate_learningcurves(sparse=True,gX_train=gX_train[:,[0,3,1,4,2,5,22,23,10,9]], gy_train=gy_train,\n                        alg=SVR(kernel='linear'), alg_name=\"Support Vector Machines - Top 10 Features\")",
        "text": "rerun learn curve with top 10 feature",
        "id": 250
    },
    {
        "code": "predictions = predict_sequences_multiple(model, X_test, seq_len, prediction_len)\nprint('Training duration (s) : ', time.time() - global_start_time)\nplot_results_multiple(predictions, y_test, prediction_len)",
        "text": "predict for 50 step and compare with true value",
        "id": 251
    },
    {
        "code": "\nreference_df = pd.read_pickle(save_load_path+'/mss_df.pkl')\nreference_df = reference_df.filter(items = ['track','title'])\nreference_df.columns = ['TRACK_ID','TITLE']\nreference_df[reference_df['TRACK_ID'].isin(foreign_songs)]",
        "text": "review foreign song detect by song title",
        "id": 252
    },
    {
        "code": "def add_elapsed(fld, prefix):\n    tmp_el = elapsed(fld)\n    df[prefix+fld] = df.apply(tmp_el.get, axis=1)",
        "text": "and a function for apply say class across dataframe row and add value to a new column  . ",
        "id": 253
    },
    {
        "code": "x = raw_data.filter(feature_columns).as_matrix()\ny = raw_data.filter(label_columns).as_matrix()\nx_train, x_test, y_train, y_test = train_test_split(x, y, \n                                                    train_size = 0.70, \n                                                    random_state = 100)",
        "text": "split data in train and test",
        "id": 254
    },
    {
        "code": "frame = Frame2D('KG82sd')\nframe.input_all()\nRS = frame.solve('all')\nframe.write_results(frame.dsname,RS)\nframe.print_input()\nframe.print_results(RS)",
        "text": "the *notional* lateral load of 6 . 9 kn at node   -  h  -  and 8 . 4 kn at node   -  g  -  be add a [ before ] ( zz   -   test   -   kg82 . ipynb )  . ",
        "id": 255
    },
    {
        "code": "\nmodel_pred = KerasClassifier(build_fn=create_model, optimizer=best_optimizer, init=best_init, epochs=best_epochs, batch_size=best_batch_size, verbose=verbose)\nmodel_pred.fit(X, Y)\ntest_df = pd.read_csv(file_test,index_col='PassengerId')\nX_test = test_df.values.astype(float)\nX_test = scale.transform(X_test)\nprediction = model_pred.predict(X_test)",
        "text": "build model and predit   -   create a classifier with best parameter   -   fit model   -   predict survived",
        "id": 256
    },
    {
        "code": "all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\nall_data.isnull().any().any()",
        "text": "mssubclass  -  , na most likely mean no build class  .  we can replace miss value with none",
        "id": 257
    },
    {
        "code": "\ndf_alcott = df[df['Alcott']>0]\ndf_alcott[['Word', 'Conc.M']].sort_values(by='Conc.M', ascending = False)\ndf_alcott[['Word', 'Conc.M']].sort_values(by='Conc.M', ascending = True)\ndf_austen = df[df['Austen']>0]\ndf_austen[['Word', 'Conc.M']].sort_values(by='Conc.M', ascending = False)\ndf_austen[['Word', 'Conc.M']].sort_values(by='Conc.M', ascending = True)",
        "text": "print the most concrete and abstract term in austen and in alcott  .  hint , you ca n't simply sort on the column austen _ con _ score  and so on  .  why not ? what be your next step  . ",
        "id": 258
    },
    {
        "code": "list_neg = []\nallFiles_neg = glob.glob('movie/neg/*.txt')\nfor file in allFiles_neg:\n    rfile = open(file, 'r')\n    list_neg.append(rfile.read())\ntrain_neg = pd.DataFrame(list_neg,columns=['message'])\ntrain_neg['label'] = 0\ntrain_neg['message'] = train_neg.apply(lambda r: fixMessage(r),axis=1)",
        "text": "load   -   explor negative review dataset",
        "id": 259
    },
    {
        "code": "\ngenerate_learningcurves(sparse=True, gX_train=gX_train[:,[0,3,2,1,20,9,5,19,4,12,22,7,10,8,15,13,16,17,6,11]], \n                        gy_train=gy_train, alg=GradientBoostingRegressor(), \n                        alg_name=\"Gradient Boosting - Top 20 Features\")\ngbr_predictors = [0,3,2,1,20,9,5,19]",
        "text": "rerun learn curve with top 20 feature",
        "id": 260
    },
    {
        "code": "\npathToFileInDisk = r'image/us7.jpg'\nwith open( pathToFileInDisk, 'rb' ) as f:\n    data = f.read()\nheaders = dict()\nheaders['Ocp-Apim-Subscription-Key'] = '9a5e0ec52bdd4486a1476a43b2f7f2ea'\nheaders['Content-Type'] = 'application/octet-stream'\njson = None\nparams = None\nresult = processRequest( json, data, headers, params )\nif result is not None:\n    data8uint = np.fromstring( data, np.uint8 ) \n    img = cv2.cvtColor( cv2.imdecode( data8uint, cv2.IMREAD_COLOR ), cv2.COLOR_BGR2RGB )\n    renderResultOnImage( result, img )\n    ig, ax = plt.subplots(figsize=(15, 20))\n    ax.imshow( img )",
        "text": "detect face from an image store on disk",
        "id": 261
    },
    {
        "code": "k = 10\nmultiPred = multiPredKNN (k, features_train, output_train, features_test[0:10])\nprint (multiPred)",
        "text": "quiz question   -  make prediction for the first 10 house in the test set use k   -   near neighbor with  k=10   .  1 .  what be the index of the house in this query set that ha the low predict value ? 6 2 .  what be the predict value of this house ? 350032",
        "id": 262
    },
    {
        "code": "yearly = data['2000':].resample('A')\n(yearly > 40).sum()\nyearly.plot()\nplt.axhline(40, linestyle='--', color='k')",
        "text": "< div class=  alert alert   -   success  >   question   , and be there exceedance of the yearly limit value of 40 µg/m3 since 200 ?",
        "id": 263
    },
    {
        "code": "min_rating = None\nmin_rating = ratings.rating.min()\nassert isinstance(min_rating, np.float64), \"Try again, make sure you are taking the min of just 1 column\"\nassert abs(min_rating - 0.5) < .01, \"Try again, the minimum should be 0.5\"",
        "text": "exercise 1 , find the minimum rat let 's start by compute the minimum rat  .  in the next cell , define the min _ rating  variable to be the minimum rat across all of the dataframe ,",
        "id": 264
    },
    {
        "code": "\nnums = np.random.rand(len(data))\nmask_suburban = (nums > 0.33) & (nums < 0.66)\nmask_urban = nums > 0.66\ndata['Area'] = 'rural'\ndata.loc[mask_suburban, 'Area'] = 'suburban'\ndata.loc[mask_urban, 'Area'] = 'urban'\ndata.head()",
        "text": "dummy categorical variable with more than two category let 's create a new feature call area , and randomly assign observation to be rural , suburban , or urban ,",
        "id": 265
    },
    {
        "code": "algorithm = alg.NSGAII( dpp, 100, log_frequency=100 )\nalgorithm.run(1000)",
        "text": "set an optimization algorithm on the differential privacy problem and run it  . ",
        "id": 266
    },
    {
        "code": "layer_conv2, weights_conv2 =\\\n    new_conv_layer(input=layer_conv1,\n                  num_input_channels=num_filters1,\n                  filter_size=filter_size2,\n                  num_filters=num_filters2,\n                  use_pooling=True)\nlayer_conv2\nweights_conv2",
        "text": "convolutional layer 2",
        "id": 267
    },
    {
        "code": "fig_size = [8, 4]\nplt.rcParams[\"figure.figsize\"] = fig_size\n \nplt.hist((train[['strain_1_dates', 'strain_2_dates']].max(axis=1)), bins=range(1960,2004))\nfigure(2)\nplt.hist((train[['strain_1_dates', 'strain_2_dates']].max(axis=1)), bins=range(1960,2004))\nplt.hist((test[['strain_1_dates', 'strain_2_dates']].max(axis=1)), bins=range(1960,2004), color = 'g')",
        "text": "check that the train and test data be disjoint look good",
        "id": 268
    },
    {
        "code": "X3pred = test3[['Overall Qual','Total Bsmt SF','Baths','Garage Area','Year Built','Gr Liv Area','TotRms AbvGrd',\n       'Lot Area','Overall Cond']]\nss = StandardScaler()  \nss.fit(X3_train)   \nX3s_train = ss.transform(X3_train)  \nX3ss_test = ss.transform(X3pred)",
        "text": "part 4 d . 3 , preprocessing and scale for test data",
        "id": 269
    },
    {
        "code": "sf[\"Country\"]\nsf['age']\nsf['age'].mean()",
        "text": "inspect column of data set",
        "id": 270
    },
    {
        "code": "botom_cat2 = rank_level2.head(15).reset_index()\nbotom_cat2_list = botom_cat2.cat2.unique().tolist()\nbotom_cat2_full = train.loc[train['cat2'].isin(botom_cat2_list)]\nplt.figure(figsize=(20,20))\nsns.boxplot(y ='cat2',x= 'price', data = botom_cat2_full, orient = 'h')\nplt.title('Top 15 second levels categories with lowest prices ', fontsize = 30)\nplt.ylabel ('Second level categories', fontsize = 20)\nplt.xlabel ('Price', fontsize = 20)",
        "text": "top 15 second level category with low price",
        "id": 271
    },
    {
        "code": "X_train_norm = (X_train - 128) / 128\nX_valid_norm = (X_valid - 128) / 128\nEPOCHS = 10\nBATCH_SIZE = 128\nrate = 0.001\ntrain(X_train_norm, y_train, X_valid_norm, y_valid, keep_prob_value=0.8)",
        "text": "round 4 , with normalization 1 and dropout",
        "id": 272
    },
    {
        "code": "\nnet_from_local2 = cy.network.create_from('data/galFiltered.json')\nnet_from_local1 = cy.network.create_from('data/sample_yeast_network.xgmml', collection='My Collection')\nnet_from_local2 = cy.network.create_from('data/galFiltered.gml', collection='My Collection')\nnetwork_locations = [\n    'sample_yeast_network.xgmml', \n    'http://chianti.ucsd.edu/cytoscape-data/galFiltered.sif', \n    'http://www.ebi.ac.uk/Tools/webservices/psicquic/intact/webservices/current/search/query/brca1?format=xml25' \n]\nnetworks = cy.network.create_from(network_locations)\npd.DataFrame(networks, columns=['CyNetwork'])",
        "text": "load network from file , url or web service",
        "id": 273
    },
    {
        "code": "\ncity = ox.gdf_from_place('Manhattan, New York City, New York, USA')\nox.save_gdf_shapefile(city)\ncity = ox.project_gdf(city)\nfig, ax = ox.plot_shape(city, figsize=(3,3))\nplace_names = ['Berkeley, California, USA', \n               'Oakland, California, USA',\n               'Piedmont, California, USA',\n               'Emeryville, California, USA',\n               'Alameda, Alameda County, CA, USA']\neast_bay = ox.gdf_from_places(place_names)\nox.save_gdf_shapefile(east_bay)\neast_bay = ox.project_gdf(east_bay)\nfig, ax = ox.plot_shape(east_bay)",
        "text": "part 1 , get shapefiles from openstreetmap osmnx let you download spatial  place boundary  geometry from openstreetmap , save them to shapefiles , project them , and plot them  .  for a more in   -   depth demonstration of create these shapefiles , see [ this notebook ] ( 02   -   example   -   osm   -   to   -   shapefile . ipynb )  . ",
        "id": 274
    },
    {
        "code": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nchurn_df = spark.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(churnFilename)\ncustomer_df = spark.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(customerFilename)",
        "text": "< a id=  getdata  >      -  span style=  color ,   -   fa04d9  > step 2 , read data into spark dataframes   note , you want to reference the spark dataframe api to learn more about the support operation , <url>",
        "id": 275
    },
    {
        "code": "len(in_spectrum), len(cumsum_filter)\nthinkplot.preplot(2)\nout_wave.plot(label='cumsum')\nout_wave2 = (in_spectrum * cumsum_filter).make_wave()\nout_wave2.plot(label='filtered')\nthinkplot.config(legend=True, loc='lower right')\nthinkplot.config(xlabel='Time (s)')",
        "text": "now we can compute the output wave use the convolution theorem , and compare the result ,",
        "id": 276
    },
    {
        "code": "data = [{'a': i, 'b': 2 * i}\n        for i in range(3)]\npd.DataFrame(data)",
        "text": "from a list of dicts any list of dictionary can be make into a  dataframe   .  we ll use a simple list comprehension to create some data ,",
        "id": 277
    },
    {
        "code": "opt = rpca.RobustPCA.Options({'Verbose': True, 'gEvalY': False,\n                              'MaxMainIter': 200, 'RelStopTol': 5e-4,\n                              'AutoRho': {'Enabled': True}})\nb = rpca.RobustPCA(S1, None, opt)\nX, Y = b.solve()",
        "text": "set option for the robust pca solver , create the solver object , and solve , return the estimate of the low rank and sparse component  x  and  y   .  unlike most other sporco class for optimisation problem , [ rpca . robustpca ] ( <url> ) ha a meaningful default regularization parameter , a use here  . ",
        "id": 278
    },
    {
        "code": "import datetime\ndates = [datetime.datetime(year = int(row[1]), month = int(row[2]), day = 1) for row in data[1:]]\ndate_counts = {}\nfor each in dates:\n    if each in date_counts:\n        date_counts[each] = date_counts[each] + 1\n    else:\n        date_counts[each] = 1\ndate_counts",
        "text": "count gun death by month and year",
        "id": 279
    },
    {
        "code": "connStr <- \"Driver=SQL Server;Server=localhost;Database=nyctaxi;Trusted_Connection=true\"\nsqlShareDir <- paste(\"C:\\\\AllShare\\\\\",Sys.getenv(\"USERNAME\"),sep=\"\")\nsqlWait <- TRUE\nsqlConsoleOutput <- FALSE\ncc <- RxInSqlServer(connectionString = connStr, shareDir = sqlShareDir, \n                    wait = sqlWait, consoleOutput = sqlConsoleOutput)\nrxSetComputeContext(cc)",
        "text": "specify the database connection * rodbc style connection string * compute context can be * server , rx command will run on the server close to the data * local , command will run locally on the client  .  data need to bring to the client   -  refer to ,   -  url>   -  url>   -  url>",
        "id": 280
    },
    {
        "code": "import random\nrandom.seed(8675309)\nhuman_files = np.array(glob.glob('/home/rushil/Downloads/dog/human/*.jpg'))\nprint('There are %d total human images.' % len(human_files))",
        "text": "import human dataset import dataset of human image , path file of human be store in array human _ files   . ",
        "id": 281
    },
    {
        "code": "\ntwice = d6 + d6\ntwice[2] = 0\ntwice[3] = 0\ntwice.normalize()\ntwice.bar()\ndecorate_dice('Two dice, greater than 3')\ntwice.mean()",
        "text": "exercise 2 ,   -  suppose i roll two dice and tell you the result be great than 3 .  plot the pmf  of the remain possible outcome and compute it mean  . ",
        "id": 282
    },
    {
        "code": "import requests\nfrom bs4 import BeautifulSoup\nurl = 'https://www.python.org/~guido/'\nr = requests.get(url)\nhtml_doc = r.text\nsoup = BeautifulSoup(html_doc)\npretty_soup = soup.prettify()\ntype(soup)          \ntype(pretty_soup)   \nguido_title = soup.title       \nguido_text = soup.get_text()   \na_tags = soup.find_all('a')    \nfor link in a_tags:\n    print(link.get('href'))    \n                               # get \"pics.html\"",
        "text": "scrap the web in python",
        "id": 283
    },
    {
        "code": "neutral = 0\npositive = 0\nnegative = 0\nfor i in range (0, len(data)):\n    if data[i][\"manual_sentiment\"] == \"neutral\":\n        neutral +=1\n    elif data[i][\"manual_sentiment\"] == \"positive\":\n        positive +=1\n    else:\n        negative +=1       \nprint ('neutral ',neutral)\nprint ('positive',positive)\nprint ('negative',negative)",
        "text": "check amount of sentiment data",
        "id": 284
    },
    {
        "code": "print(\"\\n==============> WITH my own image\")\nif int(PyVersion[0])==2:\n    data_dir_jc_photo = 'Kouassi-Jean-Claude-ID.png'\nelif int(PyVersion[0])==3:\n    data_dir_jc_photo = 'Kouassi-Jean-Claude-ID.png'\njc_image = plt.imread(data_dir_jc_photo)\njc_image = imresize(jc_image, (227, 227))\nimages_to_plot.append(jc_image)\njc_image = jc_image[np.newaxis]\njc_image = jc_image[:, :, :, :3]\nprint(\"jc_image shape : \", jc_image.shape)\nprint(\"Label Prediction with JC Photo: \", tflearn_model.predict(jc_image))\nprint(\"True Label Value : (4, 0)\")",
        "text": "< a name=  prediction _ on _ otherfaces _ tflearn  >      -  ii . 3 . 1 . 2   -   with other face image",
        "id": 285
    },
    {
        "code": "z_4d = tf.reshape(gaussian_2d, [ksize, ksize, 1, 1])\nprint(z_4d.get_shape())\nconvolved = tf.nn.conv2d(img_4d, z_4d, strides=[1, 1, 1, 1], padding='SAME')\nwith tf.Session() as sess:\n    res = convolved.eval()\nprint(res.shape)\nplt.subplot(121)\nplt.imshow(img, cmap='Greys')\nplt.subplot(122)\nplt.imshow(np.squeeze(res), cmap='Greys')",
        "text": "exercise 1 1 .  define an arbitrary square filter ( could be random or could be of your choice )   -   convolve it with the image a display above   -   display the image , the filter and the convolve image",
        "id": 286
    },
    {
        "code": "X_reduced = IncrementalPCA(n_components=n_components, batch_size=10).fit_transform(X)\ncolors = ['navy', 'turquoise', 'darkorange']\nlw = 2\nplt.figure()\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    plt.scatter(X_reduced[y == i, 0], X_reduced[y == i, 1], color=color, alpha=.8, lw=lw,\n                label=target_name)\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nplt.title('IPCA of IRIS dataset')\nplt.show()",
        "text": "2d plot of first 2 principal component of incremental pca",
        "id": 287
    },
    {
        "code": "hashingTF = HashingTF(numFeatures=250)\ntestcsv_data = sc.textFile('./data/test.csv', use_unicode=True).map(get_cleaned_tweet)\n(acc_val, actual, predication), model = decisionTree([], testcsv_data, model)",
        "text": "< font color = black  > test the model",
        "id": 288
    },
    {
        "code": "\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfeatures = []\nfeatures.append(('pca', PCA(n_components=3)))\nfeatures.append(('select_best', SelectKBest(k=4)))\nfeature_union = FeatureUnion(features)\nestimators = []\nestimators.append(('feature_union', feature_union))\nestimators.append(('GBC', GradientBoostingClassifier()))\nmodel = Pipeline(estimators)\nkfold = KFold(n_splits=10, random_state=899)\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())",
        "text": "create a pipeline that extract feature from the data then create a model ( from reference 24 )",
        "id": 289
    },
    {
        "code": "import numpy as np\nx = np.array([2, 4, 6, 8])\nx[1:3]\ny = np.linspace(0,100,26) \nprint(y)\ntype(y)\ntype(y[1])\nz = np.arange(0,100,4, dtype=float) \nprint(z)",
        "text": "slice on numpy array array be the essential data structure in numerical compute  .  they be available to you via the numpy library  .  load the library like this ,",
        "id": 290
    },
    {
        "code": "from bokeh.io import show\nfrom bokeh.layouts import column\nfrom bokeh.models import Slider\nslider = Slider(start=0, end=10, step=0.1, value=0, title=\"value\")\ncustom = Custom(text=\"Special Slider Display\", slider=slider)\nlayout = column(slider, custom)\nshow(layout)",
        "text": "then the new model can be use seamlessly in the same way a any build   -   in bokeh model ,",
        "id": 291
    },
    {
        "code": "asp30 = topology.residue(29)\nasp33 = topology.residue(32)\nprint(asp30, asp33)\ngtp201= topology.residue(166)\nmg202 = topology.residue(167)\nprint(gtp201, mg202)",
        "text": "list the atom contact most common within a give residue contact",
        "id": 292
    },
    {
        "code": "start = time.time()\nuser_sample = 0.05\npr=Evaluation.precision_recall_calculator(test_data, train_data,pm,is_model)\n(pm_avg_precision_list, pm_avg_recall_list,ism_avg_precision_list,ism_avg_recall_list)=pr.calculate_me\nasures(user_sample)\nend = time.time()\nprint(end - start)",
        "text": "use the above precision recall calculator class to calculate the evaluation measure",
        "id": 293
    },
    {
        "code": "n_col_to_display = 20\npca_range = np.arange(n_col_to_display) + 1\npca_names = ['PCA_%s' % i for i in pca_range]\nplt.figure(figsize=(10, 10))\nplt.barh(pca_range, pca.explained_variance_ratio_[:n_col_to_display], align='center')\nxticks = plt.yticks(pca_range, pca_names)\nplt.xlabel('Proportion of Variance Explained')\nplt.show()",
        "text": "viz , plot proportion of variance explain with top principal component",
        "id": 294
    },
    {
        "code": "pools[0].listSpikes()\npools[1].listSpikes()\nplt.figure()\nplt.plot(pools[1].poolTerminalSpikes[:, 0],\n         pools[1].poolTerminalSpikes[:, 1]+1, '.')\nplt.xlabel('t (ms)')\nplt.ylabel('Descending Command index')",
        "text": "the spike time of all descend command along the 5000 m of simulation be show in fig  .  \\ref { fig , spikesdescmn }  . ",
        "id": 295
    },
    {
        "code": "\nx = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\nv = np.array([1, 0, 1])\ny = x + v  \nprint(y)",
        "text": "numpy   -  broadcasting  -  allow u to perform this computation without actually create multiple copy of v .  consider this version , use broadcast ,",
        "id": 296
    },
    {
        "code": "def fib(n):\n    List = [0,1]\n    a,b = 0,1\n    List.append(a)\n    List.append(b)\n    for i in range(2,n):\n        a,b = b , a + b\n        List.append(b)\n    return List\nprint(fib(10))",
        "text": "function that return a list of number of fibonacci number",
        "id": 297
    },
    {
        "code": "criterion = nn.NLLLoss()\nlearning_rate = 0.0005\ndef train(input_line_tensor, target_line_tensor):\n    hidden = rnn.initHidden()\n    rnn.zero_grad()\n    loss = 0\n    for i in range(input_line_tensor.size()[0]):\n        output, hidden = rnn(input_line_tensor[i], hidden)\n        loss += criterion(output, target_line_tensor[i])\n    loss.backward()\n    for p in rnn.parameters():\n        p.data.add_(-learning_rate, p.grad.data)\n    return output, loss.data[0] / input_line_tensor.size()[0]",
        "text": "train the network    -  in contrast to classification , where only the last output be use , we be make a prediction at every step , so we be calculate loss at every step  .  the magic of autograd allow you to simply sum these loss at each step and call backward at the end  . ",
        "id": 298
    },
    {
        "code": "counts.is_cached\ncounts.persist()\ncounts.is_cached\ncounts.takeOrdered(5, lambda x: -x[1])\ncounts.takeOrdered(5, lambda x: x[1])\ncount_dict = counts.collectAsMap()\ncount_dict['circle']",
        "text": "persist data the top _ word  program will repeat all the computation each time we take an action such a takeordered   .  we need to persist  or cache  the result   -   they be similar except that persist  give more control over how the data be retain  . ",
        "id": 299
    },
    {
        "code": "\nparams = {'project' : 'en.wikipedia.org',\n            'access' : 'desktop',\n            'agent' : 'user',\n            'granularity' : 'monthly',\n            'start' : '2015070100',\n            'end' : '2017100100'\n         }\napi_call = requests.get(endpoint.format(**params))\nresponse_pageviews_desktop_201507_201709 = api_call.json()\nwith open('data/pageviews_desktop_201507_201709.json', 'w') as outfile:\n    json.dump(response_pageviews_desktop_201507_201709, outfile)",
        "text": "call the pageviews api ( documentation , endpoint ) that provide access to desktop traffic data from july 2015 through september 2017 and save the raw result into a json source data file  . ",
        "id": 300
    },
    {
        "code": "tips.groupby(['day', 'smoker'], as_index=True).mean()\ntips.groupby(['day', 'smoker'], as_index=False).mean()",
        "text": "return aggregate data without row index p362",
        "id": 301
    },
    {
        "code": "from glob import glob\nimport os\nimport os.path as op\nimport shutil\nBASE_PATH = '/om/user/jakubk/meningioma/'\nORIG_PATH = op.join(BASE_PATH, 'ants_skullstrip/ants_brainstripper_NKI')\nSAVE_PATH = op.join(BASE_PATH, 'ants_skullstrip/NKI_output')\nall_files = sorted(glob(op.join(ORIG_PATH,'**/strip/*Brain.nii.gz')))\nfor f in all_files:\n    subject_id = f.split('/')[-3][-17:-7]\n    new_fname = \"{}_brain.nii.gz\".format(subject_id)\n    save_abs_fname = op.join(SAVE_PATH, new_fname)\n    # shutil.copyfile(f, save_abs_fname)",
        "text": "save ant brainextraction output ( with nki template )",
        "id": 302
    },
    {
        "code": "plt.imshow(grid, cmap='Greys', origin='lower')\nplt.plot(x_init[1], x_init[0], 'ro')\nplt.xlabel(\"X\", fontsize = 18)\nplt.ylabel(\"Y\", fontsize = 18)\nfor (v1, v2) in rrt.edges:\n    plt.plot([v1[1], v2[1]], [v1[0], v2[0]], 'y-')\nplt.show()",
        "text": "now let 's plot the generate rrt  . ",
        "id": 303
    },
    {
        "code": "from scipy.ndimage import filters,morphology\ndef imrow(*args,**kw):\n    size = kw.get(\"s\",8)\n    if \"s\" in kw: del kw[\"s\"]\n    n = len(args)\n    gray()\n    subplots(1,n,figsize=(n*size,size))\n    for i,im in enumerate(args):\n        subplot(1,n,i+1); imshow(im,**kw)",
        "text": "title , grayscale morphology author , thomas m .  breuel institution , unikl",
        "id": 304
    },
    {
        "code": "train_data = train_data.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_data = test_data.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\nfull_data = [train_data, test_data]\ntrain_data.head()",
        "text": "let u drop parch , sibsp , and familysize feature in favor of isalone  . ",
        "id": 305
    },
    {
        "code": "nn = neural_net(alpha=0.001, max_steps=1000, hidden_size=30, lam=0.005)\nnn.fit(np.asarray(X_train), np.asarray(Y_train))\nnn.calc_scores(np.asarray(X_test), np.asarray(Y_test))",
        "text": "calculate accuracy and f1 score",
        "id": 306
    },
    {
        "code": "alpha.word_freq_plot('iden.txt', top=40, stop_words=EN);",
        "text": "interactive word frequency bar chart plot",
        "id": 307
    },
    {
        "code": "\nreview_df_filter_df = review_df.merge(restaurant_df,how='inner',on='business_id')\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 5))\nsns.distplot(review_df_filter_df.stars_x,kde=False,color = 'g',ax =ax,bins=20);\nax.axvline(review_df_filter_df.stars_x.mean(), 0, 1, color='r', label='Mean')\nax.legend();\nax.set_ylabel('Count',size=20)\nax.set_xlabel('Stars',size=20)\nax.set_title('Distribution(count) of different Reviews rating',size=20)",
        "text": "distribution count of review rat for restaurant we can see below more review have 5 rat than other rat",
        "id": 308
    },
    {
        "code": "\nX_text_features = vectoriser.transform(titles)\nother_features_columns = [\"html_ratio\", \"image_ratio\"]\nother_features = data[other_features_columns]\nfrom scipy.sparse import hstack\nX = hstack((X_text_features, other_features)).toarray()\nscores = cross_val_score(model, X, y, scoring = \"roc_auc\")\nprint(\"CV AUC {}, Average AUC {}\".format(scores, scores.mean()))\nmodel.fit(X, y)\nall_feature_names = vectoriser.get_feature_names() + other_features_columns\nfeature_importances = pd.DataFrame({\"Features\" : all_feature_names, \"Importance Score\": model.feature_importances_})\nfeature_importances.sort_values(\"Importance Score\", ascending = False).head()",
        "text": "exercise , build a random forest model to predict evergreeness of a website use the title feature and quantitative feature",
        "id": 309
    },
    {
        "code": "wine.loc[:2, 'alcohol'] = np.nan\nwine.head()",
        "text": "set the value of the first 3 row from alcohol a nan",
        "id": 310
    },
    {
        "code": "dir_ex_coeff_fname = 'results/usALEX - direct excitation coefficient dir_ex_t beta.csv'\ndir_ex_t = np.loadtxt(dir_ex_coeff_fname, ndmin=1)\nprint('Direct excitation coefficient (dir_ex_t):', dir_ex_t)\ngamma_fname = 'results/Multi-spot - gamma factor.csv'\ngamma = np.loadtxt(gamma_fname, ndmin=1)\nprint('Multispot Gamma Factor (gamma):', gamma)",
        "text": "load the   -  direct excitation coefficient  -  ( $ d _  { dirt } $ ) from disk ( compute in [ usalex   -   correction   -   direct excitation physical parameter ] ( usalex   -   correction   -   direct excitation physical parameter . ipynb ) ) ,",
        "id": 311
    },
    {
        "code": "num_folds = 10\nnum_instances = len(X_train)\nseed = 7\nscoring = 'accuracy'",
        "text": "test harness ? what be test harness ? we will use 10   -   fold cross validation to estimate accuracy  .  this will split our dataset into 10 part , train on 9 and test on 1 and repeat for all combination of train   -   test split  . ",
        "id": 312
    },
    {
        "code": "import gym\nmake_env = lambda: gym.make(\"Acrobot-v1\")\nenv=make_env()\nenv.reset()\nstate_shape = env.observation_space.shape\nn_actions = env.action_space.n\nplt.imshow(env.render(\"rgb_array\"))\ndel env",
        "text": "experiment setup * here we simply load the game and check that it work",
        "id": 313
    },
    {
        "code": "\npred_y_test, loss,_,rf_model = runRF(X_train.values, y_train, X_test.values, y_test,rounds=187)",
        "text": "model build   -   binary classification",
        "id": 314
    },
    {
        "code": "lens = np.array(map(len, trn))\n(lens.max(), lens.min(), lens.mean())",
        "text": "look at distribution of length of sentence",
        "id": 315
    },
    {
        "code": "C, average_clustering(lattice)",
        "text": "the cluster coefficient be a little high than in the dataset  . ",
        "id": 316
    },
    {
        "code": "\ncredit[:10][['CreditAmount', 'Duration']].values",
        "text": "a very common scenario will be the follow  .  we want to select specific observation and column of a dataframe and convert to a numpy array so that we can use it for feature extraction , classification etc  .  this can be achieve by use the values  method  . ",
        "id": 317
    },
    {
        "code": "\nsim = nengo.Simulator(model)\nsim.run(5)\nplt.plot(sim.trange(), sim.data[neurons_probe], label=\"Decoded output\")\nplt.plot(sim.trange(), sim.data[sin_probe], 'r', label=\"Sine\")\nplt.plot(sim.trange(), sim.data[cos_probe], 'k', label=\"Cosine\")\nplt.legend()\nplt.xlabel('time [s]');",
        "text": "run the model in order to run the model , we have to create a simulator  .  then , we can run that simulator over and over again without affect the original model  . ",
        "id": 318
    },
    {
        "code": "\nlayer_1 = tf.add(tf.matmul(x, weights['hidden_layer']),biases['hidden_layer'])\nlayer_1 = tf.nn.relu(layer_1)\nops = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])",
        "text": "hide and output layer definition ( use tensorflow mathematical function )",
        "id": 319
    },
    {
        "code": "tmp = np.array([40,41]).reshape(2,1)\ntmp\na[:,:] = tmp\na",
        "text": "assign 1d array to all column of 2d array",
        "id": 320
    },
    {
        "code": "\nx_def, y_def = ecdf(n_defaults)\nplt.xlabel('defaults')\nplt.ylabel('ECDF')\nplt.plot(x_def, y_def, marker='.', linestyle='none')\nplt.show()\nn_lose_money = np.sum(n_defaults >= 10)\nprint('Probability of losing money =', n_lose_money / len(n_defaults))",
        "text": "will the bank fail ? plot the number of default n _ defaults  a a cdf  .  if interest rate be such that the bank will lose money if 10 or more of it loan be default upon , what be the probability that the bank will lose money ?",
        "id": 321
    },
    {
        "code": "filter(lambda x: x%2==0,lst)",
        "text": "filter ( ) be more commonly use with lambda function , this because we usually use filter for a quick job where we do n't want to write an entire function  .  let repeat the example above use a lambda expression ,",
        "id": 322
    },
    {
        "code": "NUM_STEPS = 20\ndef experiment_fn(run_config, params):\n    feature_cols = [tf.contrib.layers.real_valued_column(\"\",\n        dimension=NUM_FEATURES)]\n    estimator = tf.contrib.learn.Estimator(model_fn=model_fn,\n        model_dir=MODEL_DIR)    \n    return tf.contrib.learn.Experiment(\n        estimator=estimator,\n        train_input_fn=train_input_fn,\n        train_steps=NUM_STEPS,\n        eval_input_fn=test_input_fn)",
        "text": "alternatively   -  define experiment a model be wrap in an estimator , which be then wrap in an experiment  .  once you have an experiment , you can run this in a distribute manner on cpu or gpu  . ",
        "id": 323
    },
    {
        "code": "orday=orders.order_dow.value_counts()\norday.sort_index().plot(kind='bar', color='r')\n_=plt.xlabel('day of week')\n_=plt.ylabel('order volume')\n_=plt.xticks([0,1,2,3,4,5,6], ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\nplt.title('Daily Order Volume')\norday.idxmax()\norday\norday.idxmin()",
        "text": "what day of the week be order volume higest and low ?    -  ( high on monday and low on friday )",
        "id": 324
    },
    {
        "code": "sorted(dcentralities.items(), key = lambda x: x[1], reverse = True)[:10]\nsorted(dcentralities.items(), key = lambda x: x[1], reverse = True)[-10:]",
        "text": "what be the top and bottom ten word in term of degree ?",
        "id": 325
    },
    {
        "code": "pred_test_x_np = np.array(pred_test_x_df)\npred_test_x_np = pred_test_x_np.reshape((pred_test_x_df.shape[0], 24, pred_test_norm_df.shape[1]))\nprint(pred_test_x_df.shape)\nprint(pred_test_x_np.shape)",
        "text": "convert the lstm prediction x dataframe to a numpy array , then reshape into the 3d format require for lstm model  . ",
        "id": 326
    },
    {
        "code": "latstarteof=10.\nlatstopeof=70.\namocDC_select_NORTH = amocdc.sel(time_counter=slice('1979-01-01','2015-12-31'),nav_lat=slice(latstarteof,latstopeof))\nEMEAN = amocDC_select_NORTH.mean(dim='e')\nINTR  = amocDC_select_NORTH - EMEAN",
        "text": "eofs from monthly amoc ( north of 10n )",
        "id": 327
    },
    {
        "code": "np.arange(7)\nnp.arange(10, 50)\nnp.arange(10, 50) - 10+ 1\nlen(np.arange(10, 25))\nnumpy_range_array = np.arange(10, 25)\nnumpy_range_array.size\nnp.arange(10, 100, 5)\nnp.arange(26, step = 4)",
        "text": "intrinsic numpy array creation use numpy method",
        "id": 328
    },
    {
        "code": "\nb = np.sum(((bblnrgdata_cut3.logEnergy - bblnrgdata_cut3.pred1) ** 2) / bblnrgdata_cut3.logEnergy)\nprint ('Chi Square value for unit v energy model is', b)\nscipy.stats.chisquare(f_obs=bblnrgdata_cut3.logEnergy, f_exp=bblnrgdata_cut3.pred1)\nlm.summary()",
        "text": "caption , line fit model seem a good fit for unit v energy",
        "id": 329
    },
    {
        "code": "from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier()\nmodel.fit(x_train_dataset1,y_train_dataset1)\nprediction=model.predict(x_test_dataset1)\nmetrics.accuracy_score(prediction,y_test_dataset1)",
        "text": "use a decision tree classifier ,",
        "id": 330
    },
    {
        "code": "connStr <- paste(\"Driver=SQL Server;Server=\", sql_server, \";Database=nyctaxi;Uid=\", user_name, \";Pwd=\", password, sep=\"\")\nsqlShareDir <- paste(\"C:\\\\AllShare\\\\\",Sys.getenv(\"USERNAME\"),sep=\"\")\nsqlWait <- TRUE\nsqlConsoleOutput <- FALSE\ncc <- RxInSqlServer(connectionString = connStr, shareDir = sqlShareDir, \n                    wait = sqlWait, consoleOutput = sqlConsoleOutput)\nrxSetComputeContext(cc)",
        "text": "specify the database connection * rodbc style connection string * compute context can be * server , rx command will run on the server close to the data * local , command will run locally on the client  .  data need to bring to the client  . ",
        "id": 331
    },
    {
        "code": "sns.countplot(titanic['Pclass'], hue=titanic['person'])\ntitanic['person'].value_counts()",
        "text": "male , female , child in pclass",
        "id": 332
    },
    {
        "code": "from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nestimator = LogisticRegression()\nselector = RFE(estimator, step=1, n_features_to_select=5)\nselector = selector.fit(Xt, y)\nrfecv_columns = Xt.columns[selector.support_]\nrfecv_columns\nselector.support_\nselector.ranking_\n# Bonus - also try from sklearn.feature_selection import RFECV",
        "text": "recursive feature elimination scikit learn  also offer recursive feature elimination a a class name rfe   .  use it in combination with a logistic regression model to see what feature would be keep with this method   -  > store them in a variable call rfecv _ columns",
        "id": 333
    },
    {
        "code": "print(len(df_AA5_Reviews['reviewText'][0]))\nprint(len(df_AA5_Reviews['reviewText'][1]))",
        "text": "check the length of entire reviewtext  field of the few record of review dataframe",
        "id": 334
    },
    {
        "code": "import matplotlib.pyplot as plt\nimgDigit = mnist.train.images[0]\nplt.imshow(imgDigit.reshape([28,28]))\nplt.show()",
        "text": "mnist train data each sample on the mnist dataset consist on a 784 feature vector encode a 28x28 grayscale image  . ",
        "id": 335
    },
    {
        "code": "min_price = min(zip(prices.values(), prices.keys()))\nmax_price = max(zip(prices.values(), prices.keys()))\n# max_price is (612.78, 'AAPL')",
        "text": "in order to perform useful calculation on the dictionary content , it be often useful to invert the key and value of the dictionary use zip ( )   .  for example , here be how to find the minimum and maximum price and stock name ,",
        "id": 336
    },
    {
        "code": "\npca_subset = pca_result.select('id','features','x_coord','y_coord')\nviz_df = pca_subset.join(results,on=['id','features']).select('x_coord','y_coord','prediction','label')\nviz_df = viz_df.dropna(how='any',subset=['x_coord','y_coord']).toPandas()\na = sns.lmplot(x='x_coord',y='y_coord',hue='prediction',col='label',data=viz_df,\n           fit_reg=False,palette={1:'red',0:'green'},size=6,aspect=1)\na.set_titles(col_template=['Decision Tree results when label==\"Benign\"','Decision Tree results when label==\"Malignant\"'])",
        "text": "logistic regression    -  from pyspark . ml . classification import logisticregression   -   define estimator and fit data estimator = logisticregression ( ) pipeline = pipeline ( stages= [ assembler , estimator ] ) lr _ model = pipeline . fit ( train )   -   get result result = lr _ model . transform ( test ) precision _ recall ( result )",
        "id": 337
    },
    {
        "code": "\nfrom sklearn.cross_validation import cross_val_score\nmy_linreg = LinearRegression()\nmse_list = cross_val_score(my_linreg, X, y, cv=10, scoring='neg_mean_squared_error')\nprint(mse_list)\nmse_list_positive = -mse_list\nrmse_list = np.sqrt(mse_list_positive)\nprint(rmse_list)\nprint(rmse_list.mean())",
        "text": "now , use 10   -   fold cross   -   validation to evaluate the performance of a linear regression in predict the balance  .  thus , rather than split the dataset into test and train , use cross   -   validation to evaluate the regression performance  .  what be the rmse when you use cross validation ?",
        "id": 338
    },
    {
        "code": "np.savetxt('gaas_test2.txt', results_array, delimiter = \"\\t\", newline=\"\\r\\n\")",
        "text": "save to a txt file",
        "id": 339
    },
    {
        "code": "\nreview=[]\nwith open(files[1]) as data_file:\n    data=data_file.read()\n    for i in data.split('\\n'):\n        review.append(i)\n        \nreviewDataframe=[]\nfor x in review:\n    try:\n        jdata=json.loads(x)\n        reviewDataframe.append((jdata['reviewerID'],jdata['asin'],jdata['overall'])) \n    except:\n        pass\nreview_dataset=pd.DataFrame(reviewDataframe,columns=['Reviewer_ID','Asin','Rating'])",
        "text": "clean of reviewsample . json  file and import the data a panda dataframe  . ",
        "id": 340
    },
    {
        "code": "import numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\nimg = np.zeros((512,512,3), np.uint8)\nimg = cv2.circle(img,(447,63), 63, (0,0,255), -1)\nplt.imshow(img)\nplt.show()",
        "text": "draw circle   to draw a circle , you need it center coordinate and radius  .  we will draw a circle inside the rectangle draw above",
        "id": 341
    },
    {
        "code": "import pickle\nwith open(\"../data/randomforest_params_gonorrhea.pickle\", \"wb\") as myfile:\n    pickle.dump(clf, myfile)\nwith open(\"../data/Ymean_gonorrhea.pickle\", \"wb\") as myfile:\n    pickle.dump(Ymean, myfile)\nwith open(\"../data/Ystd_gonorrhea.pickle\", \"wb\") as myfile:\n    pickle.dump(Ystd, myfile)\ndeployed_model = pickle.load(open('../data/randomforest_params_gonorrhea.pickle', \"rb\" ))\nprint('Variance score: %.5f\\t(%.5f)' % (deployed_model.score(X_test, Y_test), deployed_model.score(X_full, Y_full)))",
        "text": "save model parameter for use in web app ,",
        "id": 342
    },
    {
        "code": "[1,2,3] == [1,2,4]\n[1,2,3] < [1,2,4]",
        "text": "we can do boolean test on list a well ,",
        "id": 343
    },
    {
        "code": "def sumIndividual(n):\n    if len(str(n)) == 1:\n        return n\n    else:\n        return n%10 +sumIndividual(n/10)\nsumIndividual(4321)\nsumIndividual(4044010014)\nsumIndividual(43214321)",
        "text": "give an integer , create a function which return the sum of all the individual digit in that integer  .  for example , if n = 4321 , return 4+3+2+1",
        "id": 344
    },
    {
        "code": "def generator(noise_dim=NOISE_DIM):\n    model = nn.Sequential(\n        torch.nn.Linear(noise_dim, 1024),\n        torch.nn.ReLU(),\n        torch.nn.Linear(1024, 1024),\n        torch.nn.ReLU(),\n        torch.nn.Linear(1024, 784),\n        torch.nn.Tanh()\n    )\n    return model",
        "text": "generator now to build the generator network , * fully connect layer from noise _ dim to 1024 * relu * fully connect layer with size 1024 * relu * fully connect layer with size 784 * tanh  ( to clip the image to be in the range of [   -   1,1 ] )",
        "id": 345
    },
    {
        "code": "\npums_p_sf.groupby(['PUMA'])['PWGTP'].agg(['size'])",
        "text": "down to several thousand record now  .  how many record in each puma ?",
        "id": 346
    },
    {
        "code": "import pandas as pd\nmovies = pd.read_csv('imdb_1000.csv')\nmovies.head()\nmovies.info()\nmovies[movies.duration >= 200]\nmovies[(movies.duration >= 200) & (movies.genre == 'Drama')]\nmovies[movies.genre.isin(['Crime', 'Drama', 'Action'])].head(20)",
        "text": "how do i apply multiple filter criterion to a panda dataframe ?",
        "id": 347
    },
    {
        "code": "\nindir_varyCP = []\nfor x in np.arange(80,200,10):\n    indir_varyCP_matrix = indirectTest(linda,feat_weights, iterations, features, distribution, [40,40,12,x])\n    indir_varyCP_dirStd, indir_varyCP_dirMean = getStdMean(indir_varyCP_matrix)\n    X = [indir_varyCP_dirStd, indir_varyCP_dirMean]\n    indir_varyCP.append(X)\nindir_varyCP = np.array(indir_varyCP)\nplot_multiple(indir_varyCP,\"Average Probability judgements for indirect test over 100 iterations\\n (Varying CP proportion while keeping B & BF & F equal)\",\n             \"Number of common people\", np.arange(80,200,10))",
        "text": "keep b , f , and bf the same , vary common people",
        "id": 348
    },
    {
        "code": "def dayofweek_count(lst):\n    day_count = {}\n       \n    for row in lst:\n        day_of_week = row[3]\n        births = row[4]\n        if day_of_week in day_count:\n            day_count[day_of_week] += births\n        else:\n            day_count[day_of_week] = births\n            \n    return day_count\ndayofweek_count(int_list)",
        "text": "count the number of birth each day of the week",
        "id": 349
    },
    {
        "code": "dfBodyTemp = dfBodyTemp.sort_values(['temperature'])\ndfmean = np.mean(dfBodyTemp['temperature'])\ndfstd = np.std(dfBodyTemp['temperature'], ddof = 1)\nxs = np.linspace(90, 110, 10)\nys = ss.norm.pdf(dfBodyTemp['temperature'], dfmean, dfstd)\nplt.plot(dfBodyTemp['temperature'], ys, linewidth=4, color=COLOR1, label = 'Normal PDF Distribution')\nplt.hist(dfBodyTemp['temperature'], normed=True, histtype='stepfilled', color = COLOR11, label = 'Histogram PDF')\nplt.legend(loc='best', frameon=False)\nplt.xlabel('Body Temperature (F)')\nplt.ylabel('PDF')",
        "text": "the normal ( gaussian ) probability density function ( pdf ) of human body temperature",
        "id": 350
    },
    {
        "code": "_x1 = np.random.multivariate_normal(mean=[0, 0], cov=np.array([[0, 0.5], [0.5, 0]]), size=(900,))\n_x2 = np.random.multivariate_normal(mean=[0.75, 0.75], cov=np.array([[0, 0.125], [0.125, 0]]), size=(100,))\nX = np.r_[_x1, _x2]\ny = np.zeros((X.shape[0],))\ny[900:] = 1\nrand_ix = np.arange(1000)\nX = X[rand_ix, :]\ny = y[rand_ix]\n# enter code here",
        "text": "exercise , try kfold cross validation on the follow dataset ,",
        "id": 351
    },
    {
        "code": "np.argmin(euclidian_distance(features_train, features_test[2]))",
        "text": "quiz question , take the query house to be third house of the test set ( features _ test [ 2 ] )  .  what be the index of the house in the train set that be close to this query house ?",
        "id": 352
    },
    {
        "code": "y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')",
        "text": "next we have the placeholder variable for the true label associate with the image that be input in the placeholder variable x .  the shape of this placeholder variable be [ none , num _ classes ] which mean it may hold an arbitrary number of label and each label be a vector of length num _ classes which be 10 in this case  . ",
        "id": 353
    },
    {
        "code": "clf = tree.DecisionTreeClassifier() \nclf = clf.fit(train, classes)\nnp.sum(clf.predict(test) == test_classes) / len(test)",
        "text": "out the box decision tree",
        "id": 354
    },
    {
        "code": "class Average:\n    \n    def __init__(self):\n        self.series = []\n    def __call__(self, new_value):\n        self.series.append(new_value)\n        total = sum(self.series)\n        return total / len(self.series)\navg = Average()\navg(10)\navg(11)\navg(12)\ndef make_averager():\n    series = []\n    def averager(new_value):\n        series.append(new_value)\n        total = sum(series)\n        return total / len(series)\n    return averager\navg = make_averager()\navg(10)\navg(11)\navg(12)",
        "text": "a closure be a function with an extend scope that encompass non   -   global variable reference in the body of the function but not define there  .  it doe not matter whether the function be anonymous or not , what matter be that it can access non   -   global variable that be define outside of it body  . ",
        "id": 355
    },
    {
        "code": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=21)\nscaler = StandardScaler()  \nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)",
        "text": "split the data in to train and test set",
        "id": 356
    },
    {
        "code": "ts = pd.Series(np.random.randn(4),\n               index=pd.date_range('1/1/2000', periods=4, freq='M'))\nts\nts.shift(2)\nts.shift(-2)\nts / ts.shift(1) - 1 \nts.shift(2, freq='M')\nts.shift(3, freq='D')\nts.shift(1, freq='90T') # `T` for minutes (naturally ;) )",
        "text": "shift ( lead and lag ) data",
        "id": 357
    },
    {
        "code": "from scipy.ndimage import filters\ndef imrow(*args,**kw):\n    size = kw.get(\"s\",8)\n    if \"s\" in kw: del kw[\"s\"]\n    n = len(args)\n    gray()\n    subplots(1,n,figsize=(n*size,size))\n    for i,im in enumerate(args):\n        subplot(1,n,i+1); imshow(im,**kw)\ntest = mean(imread(\"testimage.jpg\")/255.0,axis=2)",
        "text": "title , binary morphology author , thomas m .  breuel institution , unikl",
        "id": 358
    },
    {
        "code": "subset_data = pd.merge(all_data, product_time, left_on= 'date', \n                       right_index=True, how='inner') \nsubset_data['date']=pd.to_datetime(subset_data['date'], format='%Y/%m/%d %H:%M:%S') \ntest=subset_data.sort_values('perexc')\ntest",
        "text": "match the date of stream flow data to the date where satellite information exists  - ",
        "id": 359
    },
    {
        "code": "num_bought_green_tea = sum(df['Bought Green Tea'])\nbought_tea_zip = df[(df['Bought Green Tea']==True) & (df['Zipcode']==88005)]\nbought_tea_zip\nnum_bought_tea_and_live_88005 = len(bought_tea_zip['Zipcode'])\nprint('{} / {} = {}'.format(num_bought_tea_and_live_88005, num_bought_green_tea, num_bought_tea_and_live_88005 / num_bought_green_tea))",
        "text": "probability someone who buy green tea life in post code 88005 p ( d|h ) , p ( 88005 | green tea )",
        "id": 360
    },
    {
        "code": "test_patterns(\n    'This is some text -- with punctuation.',\n    [('[^-. ]+', 'sequences without -, ., or space')],\n)",
        "text": "a character set can also be use to exclude specific character  .  the carat ( ^ ) mean to look for character that be not in the set follow the carat  . ",
        "id": 361
    },
    {
        "code": "avg_funding_round_acquired = companies[companies.status == 'acquired'].average_funding_round.dropna()\navg_funding_round_closed = companies[companies.status == 'closed'].average_funding_round.dropna()\navg_funding_round_ipo = companies[companies.status == 'ipo'].average_funding_round.dropna()\nnum_of_funding_rounds = {'ipo': np.mean(companies[companies.status == 'ipo'].funding_rounds),\n                        'acquired': np.mean(companies[companies.status == 'acquired'].funding_rounds),\n                        'closed': np.mean(companies[companies.status == 'closed'].funding_rounds)}\nnum_of_funding_rounds",
        "text": "then we have to separate average fund round for company that close from average fund round for company that get acquire and for company that get list  .  we can also analyze the average number of fund round undergo by each type of start   -   up  . ",
        "id": 362
    },
    {
        "code": "\ncfpbComplaintCbsaUaLl['Consumer complaint narrative']= cfpbComplaintCbsaUaLl['Consumer complaint narrative'].fillna('')\ncfpbComplaintCbsaUaLl['State']= cfpbComplaintCbsaUaLl['State'].fillna('')\ncfpbComplaintCbsaUaLl['UA']=cfpbComplaintCbsaUaLl['UA'].astype('category')\ncatCols= ['Product','Issue','UA']\ndfDummies= pd.get_dummies(cfpbComplaintCbsaUaLl[catCols])\ndataStg= pd.concat([cfpbComplaintCbsaUaLl,dfDummies],axis=1)",
        "text": "below i take care of miss value and recode categorical value into binary",
        "id": 363
    },
    {
        "code": "start = time.time()\nindices = np.arange(1, len(Y_test)+1)\npredictions = np.column_stack((indices, y_val))\noutput_csv_file = output_directory + str(exp_no).zfill(3) + '.csv'\ncolumn_names = ['id', 'label']\npredict_test_df = pd.DataFrame(data=predictions, columns=column_names)\npredict_test_df.to_csv(output_csv_file, index=False)\nend = time.time()\nprint('[ Step 11] Writing the test data to file: %s in %.6f ms' %(output_csv_file, (end-start)*1000))\ntotal_time += (end-start)",
        "text": "write the prediction to csv file",
        "id": 364
    },
    {
        "code": "D = np.random.uniform(0,1,100)\nS = np.random.randint(0,10,100)\nD_sums = np.bincount(S, weights=D)\nD_counts = np.bincount(S)\nD_means = D_sums / D_counts\nprint(D_means)\nimport pandas as pd\nprint(pd.Series(D).groupby(S).mean())",
        "text": "consider a one   -   dimensional vector d , how to compute mean of subset of d use a vector s of same size describe subset index ?",
        "id": 365
    },
    {
        "code": "pred = model.transform(test)\nRMSE = evaluator.evaluate(pred)\nprint (f'The RMSE is {RMSE}')",
        "text": "finally , validate the model on the test set and check the root mean square error again  . ",
        "id": 366
    },
    {
        "code": "df.loc[dates[0]]",
        "text": "selection by label   see more in selection by label ( <url> ) for get a cross section use a label",
        "id": 367
    },
    {
        "code": "print(model.predict_output_word(['emergency', 'beacon', 'received']))",
        "text": "you can get the probability distribution for the center word give the context word a input ,",
        "id": 368
    },
    {
        "code": "\nfig = plt.figure()\nax = fig.gca()\nax = sns.regplot(Z.iloc[:,0], Z.iloc[:,1],\n                 fit_reg=False, scatter_kws={'s':70}, ax=ax)\nax.set_xlabel('principal component 1', fontsize=16)\nax.set_ylabel('principal component 2', fontsize=16)\nfor tick in ax.xaxis.get_major_ticks():\n    tick.label.set_fontsize(12)    \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(12)   \nax.set_title('PC1 vs PC2\\n', fontsize=20)\nplt.show()",
        "text": "plot principal component 1 v 2 pc1 be the first column in $ z $ , and pc2 be the second  .  notice how they be un   -   correlate  . ",
        "id": 369
    },
    {
        "code": "s1 = State( guest, name=\"guest\" )\ns2 = State( prize, name=\"prize\" )\ns3 = State( monty, name=\"monty\" )",
        "text": "now let create the state for the bayesian network  . ",
        "id": 370
    },
    {
        "code": "\nsp500.Price < 100\nr = sp500[(sp500.Price < 10) & (sp500.Price > 0)] [['Price']]\nr",
        "text": "select row of a dataframe by boolean selection",
        "id": 371
    },
    {
        "code": "import numpy as np\nH_min = np.nan_to_num(entropy([1.0,0,0,0]))\nH_min\nassert( H_min < H_d)\n# hoffman encoding",
        "text": "todo _  _  , change the probability to have minimum entropy",
        "id": 372
    },
    {
        "code": "dates = pd.DatetimeIndex(['1/1/2000', '1/2/2000', '1/2/2000', '1/2/2000', '1/3/2000'])\ndup_ts = pd.Series(np.arange(5), index=dates)\ndup_ts\ndup_ts.index.is_unique",
        "text": "time series with duplicate index ,",
        "id": 373
    },
    {
        "code": "x = np.zeros(4, dtype=int)",
        "text": "but this be a bite clumsy  .  there 's nothing here that tell u that the three array be relate , it would be more natural if we could use a single structure to store all of this data  .  numpy can handle this through structure array , which be array with compound data type  .  recall that previously we create a simple array use an expression like this ,",
        "id": 374
    },
    {
        "code": "layer_fc1 = new_fc_layer(input=layer_flat,\n                         num_inputs=num_features,\n                         num_outputs=fc_size,\n                         use_relu=True)\nlayer_fc2 = new_fc_layer(input=layer_fc1,\n                         num_inputs=fc_size,\n                         num_outputs=num_classes,\n                         use_relu=False)",
        "text": "fully   -   connect layer add a fully   -   connect layer to the network  .  the input be the flatten layer from the previous convolution  .  the number of neuron or node in the fully   -   connect layer be fc _ size   .  relu be use so we can learn non   -   linear relation  . ",
        "id": 375
    },
    {
        "code": "multi = []\ntotal = 0\nfor val in range(0,1000):\n    if (val% 3 == 0) or (val % 5 == 0):\n        multi.append(val)         \n        total+= val\n        \nprint(total)",
        "text": "[ multiple of 3 and 5 ] ( <url> ) if we list all the natural number below 10 that be multiple of 3 or 5 , we get   3 , 5 , 6 and 9    .  the sum of these multiple be   23    .  find the sum of all the multiple of 3 or 5 below 1000  . ",
        "id": 376
    },
    {
        "code": "f = df.f.values\nx = df.x.values\ny = df.y.values\ndf.shape\nindexes=np.sort(np.random.choice(x.shape[0], size=60, replace=False))\nsamplex = x[indexes]\nsamplef = f[indexes]\nsampley = y[indexes]\nsample_df=pd.DataFrame(dict(x=x[indexes],f=f[indexes],y=y[indexes]))",
        "text": "here  x  and  y  be the predictor and measure response variable , and  f  be the true response  . ",
        "id": 377
    },
    {
        "code": "\nv[0]\nM[1,1]\nM[1]",
        "text": "index we can index element in an array use the square bracket and index ,",
        "id": 378
    },
    {
        "code": "t_start = time.time()\ntrain_data['review'] = train_data['review'].apply(text_preprocessing)\nt_end = time.time()\nprint(\"Time taken to process the reviews of train data: {:.2f} mins\".format((t_end-t_start)/60))\nt_start = time.time()\ntest_data['review'] = test_data['review'].apply(text_preprocessing)\nt_end = time.time()\nprint(\"Time taken to process the reviews of test data: {:.2f} mins\".format((t_end-t_start)/60))",
        "text": "preprocess review column of train and test data  - ",
        "id": 379
    },
    {
        "code": "fig, axes = plt.subplots(nrows=4, ncols=5)\nk,i=0,0\nfor e in bins:\n    sex_xt = pd.crosstab(df_train[e],df_train['target'])\n    sex_xt_norm = sex_xt.apply(lambda x : x/np.sum(sex_xt,axis=1))\n    sex_xt_norm.plot(kind='bar', stacked=True, \n                     ax=axes[k,i],figsize=(15,15),\n                     legend=False,ylim=[.8,1], \n                     sharex=False, sharey=True,)\n    i+=1\n    if i==5:\n        i=0\n        k+=1\n        \nplt.tight_layout()",
        "text": "percentage dist of target value by binary value",
        "id": 380
    },
    {
        "code": "array19 = np.zeros((8,8))\narray19[1::2,::2] = 1\narray19[::2,1::2] = 1\narray19",
        "text": "create a 8x8 matrix and fill it with a checkerboard pattern (    -  ) (   -  hint  -  , array\\ [ , ,2\\ ] )",
        "id": 381
    },
    {
        "code": "\ndfTRate['Rank'] = None \nrankS = 1\nrankW = 1\nfor x, row in dfTRate.iterrows():\n        \n    gameType = dfTRate['Summer'].iloc[x]\n    \n    if(gameType):\n        dfTRate.loc[x, 'Rank'] = rankS\n        rankS = rankS + 1\n    \n    else:\n        dfTRate.loc[x, 'Rank'] = rankW\n        rankW = rankW + 1\ndfTRate.to_csv( r\"..\\..\\data\\prep\\Games\\Games-TotalRank-800.csv\", index=False)",
        "text": "rank field for total rat now we ll create a rank field for the new dataframe which contain the total rat across all olympic game for each noc  .  the for loop below will be the same a the one above except rank will only be rest when we more to winter game from summer  . ",
        "id": 382
    },
    {
        "code": "month_counts = {}\nfor row in us_guns_data:\n    if row[2] in month_counts :\n        month_counts[row[2]] += 1\n    else:\n        month_counts[row[2]] = 1\nmonth_counts",
        "text": "count gun death by month",
        "id": 383
    },
    {
        "code": "layer_conv2, weights_conv2 = \\\n    new_conv_layer(input=layer_conv1,\n                   num_input_channels=num_filters1,\n                   filter_size=filter_size2,\n                   num_filters=num_filters2,\n                   use_pooling=True)",
        "text": "convolutional layer 2 and 3 create the second and third convolutional layer , which take a input the output from the first and second convolutional layer respectively  .  the number of input channel correspond to the number of filter in the previous convolutional layer  . ",
        "id": 384
    },
    {
        "code": "list_pos = []\nallFiles_pos = glob.glob('movie/pos/*.txt')\nfor file in allFiles_pos:\n    rfile = open(file, 'r')\n    list_pos.append(rfile.read())\ntrain_pos = pd.DataFrame(list_pos,columns=['message'])\ntrain_pos['label'] = 1\ndef fixMessage(row):\n    s = row['message'].replace(\"\\n\",\"\")\n    return s.replace(\"\\'\",\"\")\ntrain_pos['message'] = train_pos.apply(lambda r: fixMessage(r),axis=1)",
        "text": "load   -   explor positive review dataset",
        "id": 385
    },
    {
        "code": "ds_fret.fit_E_generic(fit_fun=bl.gaussian_fit_hist, bins=np.r_[-0.1:1.1:0.005], weights='size')\nE_kde_w = E_fitter.kde_max_pos[0]\nE_gauss_w = E_fitter.params.loc[0, 'center']\nE_gauss_w_sig = E_fitter.params.loc[0, 'sigma']\nE_gauss_w_err = float(E_gauss_w_sig/np.sqrt(ds_fret.num_bursts[0]))\nE_kde_w, E_gauss_w, E_gauss_w_sig, E_gauss_w_err",
        "text": "gaussian fit ( use burst size a weight ) ,",
        "id": 386
    },
    {
        "code": "\npathToFileInDisk = r'D:\\tmp\\3.jpg'\nwith open( pathToFileInDisk, 'rb' ) as f:\n    data = f.read()\nparams = { 'visualFeatures' : 'Color,Categories'} \nheaders = dict()\nheaders['Ocp-Apim-Subscription-Key'] = _key\nheaders['Content-Type'] = 'application/octet-stream'\njson = None\nresult = processRequest( json, data, headers, params )\nif result is not None:\n    data8uint = np.fromstring( data, np.uint8 ) \n    img = cv2.cvtColor( cv2.imdecode( data8uint, cv2.IMREAD_COLOR ), cv2.COLOR_BGR2RGB )\n    renderResultOnImage( result, img )\n    ig, ax = plt.subplots(figsize=(15, 20))\n    ax.imshow( img )",
        "text": "analysis of an image store on disk",
        "id": 387
    },
    {
        "code": "text_mtx.shape\nlen(female_text2)\nvocab = pd.DataFrame({'word':female_text2,'code':np.argmax(text_mtx,axis=1)})\nvocab=vocab.drop_duplicates()\nvocab.sort_values(by=\"code\")",
        "text": "each word will be represent by a vector of size 900 , and the the row will show 1 where the row word match the word column if it in the top 900 word  .  for that , we use text _ to _ matrix  . ",
        "id": 388
    },
    {
        "code": "def get_checksum(filename):\n    import hashlib\n    with open(filename, 'rb') as f:\n        md5 = hashlib.md5(f.read())\n    return md5.hexdigest()\nassert_equal(get_checksum('2001.csv.bz2'), 'e855ba7cc04f560199953833305a0f90')",
        "text": "in the follow code cell , we will calculate the [ checksum ] ( <url> ) of the file to check if you have the correct file  .  do n't worry if you do n't understand the code ( or what checksum be ) , just make sure that it doe n't produce any error  . ",
        "id": 389
    },
    {
        "code": "np.sum(np.nan_to_num(beh[0,:]-np.min(beh,axis=0)))\n_=plt.hist(np.nan_to_num(beh[0,:]-np.min(beh,axis=0)),100)\nplt.yscale('log')\nplt.xlim([0, 100])",
        "text": "diff between first and last point of loss history , all voxels",
        "id": 390
    },
    {
        "code": "def apply_threshold(heatmap, threshold):\n    heatmap[heatmap <= threshold] = 0\n    return heatmap\nheatmap_img =  apply_threshold(heatmap_img, 1)\nplt.figure(figsize=(10,10))\nplt.imshow(heatmap_img, cmap='hot')",
        "text": "apply a threshold on a heatmap image",
        "id": 391
    },
    {
        "code": "df\ndf.apply(np.cumsum)\ndf\ndf.max() \ndf.Ann.max()-df.Ann.min()\ndf.apply(lambda x: x.max() - x.min())",
        "text": "apply apply function to the data",
        "id": 392
    },
    {
        "code": "from __future__ import print_function\nimport time\nprint(' Last revision {}'.format(time.asctime()))",
        "text": "python course on class and functional program    -  j . a  .  hernando , usc , 2016  -  appendix   -   panda and matplotlib",
        "id": 393
    },
    {
        "code": "import numpy as np\nfrom numpy import linalg\nfrom abc import abstractmethod\nimport pandas as pd\nimport math\npd.options.display.float_format = '{:,.8f}'.format\nnp.set_printoptions(suppress=True, precision=8)\nTOR = pow(10.0, -6)\nMAX_ITR = 150\nclass NewtonMethod(object):\n    def __init__(self):\n        return\n    @abstractmethod\n    def f(self, x):\n        return NotImplementedError('Implement f()!')\n    @abstractmethod\n    def jacobian(self, x):\n        return NotImplementedError('Implement jacobian()!')\n\n    @abstractmethod\n    def run(self, x):\n        return NotImplementedError('Implement run()!')",
        "text": "the nonlinear system continuous with ex10 . 2 . 5",
        "id": 394
    },
    {
        "code": "def FormatThousands(x):\n    return x/1000.0\ncity_table['income_in_k'] =city_table['city_avg_incomes'].apply(FormatThousands)",
        "text": "let u do something similar for the income , but format the number in term of thousand  . ",
        "id": 395
    },
    {
        "code": "\nnew_data = json.load((open('data/world_bank_projects.json')))\ndf3 = json_normalize(new_data, 'mjtheme_namecode')\ndf3.head() \ndf3.groupby(['name']).size().sort_values(ascending=False).head(10)\ndf3.groupby(['code']).size().sort_values(ascending=False).head(10)\ndf3.info() #hmmm... all non-null values. Must be because we are importing empty strings",
        "text": "problem   -   2 , find the top 10 major project theme ( use column mjtheme _ namecode  )",
        "id": 396
    },
    {
        "code": "ZZ = YY ** 2\ndec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\nplt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\ncmap=mglearn.cm2, alpha=0.5)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=60, cmap=mglearn.cm2)\nplt.xlabel(\"feature1\")\nplt.ylabel(\"feature2\")",
        "text": "a a function of the original feature , the linear svm model be not actually linear anymore  .  it be not a line , but more of an ellipse  . ",
        "id": 397
    },
    {
        "code": "A\ndiag(A)\ndiag(A, -1)",
        "text": "diag with the diag function we can also extract the diagonal and subdiagonals of an array ,",
        "id": 398
    },
    {
        "code": "def create_pong_model():\n  model = tf.keras.models.Sequential([\n      \n      tf.keras.layers.InputLayer(input_shape=(6400,), dtype=tf.float32),\n      tf.keras.layers.Reshape((80, 80, 1)),\n      \n      \n      tf.keras.layers.Conv2D(filters=16, kernel_size=(8,8), strides=(4,4), activation='relu', padding='same'),\n      \n      tf.keras.layers.Conv2D('''TODO''', '''TODO''', '''TODO''', '''TODO''', padding='same'), \n      tf.keras.layers.Flatten(),\n      \n      \n      tf.keras.layers.Dense(units=256, activation='relu'),\n      \n      \n      tf.keras.layers.Dense('''TODO''', activation=None) \n  ])\n  return model\n\npong_model = create_pong_model()",
        "text": "define the agent we ll define our agent again , but this time , we ll add convolutional layer to the network to increase the learn capacity of our network  . ",
        "id": 399
    },
    {
        "code": "SIGMA      = np.sqrt(2.0/train_X.shape[0])\nn_features = train_X.shape[1]\nWeights = tf.Variable(tf.truncated_normal(shape=[n_features, n_labels], mean=3.0*SIGMA, stddev=SIGMA, dtype=tf.float32), trainable=True)\nBiases  = tf.Variable(tf.constant(value=SIGMA, dtype=tf.float32, shape=[n_labels]), trainable=True)\nt_dataset = tf.placeholder(tf.float32, shape=(None, n_features), name='t_dataset');\nt_labels  = tf.placeholder(tf.float32, shape=(None, n_labels), name='t_labels');",
        "text": "declare graph variable place to declare place holder and variable for compute graph",
        "id": 400
    },
    {
        "code": "def discriminator(x, n_units=128, reuse=False, alpha=0.01):\n    with tf.variable_scope('discriminator', reuse=reuse):\n        \n        h1 = tf.layers.dense(x, n_units, activation=None)\n        \n        h1 = tf.maximum(alpha * h1, h1)\n        logits = tf.layers.dense(h1, 1, activation=None)\n        out = tf.sigmoid(logits)\n        return out, logits\ninput_size = 784\nz_size = 100\ng_hidden_size = 128\nd_hidden_size = 128\nalpha = 0.01\nsmooth = 0.1",
        "text": "discriminator   -   the discriminator network be almost exactly the same a the generator network , except that we re use a sigmoid output layer  . ",
        "id": 401
    },
    {
        "code": "ten_highest_quality_country = hight_quality_per_country.sort_values(ascending=False)[:10]\ndf_10_highest_quality_country = pd.DataFrame({\"country\" : ten_highest_quality_country.index,\n                                                                    \"percentage\" :ten_highest_quality_country.values})\ndf_10_highest_quality_country",
        "text": "high   -   rank country in term of number of ga and fa   -   quality article a a proportion of all article about politician from that country",
        "id": 402
    },
    {
        "code": "\nprint (dfoasx.CDR[(dfoasx.CDR == 0.0)].count(),\",\", dfoasx.CDR[(dfoasx.CDR == 0.5)].count())\nprint (dfoasx.CDR[(dfoasx.CDR == 1.0)].count(),\",\", dfoasx.CDR[(dfoasx.CDR == 2)].count())",
        "text": "note categorical data for gender m/f , and label ( cdr ) vlues > =0 . 0 , and nan . cross   -   sectional mri dataset ha many row with nan value in multiple column  .  these nan value will be remove after merge the dataset with the longitudinal mri dataset  . ",
        "id": 403
    },
    {
        "code": "\nsns.set_style('whitegrid')\nsns.countplot(x='Survived', data= titanic, palette='RdBu_r')\nsns.countplot(x='Survived', hue='Sex', data= titanic, palette='RdBu_r')\nsns.countplot(x='Survived', hue='Pclass', data= titanic, palette='rainbow')\ntitanic.describe()\nsns.distplot(titanic['Age'].dropna(),color='darkred',bins=30)",
        "text": "roughly 20 percent of the age data be miss  .  the proportion of age miss be likely small enough for reasonable replacement with some form of imputation  .  look at the cabin column , it look like we be just miss too much of that data to do something useful with at a basic level  . ",
        "id": 404
    },
    {
        "code": "def dim_kde(arr, lb, ub, n=500, bw=0.25, k='epanechnikov'):\n    f = plt.figure()\n    xp = np.linspace(lb, ub, n)[:, np.newaxis]\n    kde = KernelDensity(kernel=k, bandwidth=bw).fit(arr[:, np.newaxis])\n    log_pdf = kde.score_samples(xp)\n    ind = np.argmax(np.exp(log_pdf))\n    print(np.exp(log_pdf)[ind],xp[ind])\n    ax = f.add_subplot(111)\n    ax.plot(xp, np.exp(log_pdf))",
        "text": "and define our function for plot the kernel density estimation for a particular dimension  . ",
        "id": 405
    },
    {
        "code": "lr = 0.001 \ncriterion = nn.BCELoss()\noptimizer_ae = torch.optim.Adam(params=autoencoder.parameters(), lr=lr)\noptimizer_d = torch.optim.Adam(params=discriminator.parameters(), lr=0.005)\noptimizer_g = torch.optim.Adam(params=encoder.parameters(), lr=lr)",
        "text": "we define the loss function and optimization scheme ( here   -  adam  -  ) for the autoencoder and the discriminator  . ",
        "id": 406
    },
    {
        "code": "y_test_pred = classifier.predict_labels(dists, k=7)\nnum_correct = np.sum(y_test_pred == y_test)\naccuracy = float(num_correct) / len(y_test)\nprint('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))",
        "text": "you should expect to see approximately 27 %  accuracy  .  now let try out a large  k  , say  k = 7  ,",
        "id": 407
    },
    {
        "code": "\nimport json\nwith open('important_words.json', 'r') as f: \n    important_words = json.load(f)\nimportant_words = [str(s) for s in important_words]\ndef remove_punctuation(text):\n    import string\n    return text.translate(None, string.punctuation) \nproducts['review_clean'] = products['review'].apply(remove_punctuation)\nfor word in important_words:\n    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))",
        "text": "we ll work with a hand   -   curated list of important word extract from the review data  .  we will also perform 2 simple data transformation , 1 .  remove punctuation use [ python 's build   -   in ] ( <url> ) string functionality  .  2 .  compute word count ( only for the   -  important _ words  -  )",
        "id": 408
    },
    {
        "code": "fig, ax = plt.subplots(figsize=(16,4))\npredictions = network.run(test_features).T\nax.plot(predictions[:,0], label='Prediction')\nax.plot(test_targets.values, label='Ground Truth')\nax.set_xlim(right=len(predictions))\nax.legend()",
        "text": "prediction on test data here , use the test data to view how well the network be model the data  .  if something be completely wrong here , make sure each step in your network be implement correctly  . ",
        "id": 409
    },
    {
        "code": "x = 3 \ny = x\nx = 10\nprint(\"x is\",x)\nprint (\"y is\",y)",
        "text": "what be the value of y after run these statement in order ? of x ? why ?",
        "id": 410
    },
    {
        "code": "\nimport collections\nprint('Regular dictionary:')\nd = {}\nd['a'] = 'A'\nd['b'] = 'B'\nd['c'] = 'C'\nd['d'] = 'D'\nd['e'] = 'E'\nfor k, v in d.items():\n    print(k, v)\nprint('\\nOrderedDict:')\nd = collections.OrderedDict()\nd['a'] = 'A'\nd['b'] = 'B'\nd['c'] = 'C'\nd['d'] = 'D'\nd['e'] = 'E'\n\nfor k, v in d.items():\n    print(k, v)",
        "text": "ordereddict   -   remember the order key be add to a dictionary an ordereddict be a dictionary subclass that remember the order in which it content be add  . ",
        "id": 411
    },
    {
        "code": "\nempirical_diff_means = female_mean_temp - male_mean_temp\nempirical_diff_means\nmean_temp = np.mean(df['Body temperature (degrees Fahrenheit)'])\ntemp_male_shifted = male_t - np.mean(male_t) + mean_temp\ntemp_female_shifted = female_t - np.mean(female_t) + mean_temp\nbs_replicates_male = draw_bs_reps(temp_male_shifted, np.mean, size=10000)\nbs_replicates_female = draw_bs_reps(temp_female_shifted, np.mean, size=10000)\nbs_replicates = bs_replicates_male - bs_replicates_female\n# Compute and print p-value: p\np = np.sum(bs_replicates >= empirical_diff_means) / len(bs_replicates)\nprint('p-value =', p)",
        "text": "a two   -   sample bootstrap hypothesis test for difference of mean",
        "id": 412
    },
    {
        "code": "ex = pd.DataFrame(data = {'word': vocab, 'number of appearance': dist}).sort_values(by = 'number of appearance', \n                                                                                    ascending = False).head(500)\nsns.set_style(\"whitegrid\")\nax = sns.barplot( x=\"number of appearance\", y=\"word\", data=ex)",
        "text": "we observe that some of the most represent word be relate to emotion (  great  ,  like  ,  good  ,  love  ,   -  which be important for our application )  .  we observe that the number of appearance be rapidly deacreasing  . ",
        "id": 413
    },
    {
        "code": "chosenM2_M1true = modelChoiceValidation.loc[(modelChoiceValidation['trueModel'] == 0) & (modelChoiceValidation['model2_BayesFactor'] > modelChoiceValidation['model1_BayesFactor']) & (modelChoiceValidation['chosenModel'] == 1)]\nchosenM2_M1true.head()\nsns.kdeplot(chosenM2_M1true['model2_BayesFactor'], label = 'M2 Bayes Factor when M1 true');\nsns.kdeplot(chosenM2_M3true['model2_BayesFactor'], label = 'M2 Bayes Factor when M3 true');\nplt.xlabel('M2 Bayes Factor');\nplt.axvline(float(modelFit['model2_BayesFactor']), color='black', label = 'model 2');",
        "text": "model 1 be the true model , but model 2 be choose a the best model with a large bay factor than model 1  . ",
        "id": 414
    },
    {
        "code": "graphlab.canvas.set_target('ipynb')\ncat=image_train[18:19] \ncat['image'].show()\nknn_model.query(cat)\ndef get_images_from_ids(query_result):\n    return image_train.filter_by(query_result['reference_label'],'id')\ncat_neighbors=get_images_from_ids(knn_model.query(cat))\ncat_neighbors['image'].show()",
        "text": "use the above knn model with deep feature for image retrieval to find similar image",
        "id": 415
    },
    {
        "code": "landing_page = []\nsessions = []\nwith open('task_data/Sessions_Page.json') as data_file:    \n    data = json.load(data_file)\nlist_of_dict = data['reports'][0]['data']['rows']\nfor dictionary in list_of_dict:\n    landing_page.append(dictionary['dimensions'][0])\n    sessions.append(int(dictionary['metrics'][0]['values'][0]))\nsessions_df = pd.DataFrame(data = {'page': landing_page, 'sessions': sessions})\nsessions_df.head()",
        "text": "work on second json file",
        "id": 416
    },
    {
        "code": "downsampled = img[::8,::8]\nimshow(downsampled, interpolation='nearest', vmin=0, vmax=255)",
        "text": "< div class=  btn   -   group  > < button class=  btn  onclick=  ipython . canopy _ exercise . toggle _ solution (  7  )  > solution",
        "id": 417
    },
    {
        "code": "sns.countplot(titanic['Pclass'], hue=titanic['Sex'])",
        "text": "male and female in pclass",
        "id": 418
    },
    {
        "code": "cfpbComplaintCbsaUaLl['target']=np.where(cfpbComplaintCbsaUaLl['respCode']== 0,1,0)\ncfpbComplaintCbsaUaLl['Consumer complaint narrative']= cfpbComplaintCbsaUaLl['Consumer complaint narrative'].fillna('')\ncfpbComplaintCbsaUaLl['State']= cfpbComplaintCbsaUaLl['State'].fillna('')\ncfpbComplaintCbsaUaLl['UA']=cfpbComplaintCbsaUaLl['UA'].astype('category')\ncatCols= ['Product','Issue','UA']\ndfDummies= pd.get_dummies(cfpbComplaintCbsaUaLl[catCols])\ndataStg= pd.concat([cfpbComplaintCbsaUaLl,dfDummies],axis=1)",
        "text": "text process below i take care of miss value and recode categorical value into binary",
        "id": 419
    },
    {
        "code": "import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n \nmod = ols('Rotten_Tomatoes_Score ~ Marvel_DC',\n                data=data).fit()\n                \naov_table = sm.stats.anova_lm(mod, typ=2)\nprint (aov_table)",
        "text": "one   -   way anova part 2",
        "id": 420
    },
    {
        "code": "ax = airquality.plot(column='1st Max Value',figsize=(24,24), markersize= 10)\nstates.plot(ax=ax, color='lightgrey', legend=True)",
        "text": "make a map of air quality station , color cod by bad air quality ever",
        "id": 421
    },
    {
        "code": "from sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(random_state=42)\nimport time\nt0 = time.time()\nrnd_clf.fit(X_train, y_train)\nt1 = time.time()\nprint(\"Training took {:.2f}s\".format(t1 - t0))\nfrom sklearn.metrics import accuracy_score\ny_pred = rnd_clf.predict(X_test)\naccuracy_score(y_test, y_pred)",
        "text": "exercise , train a random forest classifier on the dataset and time how long it take , then evaluate the result model on the test set   - ",
        "id": 422
    },
    {
        "code": "class Line(object):\n    def __init__(self,coor1,coor2):\n        self.coor1 = coor1\n        self.coor2 = coor2\n    def distance(self):\n        x1,y1 = self.coor1\n        x2,y2 = self.coor2\n        return ((x2-x1)**2 + (y2-y1)**2)**0.5\n    def slope(self):\n        x1,y1 = self.coor1\n        x2,y2 = self.coor2\n        return (y2-y1)/(x2-x1)\nli = Line((0,1), (3,7))\nli.distance()\nli.slope()",
        "text": "test1  .  fill in the line class method to accept coordinate a a pair of tuples and return the slope and distance of the line  . ",
        "id": 423
    },
    {
        "code": "\n[x ** 2 for x in nums]\n[x ** 2 for x in nums if x%2==0]",
        "text": "you can make this code simple use a list comprehension ,",
        "id": 424
    },
    {
        "code": "import pandas as pd\ndef load_housing_data(housing_path=HOUSING_PATH):\n    fetch_housing_data(HOUSING_URL,HOUSING_PATH)\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)",
        "text": "panda be use to load the data  .  the function below return a dataframe object contain all the data  . ",
        "id": 425
    },
    {
        "code": "\ncoeff_df = DataFrame(list(zip(X.columns, np.transpose(log_model.coef_))))\ncoeff_df",
        "text": "this mean that if our model just simply guess  no affair  we would have have 1   -   0 . 32=0 . 68 accuracy ( or 68 % ) accuracy  .  so while we be do good than the null error rate , we be n't do that much good  . ",
        "id": 426
    },
    {
        "code": "\nembeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\nnce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n                                               stddev=1.0 / np.sqrt(embedding_size)))\nnce_biases = tf.Variable(tf.zeros([vocabulary_size]))\nx_inputs = tf.placeholder(tf.int32, shape=[batch_size])\ny_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\nvalid_dataset = tf.constant(valid_examples, dtype=tf.int32)\nembed = tf.nn.embedding_lookup(embeddings, x_inputs)",
        "text": "next we define our model and placeholder  . ",
        "id": 427
    },
    {
        "code": "def new_convolutional_layer(conv_prev          \n                           ,num_input_channels \n                           ,filter_size        \n                           ,num_filters        \n                           ,use_pooling=True): \n    \n    shape = [filter_size, filter_size, num_input_channels, num_filters]\n    W = new_weights(shape)\n    b = biases(num_filters) \n    layer = tf.nn.conv2d(input=conv_prev\n                        ,filter=W\n                        ,strides=[1,1,1,1]\n                        ,padding='SAME')\n    layer += b\n    if use_pooling:\n        \n        layer = tf.nn.max_pool(value=layer\n                              ,ksize=[1,2,2,1]\n                              ,strides=[1,2,2,1]\n                              ,padding='SAME')\n    \n    layer = tf.nn.relu(layer)\n    return layer, W",
        "text": "helper function to create convolutional layer",
        "id": 428
    },
    {
        "code": "\nfig = plt.figure(figsize = (15,20))\nfor i,name in enumerate(skewed):\n    ax = fig.add_subplot(5,3,i+1)\n    ax.hist(features_trs[name],bins = 30)\n    ax.set_title(name)\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"bottom\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"left\"].set_visible(False)\nfig.tight_layout()\nplt.savefig('transformed.PNG')\nplt.show()",
        "text": "visualization of distribution of each feature after transformation",
        "id": 429
    },
    {
        "code": "\nre.findall(r'\\w+(?:\\s\\d{2}\\,|)(?=\\s\\d{4})', sample_data)\nprint(re.findall(r'\\b[a-z|A-Z]{4}\\b', sample_data))\nre.findall(r'(?<!born\\s)\\d{4}', sample_data)",
        "text": "part   -   4 1 .  find all the word which be precede by the year  .  for example in  bear 1970  bear be the word which be precede by 1970 2 .  find all the word which be 4 character long 3 .  find all the year which be not follow by the word bear",
        "id": 430
    },
    {
        "code": "from urllib.request import urlopen\nraw_html = urlopen(\"http://floatingmedia.com/columbia/topfivelists.html\").read()\nprint(raw_html) #the raw html output is now stored in a text string",
        "text": "import the urllib library and make http request",
        "id": 431
    },
    {
        "code": "labels = ['a','b','c']\nmy_list = [10,20,30]\narr = np.array([10,20,30])\nd = {'a':10,'b':20,'c':30}",
        "text": "create a series you can convert a list , numpy array , or dictionary to a series ,",
        "id": 432
    },
    {
        "code": "c1 = ROOT.TCanvas()\nROOT.gStyle.SetOptStat(0)\nROOT.gStyle.SetOptTitle(0)\nmorphing.GetXaxis().SetTitle(observable)\nmorphing.SetLineColor(2)\nmorphing.SetFillColor(2)\nmorphing.SetStats(0)\nmorphing.Draw(\"E3\")\nvalidation.Draw(\"SAME\")\nleg = ROOT.TLegend(0.7,0.7,0.9,0.9)\nleg.AddEntry(morphing)\nleg.AddEntry(validation)\nleg.Draw()\nc1.Draw()",
        "text": "print the pretty plot  .  we use the canvas create above and add legend in order to plot both histogram   -   the one obtain from the validation sample and the morph prediction for the same parameter set   -   for comparison  . ",
        "id": 433
    },
    {
        "code": "\nunique_items=len(heroes_df[\"Item ID\"].unique())\naverage_price=heroes_df[\"Price\"].mean()\ntotal_number_purchase=heroes_df[\"Price\"].count()\ntotal_revenue=heroes_df[\"Price\"].sum()\nsummary_table=pd.DataFrame({\"Number of Unique Items\":[unique_items],\n                           \"Average Purchase Price\":[average_price],\n                           \"Total Number of Purchases\":[total_number_purchase],\n                           \"Total Revenue\":[total_revenue]})\nsummary_table=summary_table[[\"Number of Unique Items\",\"Average Purchase Price\",\"Total Number of Purchases\",\"Total Revenue\"]]\nsummary_table=summary_table.round(2)\nsummary_table.head()",
        "text": "purchase analysis ( total ) number of unique item , average purchase price , total number of purchase , total revenue",
        "id": 434
    },
    {
        "code": "from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(train_x, train_y)\nconfusion_matrix(test_y, lr.predict(test_x))\n1 - lr.score(test_x, test_y)",
        "text": "perform logistic regression on the train data in order to predict mpg01  use the variable that seem most associate with mpg01  in ( 2 )  .  what be the test error of the model obtain ?",
        "id": 435
    },
    {
        "code": "psu = experiment_qc[experiment_qc['lab'] == 'Ross Hardison, PennState']\nother = experiment_qc[experiment_qc['lab'] != 'Ross Hardison, PennState']\npsu_fig = create_figure('MAD', 'Spearman', 'PSU')\nextra = setdefault_style()\npsu_fig.circle('MAD', 'Spearman', source=ColumnDataSource(psu), legend='Ross', color=Blues7[0], **extra)\npsu_fig.cross('MAD', 'Spearman', source=ColumnDataSource(other), legend='Other', color=Blues7[6], **extra)\nshow(psu_fig)\nbokeh.io.save(obj=psu_fig, \n              filename='/tmp/psu-mad-vs-spearman.html',\n              resources=bokeh.resources.CDN,\n              title='PSU MAD vs Spearman')",
        "text": "psu mad v spearman < a href=    -   index  > back to index",
        "id": 436
    },
    {
        "code": "magic_cards = pd.read_csv(\"MagicDatasets/Magic_Pandas_DF\")\nmagic_cards = magic_cards.drop(magic_cards.columns[0], axis=1)",
        "text": "data pull from kaggle dataset  . ",
        "id": 437
    },
    {
        "code": "array = np.random.randint(0,20,(30))\nprint(array.mean())",
        "text": "create a random vector of size 30 and find the mean value",
        "id": 438
    },
    {
        "code": "raw_acq['acquired_on'] = pd.to_datetime(raw_acq['acquired_on'])\nraw_acq.acquired_on.dtype",
        "text": "change acquired _ on  to datetime type",
        "id": 439
    },
    {
        "code": "vectorizer = CountVectorizer(min_df=best_min_df)\nX, y = make_xy(critics, vectorizer)\nxtrain=X[mask]\nytrain=y[mask]\nxtest=X[~mask]\nytest=y[~mask]\ntext=['This movie is not remarkable, touching, or superb in any way']\nclf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)\ntxt = vectorizer.transform(text)\nprint(clf.predict(txt))",
        "text": "exercise set vii , predict the freshness for a new review         exercise ,       use your best train classifier , predict the freshness of the follow sentence , *this movie be not remarkable , touch , or superb in any way*   be the result what you d expect ? why ( not ) ?",
        "id": 440
    },
    {
        "code": "le_sex = pp.LabelEncoder()\ny = le_sex.fit_transform(tc['sex'].astype(str))\ny\nle_child = pp.LabelEncoder()\ny1 = le_child.fit_transform(tc['child'].astype(str))\ny1\nX = tc.drop(['sex'],axis = 1)",
        "text": "preprocessing the sex and child column",
        "id": 441
    },
    {
        "code": "df_interacting_proteins_STRING = pd.concat([b,d,f])\ndf_interacting_proteins_STRING.reset_index(inplace=True)\ndf_interacting_proteins_STRING.drop(\"index\", inplace = True, axis = 1)\ndf_interacting_proteins_STRING.head()",
        "text": "we merge these three interations in a single dataframe call df _ interacting _ proteins _ string",
        "id": 442
    },
    {
        "code": "\nsns.countplot(x='Survived',data=train)\nsns.countplot(x='Survived',hue='Sex',data=train)\nsns.countplot(x='Survived',hue='Pclass',data=train,palette='rainbow')\ntrain['Age'].hist(bins=100,color='darkred',alpha=0.7)\ntrain['Fare'].hist(color='green',bins=40,figsize=(8,4))\ncorr = train.corr()\nsns.heatmap(corr)",
        "text": "roughly 20 percent of the age data be miss  .  the proportion of age miss be likely small enough for reasonable replacement with some form of imputation  .  look at the cabin column , it look like we be just miss too much of that data to do something useful with at a basic level  .  we ll probably drop this late  . ",
        "id": 443
    },
    {
        "code": "fig, ax = plt.subplots( nrows=1, ncols=1 ,figsize=(40, 9) )  \nsns.countplot(x=\"GENE\", hue=\"condition\",data=pysqldf('SELECT condition, EFFECT, GENE FROM varmult'));\nplt.xticks(rotation=70)",
        "text": "number of variant per gene per condition",
        "id": 444
    },
    {
        "code": "import datetime\ndates = [datetime.datetime(year = int(row[1]), month = int(row[2]), day = 1) for row in data]\nprint(dates[:5])\ndate_counts = {}\nfor date in dates:\n    if date not in date_counts:\n        date_counts[date] = 0\n        \n    date_counts[date] += 1\n        \ndate_counts",
        "text": "count gun death per month",
        "id": 445
    },
    {
        "code": "rng = pd.date_range('1/1/2012', periods=100, freq='S')\nts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)\nts.resample('5Min').sum()",
        "text": "time series panda ha simple , powerful , and efficient functionality for perform resampling operation during frequency conversion ( e . g .  , convert secondly data into 5   -   minutely data )  .  this be extremely common in , but not limit to , financial application  .  see the time series section",
        "id": 446
    },
    {
        "code": "labels = ['id', 'name', 'attendance', 'hw', 'test1', 'project1', 'test2', 'project2', 'final']\ndf = pd.read_csv('student_scores.csv', names=labels)\ndf.head()",
        "text": "you can also specify your own column label like this  . ",
        "id": 447
    },
    {
        "code": "(cast\n .loc[cast.n == 2]\n .loc[:'2020']\n .groupby(['year', 'type'])\n .size()\n .unstack()\n .fillna(0)\n .apply(lambda x: x['actor']/sum(x), axis = 1)\n .iplot(color='purple',\n        title = 'Distribution of Supporting Roles Between Actors and Actresses Across the Years',\n        xTitle='Year',\n        yTitle='Fraction of Supporting Roles'))",
        "text": "plot the fraction of support ( n=2 ) role that have be actor  role each year in the history of film  . ",
        "id": 448
    },
    {
        "code": "[string.upper() for string in strings]",
        "text": "with a list comprehension",
        "id": 449
    },
    {
        "code": "9*9\ndef f(x):\n    print(x * x)\nf(9)\nfrom ipywidgets import *\nfrom traitlets import link, observe\ninteract(f, x=(0, 100));",
        "text": "speed up the bottleneck in the repl < img src=  flow . svg  >",
        "id": 450
    },
    {
        "code": "x, y = h1.mean(), h1.var()\nX = np.linspace(np.min(x), np.max(x))\nlr = scipy.stats.linregress(x, y)\nplt.scatter(x, y, s=2, label='data', alpha=0.3)\nplt.plot(X, lr.intercept + lr.slope*X,\n         label='fit', lw=2, color='k', ls='--')\nplt.yscale('log')\nplt.legend()\nplt.xlabel('$\\mu$')\nplt.ylabel('$\\sigma$')\nprint('y = {0:.2g} + {1:.2g}x'.format(lr.intercept, lr.slope))",
        "text": "let 's plot $ \\sigma $ v $ \\mu $ for these healthy patient  . ",
        "id": 451
    },
    {
        "code": "for i in range(1, 101):\n    if i % 3 == 0 and i % 5 == 0:\n        print('FizzBuzz')\n    elif i % 3 == 0:\n        print ('Fizz')\n    elif i % 5 == 0:\n        print('Buzz')\n    else:\n        print(i)",
        "text": "write a program that print the integer from 1 to 100 .  but for multiple of three print  fizz  instead of the number , and for the multiple of five print  buzz   .  for number which be multiple of both three and five print  fizzbuzz    - ",
        "id": 452
    },
    {
        "code": "diaper_champ_reviews = products[products['name'] == 'Baby Trend Diaper Champ']\ndiaper_champ_reviews.head()\ndiaper_champ_reviews['predicted_sentiment'] = sentiment_model.predict(diaper_champ_reviews, output_type='probability')\ndiaper_champ_reviews = diaper_champ_reviews.sort('predicted_sentiment', ascending=False)\ndiaper_champ_reviews.head()\ndiaper_champ_reviews['predicted_sentiment'].max()\nselected_words_model.predict(diaper_champ_reviews[0:1], output_type='probability')\ndiaper_champ_reviews.head()\ndiaper_champ_reviews[0]['review']\ndiaper_champ_reviews[0]['word_count']",
        "text": "interpret the difference in performance between the model ,",
        "id": 453
    },
    {
        "code": "range(10,0,-1)",
        "text": "this can be use for descend list too , and observe that the final number b in range ( a , b , s ) be not include  . ",
        "id": 454
    },
    {
        "code": "\ndata_path = os.path.join('.', 'data')\ndataset_path = os.path.join(data_path, 'otmm_makam_recognition_dataset')\ndistribution_types = [\"pd\", \"pcd\"]\nstep_sizes = [7.5, 15.0, 25.0, 50.0, 100.0]\nkernel_widths = [0, 7.5, 15.0, 25.0, 50.0, 100.0]\nmodel_types = ['single', 'multi']\nannotations = json.load(open(os.path.join(dataset_path, 'annotations.json')))",
        "text": "set the dataset path and define the parameter",
        "id": 455
    },
    {
        "code": "from pyspark.sql.types import *\nv = 'v7'\ndf = sqlContext.createDataFrame([[[1, 'e']]], StructType([\n    StructField(\"nested\", \n                StructType([\n                        StructField(\"id\", LongType()), \n                        StructField(\"exists\", StringType())\n                    ])\n               )\n]))\ndf.write.parquet(dataset + v + partition + '1')\ndf = sqlContext.createDataFrame([[[1, 'e', 'a']]], StructType([\n    StructField(\"nested\", \n                StructType([\n                        StructField(\"id\", LongType()), \n                        StructField(\"exists\", StringType()),\n                        StructField(\"added\", StringType())\n                    ])\n               )\n]))\ndf.write.parquet(dataset + v + partition + '2')",
        "text": "nest row type   -   add a subcolumn",
        "id": 456
    },
    {
        "code": "\nfolder1= str(r\"C:/Users/u67397/AnacondaProjects/aem/input_data/\")\ndat1= \"1.dat\"\nreference_conductivity1 = 0.001\nreference_layers= 30",
        "text": "user requirement , enter detail for the first dataset",
        "id": 457
    },
    {
        "code": "def dwell_med(df, num_boots=5000, isPlot=False):\n    CI = np.array([0.0, 1.0])\n    if isPlot: \n        \n        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n    return CI",
        "text": "part g  -  , complete the function dwell _ med below to create an empirical bootstrapped confidence interval at the 95 % confidence level for the median dwell time  .  use at least 5000 bootstrap resamples  .  in addition to return the confidence interval , your function should also plot a histogram of the bootstrap resample median include some graphical indication of the 95 % confidence interval  . ",
        "id": 458
    },
    {
        "code": "class Node(object):\n    def __init__(self, inbound_nodes=[]):\n        \n        self.inbound_nodes = inbound_nodes\n        \n        self.outbound_nodes = []\n        \n        for n in self.inbound_nodes:\n            n.outbound_nodes.append(self)\n        \n        \n        self.value = None\n    def forward(self):\n        \n        raise NotImplemented",
        "text": "we know that each node might receive input from multiple other node  .  we also know that each node create a single output , which will likely be pass to other node  .  let 's add two list , one to store reference to the inbound node , and the other to store reference to the outbound node  . ",
        "id": 459
    },
    {
        "code": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\ngrid_values = {'max_depth' : [4, 6, 8, 10], 'n_estimators': [100, 50, 10, 5], \n               'max_features': ['sqrt', 'auto', 'log2'], 'bootstrap': [True, False]}\ngrid_clf_acc = GridSearchCV(RandomForestClassifier(random_state=0), param_grid = grid_values, cv=10, scoring = 'accuracy')\ngrid_clf_acc.fit(X_train, y_train)\nprint('Mean score matrix: ', grid_clf_acc.cv_results_['mean_test_score'])\nprint('Grid best parameter (max. accuracy): ', grid_clf_acc.best_params_)\nprint('Grid best score (accuracy): ', grid_clf_acc.best_score_)",
        "text": "random forest classifier with cross validation ( cv=10 ) and hyperparameter optimization",
        "id": 460
    },
    {
        "code": "from sklearn.pipeline import Pipeline\npreprocess_pipeline = Pipeline([\n    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n])\nX_train_transformed = preprocess_pipeline.fit_transform(X_train)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nlog_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\nscore = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\nscore.mean()",
        "text": "we be now ready to train our first spam classifier  .  let 's transform the whole dataset ,",
        "id": 461
    },
    {
        "code": "o=[30,462,523,632,968,970, 1298, 1324]\ntrain=train.drop(o,axis=0)\ntarget=target.drop(o,axis=0)\ntrain.index=range(0,train.shape[0])\ntarget.index=range(0,train.shape[0])",
        "text": "start the code , drop some outlier  .  the outlier be detect by package statsmodel in python , skip detail here  - ",
        "id": 462
    },
    {
        "code": "import numpy as np\nx = np.array([[1,2], [3,4]])\nprint(x)    \n            \nprint(x.T)  \n            \nv = np.array([1,2,3])\nprint(v)    \nprint(v.T)  # Prints \"[1 2 3]\"",
        "text": "you can find the full list of mathematical function provide by numpy in the [ documentation ] ( <url> )  .  apart from compute mathematical function use array , we frequently need to reshape or otherwise manipulate data in array  .  the simple example of this type of operation be transpose a matrix , to transpose a matrix , simply use the t attribute of an array object ,",
        "id": 463
    },
    {
        "code": "for k in students:\n    print(k, students[k])",
        "text": "print a dict key and value with a  for  loop",
        "id": 464
    },
    {
        "code": "\nsf_zipcodes['castro'] = 94114\nprint(sf_zipcodes)",
        "text": "add the castro district to the dictionary with zipcode 94114  . ",
        "id": 465
    },
    {
        "code": "\ncontent = Variable(image_preprocess(content_dir), requires_grad=False)\nstyle = Variable(image_preprocess(style_dir), requires_grad=False)\ngenerated = Variable(content.data.clone(),requires_grad=True)\nimshow(image_postprocess(image_preprocess(content_dir)))\nimshow(image_postprocess(image_preprocess(style_dir)))",
        "text": "train    -  1 ) prepare image",
        "id": 466
    },
    {
        "code": "hide_code\ncheckpointer = ModelCheckpoint(filepath='weights.best.model.hdf5', \n                               verbose=2, save_best_only=True)\nlr_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                 patience=5, verbose=2, factor=0.2)\nhide_code\nhistory = model.fit(x_train, y_train, \n                    epochs=epochs, batch_size=batch_size, verbose=2,\n                    validation_data=(x_valid, y_valid),\n                    callbacks=[checkpointer,lr_reduction])\nhide_code\n# TODO: Try to apply ImageDataGenerator (keras)",
        "text": "train the model",
        "id": 467
    },
    {
        "code": "temp = df[df['yearID']==1986]\ntemp['OBP'] = (temp['Hit']+temp['BB']+temp['IBB']+temp['SH']+temp['SF'])/temp['AB']\ntemp['PA'] = temp['AB']+temp['BB']+temp['IBB']+temp['SH']+temp['SF']\ntemp = temp[temp['PA']>=400]\ntemp.sort_values('OBP',ascending=False)[:1]",
        "text": "who have the high obp be 1986 with at least 400 pa ? ( dataframe )",
        "id": 468
    },
    {
        "code": "from  sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.40, random_state=42)\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=20, random_state=None, shuffle=True)\nkf",
        "text": "split the dataset into train   -   test",
        "id": 469
    },
    {
        "code": "data.payor_code.value_counts()\ndata.payor_code = [each.replace('?','Unknown') for each in data.payor_code]\ndata.payor_code.value_counts()\npayor_code_dummies = pd.get_dummies(data.payor_code, prefix = 'payor_code')\npayor_code_dummies.head()",
        "text": "payor _ code wa a cod categorical   -   it have many unknown and unclear code , but i could n't find an explanation , so i leave them   -  this feature might not even be useful , but i make dummy to test it",
        "id": 470
    },
    {
        "code": "image_size = 64\nnum_channels = 1\ndef reshape(dataset, labels):\n    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n    \n    labels = np.array(newarray).astype(np.float32)\n    return dataset, labels\ntrain_dataset, train_labels = reshape(train_dataset, train_labels)\nprint('Training set', train_dataset.shape, train_labels.shape)",
        "text": "neural network section     data array reshape",
        "id": 471
    },
    {
        "code": "def slope_func(state, t, system):\n    x, y, vx, vy = state\n    unpack(system)\n    a_grav = Vector(0, -g)\n    v = Vector(vx, vy)\n    f_drag = -rho * v.mag * v * C_d * area / 2\n    a_drag = f_drag / mass\n    a = a_grav + a_drag\n    return vx, vy, a.x, a.y",
        "text": "here 's the slope function that compute acceleration due to gravity and drag  . ",
        "id": 472
    },
    {
        "code": "from sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LinearRegression\nX_train, X_test, y_train, y_test = train_test_split(train_x_dum, train_y, test_size=0.20, random_state=0) #split set",
        "text": "split the train data into train and test  . ",
        "id": 473
    },
    {
        "code": "def entropy_on_split(dataframe, target, feature, debug=False):\n    \n    unique_classes = dataframe[feature].unique()    \n    target_subsets = [\n        target[dataframe[feature] == unique_class] \n        for unique_class in unique_classes\n    ]\n        \n    total = target.count()\n    weights = [\n        target_subset.count()/total\n        for target_subset in target_subsets\n    ]\n    \n    entropies = [\n        entropy(target_subset)\n        for target_subset in target_subsets\n    ]\n        \n    return sum(weight*entropy for weight, entropy in zip(weights, entropies))\nentropy_on_split(features, target, 'form')\nentropy_on_split(features, target, 'color')\nentropy_on_split(features, target, 'letter')",
        "text": "write a function to do this for a split on any feature",
        "id": 474
    },
    {
        "code": "def primefinder(x):\n    for i in range(2,x/2):\n        if x%i == 0:\n            return False\n    return True\nL = 0\ni = 1\ncounter = 0\nwhile counter<= 10:\n    if primefinder(i):\n        L = i\n        counter += 1\n        i += 1\n    i += 1\nprint('this is answer {0}'.format(L))\n# prime.prime(10000)",
        "text": "by list the first six prime number , 2 , 3 , 5 , 7 , 11 , and 13 , we can see that the 6th prime be 13 .  what be the 10 001st prime number ?",
        "id": 475
    },
    {
        "code": "df.loan_status.unique()\ndef target_handler(df):\n    if df['loan_status'] == 'Fully Paid':\n        return 1\n    elif df['loan_status'] in ('Default', 'Late (16-30 days)', 'Late (31-120 days)', 'Charged Off'):\n        return -1\n    elif df['loan_status'] in ('In Grace Period', 'Current','ssued'):\n        return 0\ndf['target'] = df.apply(target_handler, axis=1)\ndf['target'].value_counts(dropna=False)\ndf.dropna(subset=['target'], how='all', inplace=True)\ndf = df[(df['target'] == 1.0) | (df['target'] == -1.0)]\ndf['target'].value_counts(dropna=False)",
        "text": "feature engineer    -  process target column",
        "id": 476
    },
    {
        "code": "import osmnx as ox\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import csr_matrix\nfrom sklearn.cluster import DBSCAN\nox.config(use_cache=True, log_console=True)\neps = 300 \nminpts = 3 \nn_firms = 30\nn_clusters = 3\nplace = 'Piedmont, California, USA'\nG = ox.graph_from_place(place, network_type='drive', buffer_dist=500)",
        "text": "cluster point along a spatial network cluster a set of fake firm base on their network distance from each other  . ",
        "id": 477
    },
    {
        "code": "plt.plot([0, 1], [0, 1], sns.xkcd_rgb[\"pale red\"], lw=3)\nplt.plot([0, 1], [0, 2], sns.xkcd_rgb[\"medium green\"], lw=3)\nplt.plot([0, 1], [0, 3], sns.xkcd_rgb[\"denim blue\"], lw=3)",
        "text": "use name color from the xkcd color survey a while back , [ xkcd ] ( <url> / ) run a [ crowdsourced effort ] ( <url> / ) to name random rgb color  .  this produce a set of [ 954 name color ] ( <url> / ) , which you can now reference in seaborn use the  xkcd _ rgb  dictionary ,",
        "id": 478
    },
    {
        "code": "import re\ndef text_match(text):\n        patterns = 'ab?'\n        if re.search(patterns,  text):\n                return 'Found a match!'\n        else:\n                return('Not matched!')\nprint(text_match(\"ab\"))\nprint(text_match(\"abc\"))\nprint(text_match(\"abbc\"))\nprint(text_match(\"aabbc\"))",
        "text": "write a python program that match a string that ha an a follow by zero or one  b   . ",
        "id": 479
    },
    {
        "code": "\nimport time\ntime_format = '%Y-%m-%d %H:%M:%S'\ndef get_weekday(date_str):\n    time_obj = time.strptime(date_str,time_format)\n    return time_obj.tm_wday\ntraining_data_2['pickup_weekday'] = [get_weekday(date_str) for date_str in training_data_2['pickup_datetime']]\ndef get_hour(date_str):\n    time_obj = time.strptime(date_str,time_format)\n    return time_obj.tm_hour\ntraining_data_2['pickup_hour'] = [get_hour(date_str) for date_str in training_data_2['pickup_datetime']]\ndisplay(training_data_2.head())\nprint('Training data shape: ', training_data_2.shape)",
        "text": "multiple data column can be derive from pick up date time  .  1 .  week day for pick up 2 .  hour of the day for pick up",
        "id": 480
    },
    {
        "code": "gmh = oo[(oo.Gender == 'Men') & (oo.Medal == 'Gold') & (oo.Event == '100m')]\ngmh.sort_values('Edition',ascending=False)[['City','Edition','Athlete','NOC']]\noo.Event.value_counts()",
        "text": "display the male gold medal winner for the 100m track   -   field sprint event over the year  .  list the result start with the most recent  .  show the olympic city , edition , athlete and the country they represent  . ",
        "id": 481
    },
    {
        "code": "\nprices_noheader = prices[1:]\nprices_noheader\nfor price in prices_noheader:\n    print(int(float(price)), ' ')\nfor price in prices_noheader:\n    if not price == '':\n        print(int(float(price)))\n    else:\n        print('None')\ndef extract_int_price(price):\n    if not price == '':\n        return int(float(price))\n    else:\n        return None\nfor price in prices_noheader:\n    print(extract_int_price(price))\n\nint_prices = []\nfor price in prices_noheader:\n    int_prices.append(extract_int_price(price))\nprint(int_prices)",
        "text": "this list ha a couple of problem  .  first , it include the header  .  second , it 's all string even though price be numeric data  .  third , it contain some empty string  .  we ll have to clean it up  . ",
        "id": 482
    },
    {
        "code": "med_abs_devs = abs(mtcars['mpg']-mtcars['mpg'].median())\nmed_abs_devs.median() * 1.4826  # 各绝对差还要取中位数；Note: The MAD is often multiplied by a scaling factor of 1.4826.",
        "text": "绝对中位差 since variance and standard deviation be both derive from the mean , they be susceptible to the influence of data skew and outlier  .  median absolute deviation be an alternative measure of spread base on the median , which inherit the median 's robustness against the influence of skew and outlier  .  it be the median of the absolute value of the deviation from the median ,",
        "id": 483
    },
    {
        "code": "hashingTF = HashingTF()\ntestcsv_data = sc.textFile('./data/test.csv', use_unicode=True).map(get_cleaned_tweet)\n(acc_val, actual, predication), model = naiveBayes([], testcsv_data, naiveBayesModel)",
        "text": "< font color = black  > test the model with the test data",
        "id": 484
    },
    {
        "code": "print(optimize.fmin(f, -2))\nprint(optimize.fmin(f, 2))",
        "text": "calculate numerically the two minimum of the function $ f   -  for which value of $ x $ do we have a minimum ?",
        "id": 485
    },
    {
        "code": "jointsx01_downloaded = drive.CreateFile({'id': '1lFcHcuYTIv4zZbh2UhBEFWAlJq9XxGSN'})\njointsx01_downloaded.GetContentFile('jointsx01.csv')\ndf_jointsx01 = pd.read_csv('jointsx01.csv')\ndf_jointsx01.rename(index=str, columns={\"ID\": \"id\"}, inplace = True)\ndf_jointsx01.head()",
        "text": "import jointsx01 dataset into panda dataframe  . ",
        "id": 486
    },
    {
        "code": "a = input(\"enter a\")\nb = input(\"enter b\")\nc = input(\"enter c\")\nif (a>b):\n    if(a>c):\n        print(a)\n    else:\n        print(c)\nelif (b>c):\n    print(b)",
        "text": "write a program to print large of three number  .  take three number from user use input ( ) function  - ",
        "id": 487
    },
    {
        "code": "f = df.f.values\nx = df.x.values\ny = df.y.values\ndf.shape\nindexes=np.sort(np.random.choice(x.shape[0], size=60, replace=False))\nsamplex = x[indexes]\nsamplef = f[indexes]\nsampley = y[indexes]\nsample_df=pd.DataFrame(dict(x=x[indexes],f=f[indexes],y=y[indexes]))\nsample_df.info()\nsample_df.head()",
        "text": "here  x  and  y  be the predictor and measure response variable , and  f  be the true response  . ",
        "id": 488
    },
    {
        "code": "\ntext = str(rejected['Loan Title'])\nwc = WordCloud(font_path='/Library/Fonts/Verdana.ttf',\n               relative_scaling = 1.0,\n               stopwords=STOPWORDS,\n               max_font_size=30000).generate(text)\nplt.figure()\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show();",
        "text": "loan title word cloud by frequency",
        "id": 489
    },
    {
        "code": "\ndef divide_dataset(dataset, labels):\n    train_pct = 0.8; \n    train_index = int(dataset.shape[0]*train_pct)\n    t_X = dataset[:train_index, :]\n    t_Y = labels[:train_index,:]\n    v_X = dataset[train_index:,:]\n    v_Y = labels[train_index:,:]\n    return t_X, t_Y, v_X, v_Y\ntrain_X, train_Y, valid_X, valid_Y = divide_dataset(dataset, labels)\nprint(train_X.shape)",
        "text": "divide dataset into train dataset and validation dataset",
        "id": 490
    },
    {
        "code": "def add_and_maybe_multiply(a, b, c=None):\n    result = a + b\n    if c is not None:\n        result = result * c\n    return result",
        "text": "none be also a common default value for optional function argument ,",
        "id": 491
    },
    {
        "code": "evidence_list = []\npositive_yes_objIndex = np.argmax(prob_estimates[:,1])\nget_Information(X_test.iloc[positive_yes_objIndex],X_testS[positive_yes_objIndex],positive_yes_objIndex)",
        "text": "the most positive object with respect to the probabilities . ",
        "id": 492
    },
    {
        "code": "l = [1,2,3]\nm = [4,5,6]\nn = [7,8,9]\nmap(lambda x,y : x+y, l, m)\nmap(lambda x,y,z : x+y+z, l, m, n)\nmap(lambda num : num**2, l)",
        "text": "map and lambda use together",
        "id": 493
    },
    {
        "code": "my_list + [\"new item\"]\nmy_list = my_list + [\"new item\"]\nmy_list\nmy_list * 2",
        "text": "we can also use + to concatenate list , just like string",
        "id": 494
    },
    {
        "code": "c = cast\nc = c.groupby(['year', 'type']).size()\nc = c.unstack('type')\nc.plot(kind='area')",
        "text": "< div class=  alert alert   -   success  >   exercise   , plot the number of actor role each year and the number of actress role each year , but this time a a kind=area  plot  . ",
        "id": 495
    },
    {
        "code": "X_test, non_var = read_data(test, is_train=False)\ndummy_y = np.array(range(len(X_test)))\nmodel.train(False)\ny_pred = []\nfor X_batch, y_batch in iterate_minibatches(X_test, dummy_y, batch_size, shuffle=False):\n    X_batch = Variable(torch.FloatTensor(X_batch)).cuda(0)\n    y_pred.extend(model(X_batch).data.cpu().numpy())\ny_predicted = np.asarray(y_pred)",
        "text": "read test data , fee it to neural network , and save the output in kaggle fromat  . ",
        "id": 496
    },
    {
        "code": "x = np.array([[1,2],[3,4]], dtype=np.float64)\ny = np.array([[5,6],[7,8]], dtype=np.float64)\nprint (x + y)\nprint (np.add(x, y))\nprint (x - y)\nprint (np.subtract(x, y))\nprint (x * y)\nprint (np.multiply(x, y))\nprint (x / y)\nprint (np.divide(x, y))\nprint (np.sqrt(x))",
        "text": "array math basic mathematical function operate elementwise on array , and be available both a operator overload and a function in the numpy module ,",
        "id": 497
    },
    {
        "code": "rmse_cv(model_lasso).mean()\ncoef = pd.Series(model_lasso.coef_, index = X_train.columns)\nprint(\"O modelo Lasso escolheu \" + str(sum(coef != 0)) + \" atributos e eliminou os outros \" +  str(sum(coef == 0)) + \" atributos do dataset para construção do modelo.\")\nimp_coef = pd.concat([coef.sort_values().head(2),\n                     coef.sort_values().tail(3)])\nmatplotlib.rcParams['figure.figsize'] = (4.0, 5.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")\nplt.xlabel(\"ALPHA\")\nplt.ylabel(\"RMSE\")",
        "text": "on the second model lasso , it be find a mean rmse of 0 . 6563  . ",
        "id": 498
    },
    {
        "code": "\nnewdf = df.copy()\nState = newdf.groupby('State')\nnewdf['Lower'] = State['Revenue'].transform( lambda x: x.quantile(q=.25) - (1.5*(x.quantile(q=.75)-x.quantile(q=.25))) )\nnewdf['Upper'] = State['Revenue'].transform( lambda x: x.quantile(q=.75) + (1.5*(x.quantile(q=.75)-x.quantile(q=.25))) )\nnewdf['Outlier'] = (newdf['Revenue'] < newdf['Lower']) | (newdf['Revenue'] > newdf['Upper']) \nnewdf",
        "text": "assumign a non gaussian distribution ( if you plot it , it will not look like a normal distribution )",
        "id": 499
    },
    {
        "code": "pd.tools.plotting.scatter_matrix(df_female_condensed[ ['attr_iMeasUp_1', 'attr_o_ave'] ], diagonal = 'kde', s = 500, figsize = (8, 8))\ndf_female_condensed[ ['attr_oPercveMe_1', 'attr_o_ave'] ].plot(kind = 'hist', figsize = (8, 8))",
        "text": "observation , woman view themselves a more attractive than men see them a , woman view men a le attractive on average than woman",
        "id": 500
    },
    {
        "code": "import pandas as pd\ns=pd.read_csv('./searches.sample.csv.bz2', sep='^', usecols=['Date','Destination'],nrows=500)\ns=s.dropna()\nsFilter=s[(s['Destination']=='MAD')|(s['Destination']=='BCN')|(s['Destination']=='AGP')]\nsFilter=s[s['Destination'].isin(['MAD', 'BCN', 'AGP'])]\nsFilter.head()",
        "text": "filter out the the search for mad , bcn , agp take only the row have mad , bcn or agp a destination",
        "id": 501
    },
    {
        "code": "numbers = range(1, 11)\nsq_even_numbers = [n**2 for n in numbers if n%2==0]\nprint(sq_even_numbers)",
        "text": "square of even number in a range",
        "id": 502
    },
    {
        "code": "\ncpu[:10][['MMIN', 'MMAX']].values",
        "text": "a very common scenario will be the follow  .  we want to select specific observation and column of a dataframe and convert to a numpy array so that we can use it for feature extraction , classification etc  .  this can be achieve by use the values  method  . ",
        "id": 503
    },
    {
        "code": "num_fruit = 4\nprint('I have {0} fruits'.format(num_fruit))",
        "text": "error what happen when we make a typo and try to write something that doe n't exist ?",
        "id": 504
    },
    {
        "code": "def tokenize(text_str):\n    \n    return tokens\ntok = tokenize(monty)\nassert_is_instance(tok,list)\nassert_true(all(isinstance(t, str) for t in tok))\nassert_equal(len(tok), 16450)\nassert_equal(tok[:10], ['SCENE', '1', ':', '[', 'wind', ']', '[', 'clop', 'clop', 'clop'])\nassert_equal(tok[51:55], ['King', 'of', 'the', 'Britons'])\nassert_equal(tok[507:511], ['African', 'swallows', 'are', 'non-migratory'])",
        "text": "tokenize in this function , you will tokenize the give input text  .  the function word _ tokenize ( ) might prove helpful in this instance  . ",
        "id": 505
    },
    {
        "code": "complaints = data[['Created Date', 'Complaint Type']]\ncomplaints.head()\nnoise_complaints = complaints[complaints['Complaint Type'] == 'Noise - Street/Sidewalk']\nnoise_complaints.head()\nnoise_complaints.set_index('Created Date').sort_index().resample('H').count().plot()",
        "text": "graph the number of noise complaint each hour in new york",
        "id": 506
    },
    {
        "code": "print(TP / (TP + FN))\nprint(metrics.recall_score(y_test, y_pred))",
        "text": "sensitivity , when the actual value be positive , how often be the prediction correct ? how  sensitive  be the classifier to detect positive instance ? also know a  true positive rate  or recall",
        "id": 507
    },
    {
        "code": "\nfrom Bio.SeqUtils import GC\ndef gc_content_function(string_nucleotides):\n    gc_content = GC(string_nucleotides)\n    return(gc_content)\ngc_content_function(\"AGTGCTTTTTCGATCGAGATA\")",
        "text": "create a function that calculate the gc   -   content of the dna sequence  .  the input be a amino acid sequence string and output the molecular weight of the sequence use biopython  . ",
        "id": 508
    },
    {
        "code": "lm2.summary()\nbblnrgdata_cut3.head()\na = np.sum(((bblnrgdata_cut3.logUnits - bblnrgdata_cut3.pred2) ** 2) / bblnrgdata_cut3.logUnits)\nprint ('Chi Square value for energy v unit model is', a)\nscipy.stats.chisquare(f_obs=bblnrgdata_cut3.logUnits, f_exp=bblnrgdata_cut3.pred2)",
        "text": "caption , line fit model doe not seem the best fit for energy v unit",
        "id": 509
    },
    {
        "code": "def create_style_loss(session,model,style_image,layer_ids):\n    feed_dict = model.create_feed_dict(image=style_image)\n    layers = model.get_layer_tensors(layer_ids)\n    with model.graph.as_default():\n        gram_layers = [gram_matrix(layer) for layer in layers]\n        values = session.run(gram_layers, feed_dict=feed_dict)\n        layer_losses=[]\n        for value,gram_layer in zip(values,gram_layers):\n            value_const = tf.constant(value)\n            loss = mean_squared_error(gram_layer,value_const)\n            layer_losses.append(loss)\n        total_loss = tf.reduce_mean(layer_losses)\n    return total_loss",
        "text": "the next function create the loss   -   function for the style   -   image  .  it be quite similar to create _ content _ loss ( ) above , except that we calculate the mean square error for the gram   -   matrix instead of the raw tensor   -   output from the layer  . ",
        "id": 510
    },
    {
        "code": "\ndfATRate['Ath_Rank'] = None \nrankS = 1\nrankW = 1\nfor x, row in dfATRate.iterrows():\n        \n    gameType = dfATRate['Summer'].iloc[x]\n    \n    if(gameType):\n        dfATRate.loc[x, 'Ath_Rank'] = rankS\n        rankS = rankS + 1\n    \n    else:\n        dfATRate.loc[x, 'Rank'] = rankW\n        rankW = rankW + 1\ndfATRate.to_csv( r\"..\\..\\data\\prep\\Games\\Games-TotalRank-900.csv\", index=False)\ndfARate.to_csv( r\"..\\..\\data\\prep\\Games\\Games-900.csv\", index=False)",
        "text": "rank field for total rat now we ll create a rank field for the new dataframe which contain the total rat across all olympic game for each athlete  .  the for loop below will be the same a the one above except rank will only be reset when we more to winter game from summer  . ",
        "id": 511
    },
    {
        "code": "\ncountry_names = medals['NOC']\nmedal_counts = country_names.value_counts()\nmedal_counts.head(10)",
        "text": "rank the top 10 country by total number of medal",
        "id": 512
    },
    {
        "code": "plt.style.use('ggplot')\nbillion.plot(kind='scatter', x='age', y='networthusbillion',xlim=(-50,100),ylim=(0,80))",
        "text": "maybe plot their net worth v age ( scatterplot )",
        "id": 513
    },
    {
        "code": "messages = pd.read_sql_query(\"\"\"\nSELECT \n  Score, \n  Summary,\n  Text,\n  HelpfulnessNumerator as VotesHelpful, \n  HelpfulnessDenominator as VotesTotal\nFROM Reviews \nWHERE Score != 3\"\"\", con)",
        "text": "let 's select only what 's of interest to u ,",
        "id": 514
    },
    {
        "code": "plt.figure(figsize=(10,6))\nsns.countplot(x='purpose',hue='not.fully.paid',data=df, palette='Set1')\nplt.title(\"Bar chart of loan purpose colored by not fully paid status\", fontsize=17)\nplt.xlabel(\"Purpose\", fontsize=15)",
        "text": "countplot of loan by purpose , with the color hue define by not . fully . paid",
        "id": 515
    },
    {
        "code": "\nX_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\nX_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\nX_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\nprint('Train data shape: ', X_train.shape)\nprint('Validation data shape: ', X_val.shape)\nprint('Test data shape: ', X_test.shape)",
        "text": "append the bias dimension of one",
        "id": 516
    },
    {
        "code": "(df['NA_Sales'].head(1) - df['NA_Sales'].mean()) / df['NA_Sales'].std()",
        "text": "no  .  6 for the top   -   sell game of all time , how many standard deviation above/below the mean be it sale for north america ?    -  answer ,",
        "id": 517
    },
    {
        "code": "ppl_train = (dtst >> ppl_train_template).run()",
        "text": "now we link this template to the dataset and run ,",
        "id": 518
    },
    {
        "code": "def lastPosition(A, target):\n    if A is None or len(A) == 0:\n        return -1\n    start, end = 0, len(A) - 1\n    while start + 1 < end:\n        mid = start + (end - start) / 2\n        if A[mid] > target:\n            end = mid\n        else:\n            start = mid\n    \n    \n    if A[end] == target:\n        return end\n    if A[start] == target:\n        return start\n    return -1",
        "text": "no . 17 last position of target find the last position of a target number in a sort array  .  return   -   1 if target doe not exist  .  example give [ 1 , 2 , 2 , 4 , 5 , 5 ]  .  for target = 2 , return 2 .  for target = 5 , return 5 .  for target = 6 , return   -   1  . ",
        "id": 519
    },
    {
        "code": "personalized_model.recommend(users=['18fafad477f9d72ff86f7d0bd838a6573de0f64a'])\npersonalized_model.recommend(users=[users[1]])",
        "text": "apply the personalize model to make song recommendation a you can see , different user get different recommendation now  .  what doe  personalize  mean ?",
        "id": 520
    },
    {
        "code": "df.groupby('key1')['data1'].mean()\ndf.groupby('key1')[['data1']].mean()\ndf[['data1']].groupby(df['key1']).mean()\ndf['data1'].groupby(df['key1']).mean()",
        "text": "index a groupby object create from a dataframe with a column name or array of column name ha the effect of select those column for aggregation",
        "id": 521
    },
    {
        "code": "pd.set_option('display.max_columns', 100)\ndf.head()\npd.set_option('display.max_rows', 100)\ndf.shape\ndf.describe()",
        "text": "eda , take a high   -   level overview of the data",
        "id": 522
    },
    {
        "code": "a = slice(5, 50, 2)\ns = 'HelloWorld'\na.indices(len(s))\nfor i in range(*a.indices(len(s))):\n    print(s[i])",
        "text": "in addition , you can map a slice onto a sequence of a specific size by use it index ( size ) method  .  this return a tuple ( start , stop , step ) where all value have be suitably limit to fit within bind ( a to avoid indexerror exception when index )  .  for example ,",
        "id": 523
    },
    {
        "code": "for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ntrain_df.head()",
        "text": "convert categorical feature to numeric we can now convert the embarkedfill  feature by create a new numeric port feature  . ",
        "id": 524
    },
    {
        "code": "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(18,7))\nax1.scatter(bwlen[60:-80], bres_interp, s=1)\nax2.scatter(rwlen[60:-80], rres_interp, s=1)\nax1.set_title('Blue side row size (20 plates)')\nax1.set_xlabel('Wavelength [Angstrom]')\nax1.set_ylabel('Pixel size [Angstrom/Pixel]')\nax2.set_title('Red side row size (20 plates)')\nax2.set_xlabel('Wavelength [Angstrom]')\nplt.tight_layout()\nplt.savefig('resolution.png')",
        "text": "median of interpolate resolution value on wavelength grid",
        "id": 525
    },
    {
        "code": "from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1e5)\nlogreg_result = logreg.fit(X_train_tfidf, y_train)\nprediction['Logistic'] = logreg.predict(X_test_tfidf)",
        "text": "create a logistic regression model",
        "id": 526
    },
    {
        "code": "men_target_df = women_men[women_men['gender'] == 1]['dec'].copy()\npca = PCA(n_components = 3)\npca.fit(X = men_new_input_df, y = men_target_df)\ntransformed_new_input_df = pca.fit_transform(X = men_new_input_df, y = men_target_df)\ntransformed_pca = pd.DataFrame(transformed_new_input_df, columns = ['x_s', 'y_s', 'z_s'])\nfc.pca_plotter(transformed_pca, men_target_df)",
        "text": "pca , projection to 3 dimension ( male )",
        "id": 527
    },
    {
        "code": "print(titanic[\"Embarked\"].unique())\ntitanic[\"Embarked\"] = titanic[\"Embarked\"].fillna('S')\ntitanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntitanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntitanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2",
        "text": "convert the embark column we now can convert the embark column to code the same way we convert the sex column  .  the unique value in embark be s , c , q , and miss ( nan )  .  each letter be an abbreviation of an embarkation port name  . ",
        "id": 528
    },
    {
        "code": "import statsmodels.api as sm\nfrom sklearn.metrics import mean_absolute_error\nmodel = sm.tsa.ARIMA(train, (1, \n                             0, \n                             0) \n                    ).fit()\npredictions = model.predict(\n    '2012-02-26',\n    '2012-10-28',\n    dynamic=True, \n)\nprint(\"Mean absolute error: \", mean_absolute_error(test, predictions))\nmodel.summary()",
        "text": "create an ar ( 1 ) model on the train data and compute the mean absolute error of the prediction  . ",
        "id": 529
    },
    {
        "code": "import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\nchurn_df = pd.read_csv('data/churn.all')\nchurn_df.head()\nprint(\"Num of rows: \" + str(churn_df.shape[0]))  \nprint(\"Num of columns: \" + str(churn_df.shape[1])) # count columns",
        "text": "part 1 . 1 , understand the raw dataset     data source , <url>   data info , <url>",
        "id": 530
    },
    {
        "code": "import numpy\nx = numpy. array([2, 4, 6, 8])\nx[0]\nx[1:3]\nx = numpy.linspace(0, 100, 26, dtype=int)\nprint(x)\nx[3:12]\ny = numpy.arange(0,100.0,4)\nprint(y)\ntype(x)\ntype(y[0])\ntype(x[0])",
        "text": "slice on numpy array array be the wssential data structure in numerical compute  .  they be available to you via the numpy library  .  load the library like this ,",
        "id": 531
    },
    {
        "code": "vcov_r = s2_r * np.linalg.inv(X2.T.dot(X2))\nvcov_r.round(2)",
        "text": "ols variance   -   covariance matrix of coefficient",
        "id": 532
    },
    {
        "code": "import random\nrandom.seed(123)\nparameters = [random.randint(100,1000) for i in range(10)]",
        "text": "question parameter generate parameter , that appear in the question  .  we use seed for random generator  . ",
        "id": 533
    },
    {
        "code": "\nmodel['src'][2] = 10\nmodel['rec'][2] = 50\nsstarget = epm.dipole(res=rtg, **model)\nssnotarg = epm.dipole(res=rhs, **model)",
        "text": "calculate shallow model , shallow src/rec   -   water depth of 100 m   -   source depth 10 m   -   receiver depth 50 m",
        "id": 534
    },
    {
        "code": "\nlevel2B_file = input_basename + '_cal.fits'\nspec_file = input_basename + '_x1d.fits'",
        "text": "check of output the level 2b pipeline for the lr produce two file ,  _ cal . fits   -   the calibrate level 2b file  _ x1d . fits   -   the 1d spectrum we can have a look at each of these",
        "id": 535
    },
    {
        "code": "x, y = next(dl)\ny_pred = net2.forward(Variable(x).cuda())\nloss = loss_fn(y_pred, Variable(y).cuda())\nprint(loss.data)",
        "text": "now , let 's make another set of prediction and check if our loss be low ,",
        "id": 536
    },
    {
        "code": "train = bike_rentals.sample(frac=.8)\ntest = bike_rentals.loc[~bike_rentals.index.isin(train.index)]\nfrom sklearn.linear_model import LinearRegression\npredictors = list(train.columns)\npredictors.remove(\"cnt\")\npredictors.remove(\"casual\")\npredictors.remove(\"registered\")\npredictors.remove(\"dteday\")\nreg = LinearRegression()\nreg.fit(train[predictors], train[\"cnt\"])\nimport numpy\npredictions = reg.predict(test[predictors])\nnumpy.mean((predictions - test[\"cnt\"]) ** 2)\nactual\ntest[\"cnt\"]",
        "text": "error metric the mean square error metric make the most sense to evaluate our error  .  mse work on continuous numeric data , which fit our data quite well  . ",
        "id": 537
    },
    {
        "code": "for index_ in house_prices_df.index:\n    if pd.isnull(house_prices_df.loc[index_,\"BsmtFinType2\"]):\n        if house_prices_df.loc[index_,\"BsmtFinSF2\"] > house_prices_df.loc[index_,\"BsmtUnfSF\"]:\n            house_prices_df.loc[index_,\"BsmtFinType2\"] = \"Low Quality\"\n        elif house_prices_df.loc[index_,\"BsmtFinSF2\"] < house_prices_df.loc[index_,\"BsmtUnfSF\"]:\n            house_prices_df.loc[index_,\"BsmtFinType2\"] = \"Unfinshed\"\n        else:\n            house_prices_df.loc[index_,\"BsmtFinType2\"] = \"No Basement\"",
        "text": "handle misisng value in bsmtfintype2 na    -  > no basement",
        "id": 538
    },
    {
        "code": "subplot(1,2,1)\nplot(x, y, 'r--')\ntitle('title1')\nsubplot(1,2,2)\nplot(y, x, 'g*-')\ntitle('title2');",
        "text": "most of the plot relate function in matlab be cover by the pylab  module ( and by matplotlib . pyplot  )  .  for example , subplot and color/symbol selection ,",
        "id": 539
    },
    {
        "code": "pd_mnist_testing = mnist_testing.as_data_frame()\npd_mnist_sample = pd_mnist_testing.sample(n=num_ref_images)\npd_mnist_sample",
        "text": "sample specify number of image from the test data set  . ",
        "id": 540
    },
    {
        "code": "test = [\"BTER/PPCBTC\", \"BTER/DOGEBTC\",\"BTER/NMCBTC\",\"BTER/FTCBTC\",\"BTER/ETHBTC\",\n       \"BTER/BLKBTC\",\"BTER/NXTBTC\",\"BTER/QRKBTC\",\"BTER/NBTBTC\", \"BTER/BTSBTC\",\n       \"BTER/XCPBTC\", \"BTER/XMRBTC\"]\nfor crypto in test:\n    response = quandl.get(crypto, authtoken=\"6qwbDx25mja4FhBpTfRo\")[\"Average\"]\n    plt.plot(response)\n    plt.show()\n    additional = pandas.Series(response, name=crypto[-6:])\n    dataset = pandas.concat([dataset, additional], axis=1)\ndataset.to_csv(\"crypto_currencies.csv\")\ndataset.describe()",
        "text": "shapeshift currency available , btc , ltc , ppc , drk , doge , nmc , ftc , blk , nxt , btcd , qrk , rdd , nbt , bts , bitusd , xcp , xmr ethtokens available , augur , golem , eth , singular dtv , iconomi",
        "id": 541
    },
    {
        "code": "df.loc[:,'D'] = np.array([5] * len(df))\ndf\ndf.loc[:,'F'] = [np.nan, 1,2,3,4,5]\ndf",
        "text": "set by assign with a numpy array ,",
        "id": 542
    },
    {
        "code": "churn_df = sqlContext.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"churn.csv\")\ncustomer_df = sqlContext.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"customer.csv\")",
        "text": "< span style=  color ,   -   fa04d9  > step 2 , read data into spark dataframes   note , you want to reference the spark dataframe api to learn more about the support operation , <url>",
        "id": 543
    },
    {
        "code": "from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold=(.7 * (1 - .7)))\ndata2 = pd.concat([X_train,X_test])\ndata_new = pd.DataFrame(sel.fit_transform(data2))\ndata_y = pd.concat([y_train,y_test])\nfrom sklearn.model_selection import train_test_split\nX_new,X_test_new = train_test_split(data_new)\nY_new,Y_test_new = train_test_split(data_y)",
        "text": "identify highly correlate column and drop those column before build model",
        "id": 544
    },
    {
        "code": "make_offer(delta*offer[4][2],delta*offer[4][1])\nmake_offer(delta*offer [3][1], delta*offer[3][2])\nmake_offer(delta*offer [2][2], delta*offer[2][1])",
        "text": "what be the payoff for player 1 and 2 in a three period version of the bargain model in the lecture ? the payoff for player 1 be 10 and for player 2 be 0   -  7 )   -  in which period will the player reach an agreement ? they reach an agreement in period 1  . ",
        "id": 545
    },
    {
        "code": "with open('../data/train.csv', 'r') as train_csv:\n    csv_reader = csv.reader(train_csv)\n    next(csv_reader)\n    x = list(csv_reader)\n    train_data = np.array(x).astype(\"int\")",
        "text": "load data read the train dataset  .  header be skip and then all data transform into an numpy array  .  then picture be store into a numpy 2d array where every row be a train example  .  image  label be store into a numpy array  . ",
        "id": 546
    },
    {
        "code": "subset_test_users = test_data['user_id'].unique()[0:10000]\nrecommend = personalized_model.recommend(subset_test_users,k=1)\nrecommend.head()\ncount_recommend = recommend.groupby(key_columns='song',operations={'count': graphlab.aggregate.COUNT()})\ncount_recommend.sort('count', ascending = False)",
        "text": "part 3 , use groupby   -   aggregate to find the most recommend song",
        "id": 547
    },
    {
        "code": "data = pd.DataFrame(np.random.randn(20).reshape((5, 4)),\n         index=['Ohio', 'Colorado', 'Utah', 'New York','Arizona'],\n         columns=['one', 'two', 'three', 'four'])\ndata",
        "text": "with a dataframe , you can sort by index on either axis",
        "id": 548
    },
    {
        "code": "bac15 = BAC[['Open' , 'High' , 'Low','Close']].ix['2015-01-01' : '2016-01-01']\nbac15.iplot(kind = 'candle')",
        "text": "use  . iplot ( kind=candle ) to create a candle plot of bank of america 's stock from jan 1st 2015 to jan 1st 2016  . ",
        "id": 549
    },
    {
        "code": "\nx = np.arange(0, 3*np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\nplt.plot(x, y_sin)\nplt.plot(x, y_cos)\nplt.xlabel('X-axis label')\nplt.ylabel('Y-axix label')\nplt.title('Sine and Cosine')\nplt.legend(['sine, cosine'])\nplt.show()",
        "text": "with a little bite of extra work , we can plot multiple line at once , add a title , legend and axis label ,",
        "id": 550
    },
    {
        "code": "A\nnp.diag(A)\nnp.diag(A, -1)",
        "text": "diag with the diag  function we can also extract the diagonal and subdiagonals of an array",
        "id": 551
    },
    {
        "code": "age_dict = {'Max': 26,\n            'Andy': 25,\n            'Ben': 28,\n            'Sarah': 26,\n            'Anne': 21}\nage = pd.Series(age_dict)\nage",
        "text": "a series can also be create from a dictionary  . ",
        "id": 552
    },
    {
        "code": "\nimport os\n_ = [hw05.grade(q[:-3]) for q in os.listdir(\"tests\") if q.startswith('q')]",
        "text": "write your conclusion here , replace this text   - ",
        "id": 553
    },
    {
        "code": "def processing_pipeline(img):\n    undist = undistort_img(img)\n    grad_thresholded = gradthresh(undist)\n    color_thresholded = colorthresh(undist)\n    combined_thresholded = combinethresh(grad_thresholded, color_thresholded)\n    warped_img, M, Minv = warp(combined_thresholded)\n    ploty, lefty, righty, leftx, rightx, left_fitx, right_fitx, out_img = find_lane(warped_img)\n    curverad, vehicle_offset = calculate_curvature_offset(ploty, lefty, righty, leftx, rightx, undist.shape[1], undist.shape[0])\n    result = draw_lane(warped_img, out_img, undist, Minv, ploty, left_fitx, right_fitx, curverad, vehicle_offset)\n    return result",
        "text": "image process pipeline    -  basically call all the function define above in the desire sequence to successfully detect lane line on the image  . ",
        "id": 554
    },
    {
        "code": "BATCH_SIZE = 16\nwith g.as_default():\n    filename_queue = tf.train.string_input_producer(csv_file_list)\n    \n    reader = tf.TextLineReader(skip_header_lines=0)\n    \n    key, line_from_file = reader.read(filename_queue)\n    \n    record_defaults = [[0.5], [0.5], [tf.to_int32(0)]]\n    \n    example = tf.decode_csv(line_from_file, record_defaults=record_defaults)\n    \n    features = example[:2] + [1.0]\n    label = example[-1]\n    \n    key_batch, data_batch, label_batch = tf.train.shuffle_batch(\n        [key, features, label], \n        batch_size=BATCH_SIZE, \n        capacity=400, \n        min_after_dequeue=100)",
        "text": "add operation to the graph which prepare mini   -   batch  . ",
        "id": 555
    },
    {
        "code": "\nfrom sklearn.svm import LinearSVC\nsvm_test = LinearSVC()\nsvm_test.fit(x_train,y_train)\nprint(\"default score: \",svm_test.score(x_test,y_test))\nbest_score = 0\nfor C in range(1, 1000):\n    temp_c= C/10\n    svm = LinearSVC(C=temp_c,penalty='l2')\n    svm.fit(x_train, y_train)\n    score = svm.score(x_test, y_test)\n    if score > best_score:\n        best_score = score\n        best_parameters = {'C': temp_c}\n            \nprint(\"Best score: {:.2f}\".format(best_score))\nprint(\"Best parameters: {}\".format(best_parameters))",
        "text": "linear support vector classifier   the linear support vector classifier be use next  .  for this algorithm , two hyperparameters be important of which hyperparameter c be fit with different value between 10  -  4 and 10^4 to find the best perform algorithm  . ",
        "id": 556
    },
    {
        "code": "for name in ('JADE', 'CHLOÉ', 'MANON', 'LINA', 'LOLA', 'INÈS', 'SARAH', 'ZOÉ'):\n    plotname(name, 1)\nplt.xlim([1900,2015])\nplt.ylim([0,8500])\nplt.legend(loc='upper left')\nplt.xlabel('Year')\nplt.ylabel('Number of people')\nplt.title('\"New\" female names in Top 15 in 2015')\nplt.show()",
        "text": "new  female name in top 15 in 2015    -  in term of absolute number of birth ,",
        "id": 557
    },
    {
        "code": "total_rows, total_cols, total_layers = photo_data.shape\nX, Y = np.ogrid[:total_rows, :total_cols]\ncenter_row, center_col = total_rows / 2, total_cols / 2\ndist_from_center = (X - center_row)**2 + (Y - center_col)**2\nradius = (total_rows / 2)**2\ncircular_sieve = (dist_from_center > radius)\nphoto_data = misc.imread('./wifire/sd-3layers.jpg')\nphoto_data[circular_sieve] = 0\nplt.figure(figsize=(15,15))\nplt.imshow(photo_data)",
        "text": "< p style=  font   -   family , arial , font   -   size,1 . 75em , color ,   -   2462c0 , font   -   style , bold  >   let u try something cool   -  a mask that be in shape of a circular disc",
        "id": 558
    },
    {
        "code": "sns.set_style('whitegrid')\nsns.countplot(x='Survived',data=train,palette='RdBu_r')\nsns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=train,palette='RdBu_r')\nsns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=train,palette='rainbow')\nsns.distplot(train['Age'].dropna(),kde=False,color='darkred',bins=30)\ntrain['Age'].hist(bins=30,color='darkred',alpha=0.7)\nsns.countplot(x='SibSp',data=train)\ntrain['Fare'].hist(color='green',bins=40,figsize=(8,4))",
        "text": "roughly 20 percent of the age data be miss  .  the proportion of age miss be likely small enough for reasonable replacement with some form of imputation  .  look at the cabin column , too much of data be miss  . ",
        "id": 559
    },
    {
        "code": "bins = np.linspace(-1.0, 1.0, 100)\nplt.hist(RLR_err.tolist(), bins, alpha=0.3, label='Ridge Regression',color=\"green\")\nplt.legend(loc='upper right')\nplt.title(\"Error Distribution\")\nplt.xlabel(\"Error Value\")\nplt.ylabel(\"Number\") \nplt.show()\nRLR_err.tolist()",
        "text": "plot the error distribution of ridge regression",
        "id": 560
    },
    {
        "code": "n = 0\nif n % 2 == 0:\n    print(\"Even\")\nelif n % 2 == 1:\n    print(\"Odd\")\nelse:\n    print(\"Neither even nor odd\")",
        "text": "write a cell that look at an integer and print  even  if the number be even or  odd  if the number be odd  .  ( if you want to make your program more robust , you can have it print an error message if it receive a non   -   integer )",
        "id": 561
    },
    {
        "code": "\nmax_iters = 600\ngamma = 1e-7\nlosses, ws = logistic_regression(y, tX, gamma, max_iters)\nw_star, min_loss = get_best_model(losses, ws)\nprint(\"Min loss = %f\"%(min_loss))\nDATA_TEST_PATH = 'data/test.csv' \n_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\ntX_test, mean_tX_test, std_tX_test = standardize(tX_test)\nOUTPUT_PATH = 'output/LR_GD.csv' \ny_pred = predict_labels(w_star, tX_test)\ncreate_csv_submission(ids_test, y_pred, OUTPUT_PATH)",
        "text": "generate prediction and save ouput in csv format for submission , we retrain on all the data",
        "id": 562
    },
    {
        "code": "x=np.arange(11)\nprint(x)\nprint(type(x))\nx[0]\nx[3]\nx[1:10:2]\nx2=np.array( [  [3,4,5,2] , [2,3,4,6] , [9,5,6,1] ] )\nprint(x2)\nprint(x2[0:2,0:2])",
        "text": "array slice , access subarray    -  x [ start , stop , step ]",
        "id": 563
    },
    {
        "code": "bp_enr_RAG[res_cols]\nbp_enr_RAG[res_cols].to_csv(os.path.join(BASE,'bp_enr_RAG.csv'))",
        "text": "up in rag , biological process",
        "id": 564
    },
    {
        "code": "x = set()\nx.add(1)\nx",
        "text": "set and booleans    -  set set be an unordered collection of unique element  .  we can construct them by use the set ( ) function  . ",
        "id": 565
    },
    {
        "code": "from sklearn.datasets import load_digits\ndigits = load_digits()\nX = digits.data\ny = digits.target\nX.shape",
        "text": "validate model one of the most important piece of machine learn be   -  model validation  -  , that be , check how well your model fit a give dataset  .  but there be some pitfall you need to watch out for  .  consider the digit classification example  .  how might we check how well our model fit the data ?",
        "id": 566
    },
    {
        "code": "c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(len(idx)-cs)]",
        "text": "for each of 0 through 7 , create a list of every 8th character with that start point  .  these will be the 8 input to our model  . ",
        "id": 567
    },
    {
        "code": "from sklearn.model_selection import GridSearchCV \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [60, 70, 80],\n    'max_features': ['auto'],\n    'min_samples_leaf': [1, 2],\n    'min_samples_split': [2, 5],\n    'n_estimators': [700, 800, 900]\n}\nrf = RandomForestRegressor()\ncv = KFold(n_splits=5, shuffle=True, random_state=7)\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n                           scoring=None,\n                           cv=cv, n_jobs=-1, verbose=3)\ngrid_search.fit(Xtrain, ytrain)",
        "text": "gridsearchcv slight increase in performance with the parameter suggest by randomizedsearchcv  .  next , we use gridsearchcv which iterate over all of the possible combination instead of randomly sample   -  note , user input require in the next section to create the gridsearch parameter grid base on randomizedsearch result   - ",
        "id": 568
    },
    {
        "code": "log_clf2 = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\nt0 = time.time()\nlog_clf2.fit(X_train_reduced, y_train_emotion)\nt1 = time.time()\nT = t1 - t0\nprint(\"Training took {:.2f}s\".format(T))\ny_pred = log_clf2.predict(X_test_reduced)\nacc = accuracy_score(y_test_emotion, y_pred)\nacc\npart_d = [log_clf2, T, acc]\njoblib.dump(part_d, 'part_d.pkl', protocol = 2)",
        "text": "use pca to reduce the train dataset 's dimensionality , with an explain variance ratio of 95 %  .  train a new logistic regression classifier on the reduce dataset and see how long it take  .  wa train much fast ? return the train model , time of train , and accuracy on the test set in a pickle format a *part _ d . pkl  - ",
        "id": 569
    },
    {
        "code": "\ndiabetes_replaced = diabetesdrop.replace({'>7':1,'>8':2,'Norm':3,'None':0,'No':0,'Yes':1,'NO':0,'Female':1,'Male':0,'Caucasian':1,'?':0,'AfricanAmerican':2,'Other':3,'Hispanic':4,'Asian':5, 'Steady':1,'Up':2,'Down':3,'>30':1,'<30':2,'[0-10)':10,'[10-20)':20,'[20-30)':30,'[30-40)':40,'[40-50)':50,'[50-60)':60,'[60-70)':70,'[70-80)':80,'[80-90)':90,'[90-100)':100})\nages = [10,20,30,40,50,60,70,80,90,100]\nbins = [0,10,20,30,40,50,60,70,80,90,100]\ncats = pd.cut(ages,bins)\ncats",
        "text": "transform multi   -   class and categorical variable",
        "id": 570
    },
    {
        "code": "test_x_np = np.array(test_x_df)\ntest_x_np = test_x_np.reshape((test_x_df.shape[0], 24, test_norm_df.shape[1]))\nprint(test_x_df.shape)\nprint(test_x_np.shape)",
        "text": "convert the lstm test x dataframe to a numpy array , then reshape into the 3d format require for lstm model  . ",
        "id": 571
    },
    {
        "code": "xt, yt = next(dl)\ny_pred = net2(Variable(xt).cuda())\nl = loss(y_pred, Variable(yt).cuda())\nprint(l)",
        "text": "now , let 's make another set of prediction and check if our loss be low ,",
        "id": 572
    },
    {
        "code": "\nx_vals = [x for x in nx.get_node_attributes(test_graph, 'x').values()]\ny_vals = [x for x in nx.get_node_attributes(test_graph, 'y').values()]\nz_vals = [x for x in nx.get_node_attributes(test_graph, 'z').values()]\ncolor_dict = {\n    'H': 'red',\n    'O': 'blue',\n}\nsize_dict = {\n    'H': 25,\n    'O': 60,\n}\nc_vals = [color_dict[x[0]] for x in test_graph]\ns_vals = [size_dict[x[0]] for x in test_graph]",
        "text": "state of the graph the graph ha a bunch of node that have no edge inbetween them  .  the node do however , have their x , y , z position attach to them  .  we should at least be able to generate a scatter plot   -  get node attribute from the graph",
        "id": 573
    },
    {
        "code": "clf = learn.DNNLinearCombinedClassifier(\n                    linear_feature_columns=wide_columns,\n                    dnn_feature_columns=deep_columns,\n                    dnn_hidden_units=[100, 50, 25, 10, 5])\nfit_save(clf, 'DNN with 5 hidden layers', compare_classifiers)",
        "text": "dnn with 5 hide layer",
        "id": 574
    },
    {
        "code": "def get_numpy_data(dataframe, features, label):\n    dataframe['one'] = 1\n    features = ['one'] + features\n    features_array = dataframe[features].as_matrix()\n    output_label = dataframe[label].as_matrix()\n    return (features_array, output_label)\nfeature_matrix, sentiments = get_numpy_data(products, important_words, 'sentiment')",
        "text": "convert data frame to multi   -   dimensional array",
        "id": 575
    },
    {
        "code": "url='https://movie.douban.com/review/best/'\nimport requests\ndef getHTMLtext(url):\n    try:\n        r=requests.get(url,timeout=20)\n        r.raise_for_status()\n        r.encoding=r.apparent_encoding\n        return r.text\n    except:\n        return'error'\nif __name__=='__main__':\n    url='https://movie.douban.com/review/best/'\n    print(getHTMLtext(url))\nimport requests\nfrom lxml import etree\nr=requests.get(url).text\ns=etree.HTML(r)\na=s.xpath(\"//div[@class='short-content']/text()\")\nprint(a)",
        "text": "get the douban . com a practice",
        "id": 576
    },
    {
        "code": "ffinal = pd.merge(p_stats, finals[[\"season\", \"PLAYER_ID\", 'pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l']], \n                  on=['PLAYER_ID', \"season\"], how='left')\nffinal[['pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l']] = ffinal[['pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l']].fillna(0)\nffinal['season'] = ffinal['season'] + 1\nffinal.groupby(['GAME_ID', \"TEAM_CITY\", 'TEAM_ID'])['pts_l', 'ast_l', 'blk_l', 'reb_l', 'stl_l'].sum().reset_index().head()",
        "text": "link back player stats to each game",
        "id": 577
    },
    {
        "code": "dview.apply(mul, 5, 6)",
        "text": "and the same thing in parallel ?",
        "id": 578
    },
    {
        "code": "basic_plot2(c_totals=new_totals[:,0], sd=sd, oldmean=mean)",
        "text": "show the new distribution with original mean flank by desire standard deviation ,",
        "id": 579
    },
    {
        "code": "from sklearn.externals import joblib\njoblib.dump(clf, 'model.pkl')\nclf = joblib.load('model.pkl')",
        "text": "in the specific case of the scikit , it may be more interest to use joblib  s replacement of pickle ( joblib . dump   -   joblib . load ) , which be more efficient on object that carry large numpy array internally a be often the case for fit scikit   -   learn estimator , but can only pickle to the disk and not to a string ,",
        "id": 580
    },
    {
        "code": "library(dplyr)\nentity_project_page <- read.table(\"../results/sql_queries/entity_project_page_views.tsv\", header=FALSE, sep=\"\\t\")\nentity_views <- read.table(\"../results/sql_queries/entity_views.tsv\", header=FALSE, sep=\"\\t\")\nentity_project_page_views <- entity_project_page",
        "text": "statistic for entity project page view",
        "id": 581
    },
    {
        "code": "df.apply(np.cumsum)\ndf.apply(lambda x: x.max() - x.min())",
        "text": "apply function to the data",
        "id": 582
    },
    {
        "code": "\ntfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=2, stop_words=my_stop_words, max_features=2000)\ntfidf4lsa = tfidf_vectorizer.fit_transform(documents)\nlsa_feature_names = tfidf_vectorizer.get_feature_names()\nlsa_model = TruncatedSVD(n_components=n_topics, n_iter=20 ,random_state=1).fit(tfidf4lsa)\nlsa_W = lsa_model.transform(tfidf4lsa)\nlsa_H = lsa_model.components_\nfor topic_idx, topic in enumerate(lsa_H):\n    print(\"Topic {}:\".format(topic_idx))\n    print(\" \".join([lsa_feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))",
        "text": "category of topic ,   topic 0 , american job , illegal vote   topic 1 , american job   topic 2 , donald win   topic 3 , indiana   topic 4 , military ( foriegn )   topic 5 , military ( domestic )   topic 6 , donald win   topic 7 , trade   topic 8 , clinton email   topic 9 , healthcare",
        "id": 583
    },
    {
        "code": "df_high_tm_low_we = df_sim[(df_sim[\"we_sim\"] > 0.8)]\ndf_high_tm_low_we.iloc[np.random.permutation(len(df_high_tm_low_we))]",
        "text": "high tm similarity , high we similarity",
        "id": 584
    },
    {
        "code": "\ndef worker():\n    print (threading.currentThread().getName(), 'Starting')\n    time.sleep(2)\n    print (threading.currentThread().getName(), 'Exiting')\ndef my_service():\n    print (threading.currentThread().getName(), 'Starting')\n    time.sleep(3)\n    print (threading.currentThread().getName(), 'Exiting')\nt = threading.Thread(name = 'my_service', target = my_service)\nw1 = threading.Thread(name = 'worker', target = worker)\nw2 = threading.Thread(target = worker)\nt.start()\nw1.start()\nw2.start()",
        "text": "determine the current thread use argument to identify or name the thread be cumbersome and unnecessary  .  each thread  instance ha a name with a default value that can be change a the thread be create  .  name thread be useful in server process with multiple service thread handle different operation  . ",
        "id": 585
    },
    {
        "code": "hide_code\nhuman_count = 0\nfor image in human_files_short:\n    if dog_detector(image):\n        human_count += 1\n        \ndog_count = 0\nfor image in dog_files_short:\n    if dog_detector(image):\n        dog_count += 1\n        \nprint(human_count, \"% of the humans were detected as dogs\")\nprint(dog_count, \"% of the dogs were detected as dogs\")",
        "text": "( implementation ) ass the dog detector  _  _ question 3 ,  _  _  use the code cell below to test the performance of your dog _ detector  function   -  what percentage of the image in human _ files _ short  have a detect dog ?   -   what percentage of the image in dog _ files _ short  have a detect dog ?  _  _ answer 3 ,  _  _ ",
        "id": 586
    },
    {
        "code": "\nX_train = train.iloc[:, 1:]\ny_train = train.iloc[:, 0]\nX_test = test.iloc[:, 1:]\ny_test = test.iloc[:, 0]\nfrom sklearn.ensemble import BaggingRegressor\nbagreg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=500, bootstrap=True, oob_score=True, random_state=1)\nbagreg.fit(X_train, y_train)\ny_pred = bagreg.predict(X_test)\ny_pred\nnp.sqrt(metrics.mean_squared_error(y_test, y_pred))",
        "text": "bag decision tree in scikit   -   learn ( with b=500 )",
        "id": 587
    },
    {
        "code": "print(dfs.keys())\ndfs['2014-kentucky-senate-mcconnell-vs-grimes'].head()",
        "text": "show the head of the dataframe contain the poll for the 2014 senate race in kentucky between mcconnell and grime",
        "id": 588
    },
    {
        "code": "from pandas.tools.plotting import scatter_matrix\nscatter_matrix(data[['Sample_size', 'Median']], figsize=(8,8));\nscatter_matrix(data[['Sample_size', 'Median', 'Unemployment_rate']], figsize=(8,8));",
        "text": "assignment   -   make a scatter matrix of sample size and median   -   make a scatter matrix of sample size , median and unemployment rate",
        "id": 589
    },
    {
        "code": "d = pd.Period('2016-02-28', freq='D')\nd\nd.start_time\nd.end_time\nd+1",
        "text": "< h3 style=  color , purple  > daily period",
        "id": 590
    },
    {
        "code": "a_website = requests.get(\"http://google.com\")  \ndata = BeautifulSoup(a_website.content, \"lxml\")    \n# We create a beautifulsoup object",
        "text": "first , we request the webpage and extract the content ,",
        "id": 591
    },
    {
        "code": "import pandas as pd\nmovies = pd.read_csv('./data/imdbratings')\nmovies.head()\nmovies['title'].sort_values(ascending=False)\nmovies.sort_values('title').head()\nmovies.sort_values(['genre','duration'])\nmovies.sort_values('genre', ascending=True).sort_values('duration', ascending=False)\nmovies.sort_values(by=['genre','duration'], ascending=[True,False])",
        "text": "how do i sort a dataframe or a series",
        "id": 592
    },
    {
        "code": "entities_with_no_page_views <- subset(entity_views, page_views == 0)\nnrow(entities_with_no_page_views)",
        "text": "entity that do not have page view",
        "id": 593
    },
    {
        "code": "\na=np.random.randint(2500, size=50)\ndfx1[a]=dfx1[a]*-1\ninput_vector=dfx1 \nS=W.dot(input_vector)\nSdf=pd.DataFrame(S)\nnum = Sdf._get_numeric_data()\nnum[num < 0] = -1\nnum[num >= 0] = 1\nSnp=np.array(Sdf)\nSnew=np.reshape(Snp, (50, 50))",
        "text": "input pattern to be test , first try image 1",
        "id": 594
    },
    {
        "code": "dimensions = { 'longitude': {'range': (149.06, 149.17)},   \n               'latitude':  {'range': (-35.33, -35.27)},\n               'time':      {'range': ((2013, 1, 1), (2014, 1, 1))} }\nlake_str = \"Lake_Burley_Griffin\"        \nlake_dispname = \"Lake Burley Griffin\"   \nwater_mask_thr = 90.0     \nwater_mask_buffer = 1.5   \nmin_valid_pix_thr = 10.0  \nsave_basedir = '/g/data/jr4/vis_data_v1.2/'   \nWQ_type = \"WQ_(B2+B3)/2\"  # Type of WQ info generated by this code",
        "text": "user input here be the input parameter need from the user ,",
        "id": 595
    },
    {
        "code": "def max_citation():    \n    rank_energy_gdp['SelfCitationRatio']=rank_energy_gdp['Self-citations']/rank_energy_gdp['Citations']\n    rank_energy_gdp.sort_values(['SelfCitationRatio'], ascending=False)\n    country=rank_energy_gdp.index[0]\n    topren=rank_energy_gdp['SelfCitationRatio'][0]\n    return (country, topren)\ntopren=max_citation()\ntopren",
        "text": "create a new column that be the ratio of self   -   citation to total citation  .  what be the maximum value for this new column , and what country ha the high ratio ?",
        "id": 596
    },
    {
        "code": "\nG = ox.simplify_graph(G)\nfig, ax = ox.plot_graph(G, node_color='b', node_zorder=3)\nec = ox.get_edge_colors_by_attr(G, attr='length')\nfig, ax = ox.plot_graph(G, node_color='w', node_edgecolor='k', node_size=30, \n                           node_zorder=3, edge_color=ec, edge_linewidth=3)\nec = ['r' if data['oneway'] else 'b' for u, v, key, data in G6.edges(keys=True, data=True)]\nfig, ax = ox.plot_graph(G6, node_size=0, edge_color=ec, edge_linewidth=1.5, edge_alpha=0.5)",
        "text": "the red and blue dot above be osm node  .  we ll remove the node in red a they re not real network node  . ",
        "id": 597
    },
    {
        "code": "obj = pd.Series(['a', 'a', 'b', 'd'] * 4)\nobj.describe()",
        "text": "on non   -   numeric data , describe produce alternate summary statistic",
        "id": 598
    },
    {
        "code": "vectorizer = pickle.load(open(RESOURCE_PATH['TFIDF_VECTORIZER'], 'rb'))\nX_train_tfidf, X_test_tfidf = vectorizer.transform(X_train), vectorizer.transform(X_test)",
        "text": "let u reuse the tf   -   idf vectorizer that we have already create above  .  it should not make a huge difference which data wa use to train it  . ",
        "id": 599
    },
    {
        "code": "a=range(5)\na",
        "text": "head regular text   -  bold   -  buttet 1   -   bullet 2",
        "id": 600
    },
    {
        "code": "temp = df.groupby(['yearID','playerID']).sum()\ntemp = temp[(temp['2B']>=40) & (temp['3B']>=10) & (temp['Hit']>=200) & (temp['HR']>=30)]\ntemp = temp.reset_index('playerID')\ntemp = temp['playerID'].unique()\nlen(temp)",
        "text": "how many player have hit 40 2bs , 10 3bs , 200 hit , and 30 hr ( inclusive ) in one season ? ( number only ) ( above )",
        "id": 601
    },
    {
        "code": "bq.Query('SELECT * FROM drivedatasource LIMIT 5', data_sources={'drivedatasource': drivedata}).execute().result()",
        "text": "query the table now let 's verify that we can access the data  .  we will run a simple query to show the first five row  .  note that we specify the federate table by use a name in the query , and then pas the table in use a data _ sources  dictionary parameter  . ",
        "id": 602
    },
    {
        "code": "import logging\nlog = logging.getLogger('test')\ndef testlog():\n    log.critical('critical')\n    log.error('error')\n    log.warning('warning')\n    log.info('info')\n    log.debug('debug')\nlog2 = logging.getLogger('test')\nlog is log2\nlog.setLevel(level = logging.CRITICAL)\ntestlog()\nlog.setLevel(level = logging.WARNING)\ntestlog()\nlog.setLevel(level = logging.DEBUG)\ntestlog()",
        "text": "log   -   often two type of log be perform   -   during development , may want verbose log to help debug system   -   during production , want to log  important  event , like web hit , major failure , service perform , account data   -   want one system to handle both need   -   can send logger output to file and stream   -   python log similiar to java log4j",
        "id": 603
    },
    {
        "code": "node3 = tf.add(node1, node2)\nprint(\"node3: \", node3)\nprint(\"sess.run(node3): \", sess.run(node3))\nshow_graph(tf.get_default_graph())",
        "text": "we can build more complicate computation by combine   -  tensor  -  node with operation ( operation be also nodes .  )  .  for example , we can add our two constant node and produce a new graph a follow ,",
        "id": 604
    },
    {
        "code": "alternatives = find_alternatives(\"ifttt\")\nimport os\nimport smtplib \nserver = smtplib.SMTP('smtp.gmail.com', 587)\nserver.starttls()\nserver.login(\"lucasberbesson@gmail.com\", os.environ['MY_PWD'])\nmsg = \"IFTTT has {} alternatives\".format(len(alternatives))\nserver.sendmail(\"lucasberbesson@gmail.com\",\"lucas.berbesson@fabdev.fr\", msg)\nserver.quit()",
        "text": "smtplib the smtplib module define an smtp client session object that can be use to send mail to any internet machine  . ",
        "id": 605
    },
    {
        "code": "\ny_pred = list()\ny_prob = list()\ncoeff_labels = ['lr', 'l1', 'l2']\ncoeff_models = [lr, lr_l1, lr_l2]\nfor lab,mod in zip(coeff_labels, coeff_models):\n    y_pred.append(pd.Series(mod.predict(X_test_new), name=lab))\n    y_prob.append(pd.Series(mod.predict_proba(X_test_new).max(axis=1), name=lab))\ny_pred = pd.concat(y_pred, axis=1)\ny_prob = pd.concat(y_prob, axis=1)\ny_pred.head()",
        "text": "predict and store the class for each model   -  also store the probability for the predict class for each model  . ",
        "id": 606
    },
    {
        "code": "word_coefficient_tuples = sorted(word_coefficient_tuples, key=lambda x:x[1], reverse=False)\nword_coefficient_tuples[0:20]",
        "text": "twenty  most negative  word next , we repeat this exercise on the 10 most negative word  .  that be , we compute the 10 word that have the most negative coefficient value  .  these word be associate with negative sentiment  . ",
        "id": 607
    },
    {
        "code": "whWords = set(whReleases['normalized_tokens'].sum())\nkenWords = set(kenReleases['normalized_tokens'].sum())\noverlapWords = whWords & kenWords\noverlapWordsDict = {word: index for index, word in enumerate(overlapWords)}\noverlapWordsDict['student']",
        "text": "now we need to compare the two collection of word , remove those not find in both , and assign the remain one index  . ",
        "id": 608
    },
    {
        "code": "\ntrain_Y = keras.utils.to_categorical(train_Y, num_classes)\ntest_Y = keras.utils.to_categorical(test_Y, num_classes)\ntrain_X = train_X.astype('float32')\ntest_X = test_X.astype('float32')\ntrain_X = train_X/255\ntest_X = test_X/255\ntrain_XV=train_X.reshape(50000,3072)\ntest_XV=test_X.reshape(10000,3072)",
        "text": "in order for our dataset to be compatible with the kera api , we nee our label to be represent use [ one   -   hot encode ] ( <url> )  .  we also want to convert the image representation from 8   -   bite ( integer ) rgb pixel to 32   -   bite float point between 0 and 1  . ",
        "id": 609
    },
    {
        "code": "threshold = 1.0\nweights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\nclipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\nclip_weights = tf.assign(weights, clipped_weights)",
        "text": "next , let 's get a handle on the first hide layer 's weight and create an operation that will compute the clip weight use the clip _ by _ norm ( )  function  .  then we create an assignment operation to assign the clip weight to the weight variable ,",
        "id": 610
    },
    {
        "code": "\nmodel['depth'] = [0, 100, 1100, 1140]\nmodel['src'][2] = 75\nmodel['rec'][2] = 100\nsdtarget = epm.dipole(res=rtg, **model)\nsdnotarg = epm.dipole(res=rhs, **model)",
        "text": "calculate shallow model , deep src/rec   -   water depth of 100 m   -   source depth 75 m   -   receiver depth 100 m",
        "id": 611
    },
    {
        "code": "sw_classifier = bnn.CnvClassifier(bnn.NETWORK_CNVW1A1, \"streetview\", bnn.RUNTIME_SW)\nresult_class_idx = sw_classifier.classify_image(img)\nprint(\"Inferred number: {0}\".format(sw_classifier.class_name(result_class_idx)))",
        "text": "launch bnn in software the inference on the same image be perform in sofware on the arm core ,",
        "id": 612
    },
    {
        "code": "\ntank.plot(color='0.7', label='prior')\ntank.update(likelihood_tank, 17)\ntank.plot(label='posterior')\nplt.legend()\ndecorate_tank('Distribution after two tanks')\ntank.mean()",
        "text": "exercise 3 ,   -  suppose we see another tank with serial number 17 .  what effect doe this have on the posterior probability ? update the pmf  with the new data and plot the result  . ",
        "id": 613
    },
    {
        "code": "import pandas as pd\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)\nhousing = load_housing_data()\nhousing.head()",
        "text": "load the data in panda",
        "id": 614
    },
    {
        "code": "\nclf = GaussianNB()\nscores = cross_val_score(clf, data.data, data.target, cv=5)\nprint(scores)",
        "text": "fold cross validation this estimate the accuracy of a gaussian naive bay model by split the data , fit a model and compute the score 5 consecutive time  .  the result be a list of the score from each consecutive run  . ",
        "id": 615
    },
    {
        "code": "b = np.array([1, 2, 3, 4])\nprint(b)\nprint(b.dtype)",
        "text": "we can create an array from a list  . ",
        "id": 616
    },
    {
        "code": "titanic.loc[(titanic.age == 25) & (titanic.who=='child'), 'who'] = \"woman\"\ntitanic.loc[813,]",
        "text": "in this case , we have to use the value of < font color=  blue  > sex   to figure out the value of < font color=  blue  > who    . ",
        "id": 617
    },
    {
        "code": "def powerSet(x):\n    output=[]\n    n=len(x)\n    for i in range(2**n):\n        vec=intToNDigits(i,n)\n        output.append(set(select(x,vec)))\n    return output\npowerSet(['avery', 'math', 'butler'])\npowerSet(['avery', 'math', 'butler', 'dodge'])\nlen(powerSet(['avery', 'math', 'butler', 'dodge']))",
        "text": "problem 3c   -   powerset   -   use select  andinttondigits  , define a function powerset ( x )  that return a list of all possible subset of the element of input list x , include the empty set and the set of all element   -   if a set ha n element , the power set will have 2  -  n element",
        "id": 618
    },
    {
        "code": "for i in range(3):\n    trainer.train()\n    test_network()\nfor i in range(3):\n    trainer.train()\n    test_network()\nfor i in range(3):\n    trainer.train()\n    test_network()\nfor i in range(3):\n    trainer.train()\n    test_network()",
        "text": "train the network , for a give number of iteration  .  you can re   -   run this step many time , and it will keep learn but if you train too much you can end up overfitting the train data ( this be visible when the test set accuracy start to decrease )  . ",
        "id": 619
    },
    {
        "code": "x = np.linspace(-1.4, 1.4, 50)\nplt.plot(x, x**2, \"r--\", label=\"Square function\")\nplt.plot(x, x**3, \"g-\", label=\"Cube function\")\nplt.legend(loc=\"best\")\nplt.grid(True)\nplt.show()",
        "text": "legend the simple way to add a legend be to set a label on all line , then just call the legend function  . ",
        "id": 620
    },
    {
        "code": "data = np.zeros(4,                                        \n                dtype={'names':('name', 'age', 'weight'), \n                       'formats':('U10', 'i4', 'f8')})    \ndata",
        "text": "there 's nothing here that tell u that the three array be relate , it would be more natural if we could use a single structure to store all of this data  .  numpy can handle this through structure array , which be array with compound data type  . ",
        "id": 621
    },
    {
        "code": "n_comp = 100\nprint(\"SVD on TFID to get Latent Representation : k = {} ...\".format(n_comp))\nt0 = startTime()\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = csr_matrix(svd_obj.transform(train_tfidf))\ntest_svd =  csr_matrix(svd_obj.transform(test_tfidf))\nendTime(t0)\n#df_test = pd.concat([df_test, test_svd], axis=1)",
        "text": "svd on tf   -   idf feature",
        "id": 622
    },
    {
        "code": "n_comp = 25\nprint(\"SVD on TFID to get Latent Representation : k = {} ...\".format(n_comp))\nt0 = startTime()\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = csr_matrix(svd_obj.transform(train_tfidf))\ntest_svd =  csr_matrix(svd_obj.transform(test_tfidf))\nendTime(t0)\n#df_test = pd.concat([df_test, test_svd], axis=1)",
        "text": "svd on tf   -   idf feature",
        "id": 623
    },
    {
        "code": "9 * 9\ndef f(x):\n    print(x * x)\nf(9)\nfrom ipywidgets import *\nfrom traitlets import dlink\ninteract(f, x=(0, 100))",
        "text": "speed up the bottleneck in the repl < img src=  flow . svg  >",
        "id": 624
    },
    {
        "code": "\nX_train, X_test, y_train, y_test = _________________(d,survived, random_state=1)\nctree = tree.DecisionTreeClassifier(random_state=1, max_depth=2)\nctree.fit(X_train, y_train)",
        "text": "split into train and test datasets , and build the model",
        "id": 625
    },
    {
        "code": "columns = ['﻿\"Data Source\"'] + [str(year) for year in range(1990, 2011)]\npop = df[columns].dropna()",
        "text": "select the column we need",
        "id": 626
    },
    {
        "code": "import cv2\nimport numpy as np\nimage = cv2.imread('images/input.jpg')\nM = np.ones(image.shape, dtype = \"uint8\") * 16\ncv2.imshow(\"Original\", image)\nadded = cv2.add(image, M)\ncv2.imshow(\"Added\", added)\nsubtracted = cv2.subtract(image, M)\ncv2.imshow(\"Subtracted\", subtracted)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\nM = np.ones(image.shape, dtype = \"uint8\") * 75 \nM",
        "text": "arithmetic operation these be simple operation that allow u to directly add or subract to the color intensity  .  calculate the per   -   element operation of two array  .  the overall effect be increase or decrease brightness  . ",
        "id": 627
    },
    {
        "code": "def find_nth_prime(maxprime):\n    primes = [2]\n    len_primes = 1\n    i = 2\n    while len_primes < maxprime:\n        for p in primes:\n            \n            if i % p == 0:\n                break\n                \n        else:\n            primes.append(i)\n            len_primes += 1\n        i += 1\n    return primes[-1]\nfind_nth_prime(10001)",
        "text": "exercise 7 by list the first six prime number , 2 , 3 , 5 , 7 , 11 , and 13 , we can see that the 6th prime be 13 .  what be the 10 001st prime number ?",
        "id": 628
    },
    {
        "code": "x = 0\nwhile x < 5:\n    x = x + 1 \n    print(x)",
        "text": "repetition in python there be three basic way to repeatedly do something in python 1  .  [  while  loop ] ( <url> ) 1  .  [  for  loop ] ( <url> ) 1  .  [ recursion ] ( <url> ) we will not explore recursion here  .  with  while  loop we do something a long a a condition be true",
        "id": 629
    },
    {
        "code": "n_c = output.expect[0]\nn_a = output.expect[1]\nfig, axes = plt.subplots(1, 1, figsize=(8,6))\naxes.plot(tlist, n_c, label=\"Cavity\")\naxes.plot(tlist, n_a, label=\"Atom excited state\")\naxes.set_xlim(0, 150)\naxes.legend(loc=0)\naxes.set_xlabel('Time')\naxes.set_ylabel('Occupation probability');",
        "text": "visualize the result here we plot the excitation probability of the cavity and the atom ( these expectation value be calculate by the mesolve  above )  . ",
        "id": 630
    },
    {
        "code": "\nprint([len(word) for word in words])\nx = [len(word) for word in words]\nprint(x)\n[len(word) for word in words]",
        "text": "use a list comprehension to find the length of each word the result should be ,   [ 3 , 3 , 6 , 4 , 3 , 4 ]",
        "id": 631
    },
    {
        "code": "\nconfint(aov(m))\nprint(\"There is no opt factor so here is the confidence interval for all of them.\")",
        "text": "compute the 95 % confidence interval for the  opt  factor  .  the  anova  function doe n't do this , but you can see something similar use confint ( aov ( l ) ) where  l  be your linear model  .  for model involve factor , the confidence interval be express for the different level in each categorical factor  . ",
        "id": 632
    },
    {
        "code": "for df in [X_train, X_test]:\n    df['GENDER']  = pd.get_dummies(df.GENDER, drop_first=True)\nX_train.GENDER.unique()\nX_test.GENDER.unique()\ndef encode_categorical_variables(var, target):\n        \n        ordered_labels = X_train.groupby([var])[target].mean().to_dict()\n        \n        \n        X_train[var] = X_train[var].map(ordered_labels)\n        X_test[var] = X_test[var].map(ordered_labels)\n        \nfor var in ['MART_STATUS','RTD_ST_CD']:\n    encode_categorical_variables(var, 'Call_Flag')\nX_train.head()",
        "text": "gender , one hot encode   -   remain variable , replace by risk probability",
        "id": 633
    },
    {
        "code": "\nempty_set = set()\nlanguages = {'python', 'r', 'java'}\nlanguages\nsnakes = set(['cobra', 'viper', 'python'])\nsnakes",
        "text": "set   -  set property ,   -  unordered , iterable , mutable , can contain multiple data type * make of unique element ( string , number , or tuples ) * like dictionary , but with key only ( no value )",
        "id": 634
    },
    {
        "code": "x = fibList\ny = numList\np = plt.plot(x,y)",
        "text": "now let 's do a quick simple plot use the list we define early  . ",
        "id": 635
    },
    {
        "code": "library(ggplot2)\nggplot(glass_data, aes(Sodium, Refractive_Index)) + geom_point(aes(colour=factor(Type),shape=factor(Type)))",
        "text": "we should plot the variable refrective _ index and sodium of glass _ data with type variable supply to color and shape parameter  . ",
        "id": 636
    },
    {
        "code": "negative_features = [\n    (build_bag_of_words_features_filtered(movie_reviews.words(fileids=[f])), 'neg') \\\n    for f in negative_fileids\n]\nprint(negative_features[3])\npositive_features = [\n    (build_bag_of_words_features_filtered(movie_reviews.words(fileids=[f])), 'pos') \\\n    for f in positive_fileids\n]\nprint(positive_features[6])\nfrom nltk.classify import NaiveBayesClassifier\nsplit = 800\nsentiment_classifier = NaiveBayesClassifier.train(positive_features[:split]+negative_features[:split])",
        "text": "train a classifier for sentiment analysis use build _ bag _ of _ words _ features  function we will build separately the negative and positive feature  .  basically for each of the 1000 negative and for the 1000 positive review , we create one dictionary of the word and we associate the label  neg  and  po  to it  . ",
        "id": 637
    },
    {
        "code": "import numpy as np\na = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\nrow_r1 = a[1, :]    \nrow_r2 = a[1:2, :]  \nprint(row_r1, row_r1.shape)\nprint(row_r2, row_r2.shape)\ncol_c1 = a[:, 1]\ncol_c2 = a[:, 1:2]\nprint(col_c1, col_c1.shape)\nprint(col_c2, col_c2.shape)",
        "text": "you can also mix integer index with slice index  .  however , do so will yield an array of low rank than the original array  .  note that this be quite different from the way that matlab handle array slice",
        "id": 638
    },
    {
        "code": "import os\npwd = os.getcwd()\nprint(pwd)\npath = pwd + '/data/imagenet/sample/'\ndpath = pwd + '/data/imagenet/sample/'\nprint(path)\nprint(dpath)",
        "text": "data can be download from [ here ] ( <url> )  .  update path below to where you download data to  .  optionally use a 2nd path for fast ( e . g  .  ssd ) storage   -   set both to the same path if use aws  . ",
        "id": 639
    },
    {
        "code": "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(1,28,28)))",
        "text": "next , we declare the input layer ,",
        "id": 640
    },
    {
        "code": "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size= .3,random_state=0)\nKfold = KFold(len(all_data),n_folds = 10,shuffle = False)",
        "text": "split feature data into train and test data",
        "id": 641
    },
    {
        "code": "input_img = Input(shape=(784,))\nencoded = Dense(128, activation='relu')(input_img)\nencoded = Dense(64, activation='relu')(encoded)\nencoded = Dense(32, activation='relu')(encoded)\ndecoded = Dense(64, activation='relu')(encoded)\ndecoded = Dense(128, activation='relu')(decoded)\ndecoded = Dense(784, activation='sigmoid')(decoded)",
        "text": "deep autoencoder we do not have to limit ourselves to a single layer a encoder or decoder , we could instead use a stack of layer , such a ,",
        "id": 642
    },
    {
        "code": "from ecbm4040.optimizers import RMSpropOptim\nmodel = MLP(input_dim=3072, hidden_dims=[100, 100], num_classes=10, l2_reg=0.0, weight_scale=1e-3)\noptimizer = RMSpropOptim(model)\nhist_rmsprop = optimizer.train(model, X_train, y_train, X_val, y_val, \n                               num_epoch=30, batch_size=200, learning_rate=1e-3, \n                               learning_decay=0.95, verbose=False, record_interval=1)",
        "text": "rmsprop < span style=  color , red  >  _  _ todo ,  _  _    edit   -  rmspropoptim  -  in   -  ecbm4040/optimizers . py  - ",
        "id": 643
    },
    {
        "code": "diffs_age = []\nfor _ in range(10000):\n    bootsamp = sample_data.sample(200, replace = True)\n    under21_mean = bootsamp[bootsamp['age'] == '<21']['height'].mean()\n    over21_mean = bootsamp[bootsamp['age'] != '<21']['height'].mean()\n    diffs_age.append(over21_mean - under21_mean)\nnp.percentile(diffs_age, 0.5), np.percentile(diffs_age, 99.5)\n# statistical evidence that over21 are on average taller",
        "text": "for 10,000 iteration , bootstrap sample your sample data , compute the difference in the average height for those old than 21 and those young than 21 .  build a 99 % confidence interval use your sample distribution  .  use your interval to finish answer the first quiz question below  . ",
        "id": 644
    },
    {
        "code": "s2prd = \"%s.SAFE/%s.xml\" % (s2path, s2meta)\nreader = ProductIO.getProductReader(\"SENTINEL-2-MSI-60M-UTM31N\")\nproduct = reader.readProductNodes(s2prd, None)\nwidth = product.getSceneRasterWidth()\nheight = product.getSceneRasterHeight()\nname = product.getName()\ndescription = product.getDescription()\nband_names = product.getBandNames()\nprint(\"Product: %s, %d x %d pixels\" % (name, width, height))",
        "text": "open the product and get some information about it ,",
        "id": 645
    },
    {
        "code": "\nfeature_cols = ['CompetitionDistance','Promo','Promo2','NewAssortment','NewStoreType']\nX = combined_train_data[feature_cols]\ny = combined_train_data.Customers\ntest=combined_test_data\nfrom sklearn import cross_validation\nfeatures_train, features_test, labels_train, labels_test =cross_validation.train_test_split(X, y, test_size=0.3, random_state=42)",
        "text": "split train data into 70 % train data and 30 % test data",
        "id": 646
    },
    {
        "code": "def weighted_degree(G, weight):\n    result = dict()\n    for node in G.nodes():\n        weight_degree = 0\n        for n in G.edges([node], data=True):\n            weight_degree += ____________\n        result[node] = weight_degree\n    return result\nplt.hist(___________)\nplt.show()\nsorted(weighted_degree(G_book1, 'weight').items(), key=lambda x:x[1], reverse=True)[0:10]",
        "text": "exercise create a new centrality measure , weighted _ degree ( graph , weight ) which take in graph and the weight attribute and return a weight degree dictionary  .  weight degree be calculate by sum the weight of the all edge of a node and find the top five character accord to this measure  .  [ 5 min ]",
        "id": 647
    },
    {
        "code": "def transform_country(country):\n    if country == 'USA':\n        return 'United States'\n    else:\n        return country\ndf['Country'].apply(transform_country)",
        "text": "use the apply function to do an advance transformation of the data",
        "id": 648
    },
    {
        "code": "poly2 = preprocessing.PolynomialFeatures(2,interaction_only=False)\nX2=poly2.fit_transform(X)[:,1:] \nregress.fit(X2,Y)\nprint('The coefficients for fitted regression model are: a =', regress.intercept_,', b =',regress.coef_[0])\nprint ('and c =',regress.coef_[1])\nprint ('\\nThe 2nd order poly. regression line fitted is: ',regress.intercept_,'+',regress.coef_[0],'x',regress.coef_[1],'x^2')",
        "text": "fitting the second order polynomial regression model $ y=a+bx+cx^2 $ to the data , where x   -   lift _ kg and y   -   put _ m  - ",
        "id": 649
    },
    {
        "code": "W_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\nkeep_prob = tf.placeholder(\"float\")\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)",
        "text": "densly connect layer now that the image size ha be reduce to 7x7 , we add a fully   -   connect layer with 1024 neuron to allow process on the entire image  .  we reshape the tensor from the pool layer into a batch of vector , multiply by a weight matrix , add a bias , and apply a relu  . ",
        "id": 650
    },
    {
        "code": "print(\"On average billionaires are\", richpeople['age'].mean(), \"years old.\")\nselfmade = richpeople[richpeople['selfmade'] == 'self-made']\nprint(\"Selfmade billionaires are about\", selfmade['age'].mean(), \"years old.\")\nnon_selfmade = richpeople[richpeople['selfmade'] != 'self-made']\nprint(\"Non-selfmade billionaires are on average\", non_selfmade['age'].mean(), \"years old.\")",
        "text": "how old be billionaire ? how old be billionaire self make vs .  non self make ?",
        "id": 651
    },
    {
        "code": "\nLowElevBC_P_wrfbc=BiasCorr_wrfbc['PRECIP_0to3000m'][minElevStation[0][0]]\nLowElevBC_P_wrfbc_lowelev=BiasCorr_wrfbc_lowLiv['PRECIP_0to3000m'][minElevStation[0][0]]\nLowElevBC_P_wrfbc_X_lowelev=BiasCorr_wrfbc_lowLiv['PRECIP_0to3000m'][minElevStation[0][0]]*BiasCorr_wrfbc['PRECIP_0to3000m'][minElevStation[0][0]]\nprint(LowElevBC_P_wrfbc)\nprint(LowElevBC_P_wrfbc_lowelev)\nprint(LowElevBC_P_wrfbc_X_lowelev)\n#firstStation[0][0]",
        "text": "low elevation validation of bias correction",
        "id": 652
    },
    {
        "code": "model.add(Convolution2D(32, kernel_size=(3, 3),\n                 activation='relu',\ninput_shape=input_shape))",
        "text": "next we declare the input layer ,",
        "id": 653
    },
    {
        "code": "l.append(6)\nl\nl.count(2)\nhelp(l.count)",
        "text": "fortunately , with ipython and the jupyter notebook we can quickly see all the possible method use the tab key  .  the method for a list be ,   -   append   -   count   -   extend   -   insert   -   pop   -   remove   -   reverse   -   sort",
        "id": 654
    },
    {
        "code": "import statsmodels.api as sm\nfrom statsmodels.formula.api import glm, ols\nfor key in decisions.keys():\n    if decisions[key][2] == -1:\n        decisions[key][2] = 0\n        \nlength = []\nfor key in fomc_mins.keys():\n    new = (len(pq(fomc_mins[key]).text()),decisions[key][2])\n    length.append(new)\nlength_data = pd.DataFrame(length, columns = [\"Length\", \"Y\"])\nlength_data.head()\nlength_ols_model = ols('Y ~ Length', length_data).fit()\nlength_ols_model.summary()",
        "text": "we now prepare data to carry out a regression  . ",
        "id": 655
    },
    {
        "code": "c = a.view()\nprint(c)\nprint(c is a)\nprint(c.base is a)     \nprint(c.base)\nc.flags.owndata\nc.shape = 2,6                      \na.shape\nc[0,4] = 1234\nprint(\"a: \", a)",
        "text": "view or shallow copy different array object can share the same data  .  the view method create a new array object that look at the same data  . ",
        "id": 656
    },
    {
        "code": "\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(units=16, activation='relu', \n                         kernel_regularizer=regularizers.l2(0.01),\n                        input_shape = (number_of_features,)))\nnetwork.add(layers.Dense(units=16, kernel_regularizer=regularizers.l2(0.01),\n                       input_shape=(number_of_features,)))\nnetwork.add(layers.Dense(units=1, activation='sigmoid'))",
        "text": "create neural network architecture with weight regularization in kera , we can add a weight regularization by include use include kernel _ regularizer=regularizers . l2 ( 0 . 01 ) a late  .  in this example , 0 . 01 determine how much we penalize high parameter value  . ",
        "id": 657
    },
    {
        "code": "\nmodel_pred = KerasClassifier(build_fn=create_model, optimizer=best_optimizer, init=best_init, epochs=best_epochs, batch_size=best_batch_size, verbose=verbose)\nmodel_pred.fit(X, Y)\ntest_df = pd.read_csv(file_test,index_col='PassengerId')\ntest_df = prep_data(test_df)\nX_test = test_df.values.astype(float)\nX_test = scale.transform(X_test)\nprediction = model_pred.predict(X_test)",
        "text": "build model and predit   -   create a classifier with best parameter   -   fit model   -   predict survived",
        "id": 658
    },
    {
        "code": "seriesA = pd.Series([1,2,3,4],index = ['USA', 'Germany','USSR', 'Japan']) \nseriesA\nseriesB = pd.Series([5,6,7,8],index = ['USA', 'Australia','UK', 'Japan']) \nseriesB\nseriesA['USA']\nseriesB['UK']\nseriesC = seriesA + seriesB\nseriesC",
        "text": "use an index the key to use a series be understand it index  .  panda make use of these index name or number by allow for fast look up of information ( work like a hash table or dictionary )  .  let 's see some example of how to grab information from a series  .  let u create two series , seriesa and seriesb ,",
        "id": 659
    },
    {
        "code": "schoolcharacteristicsa = schoolcharacteristicsa.drop(schoolcharacteristicsa.columns[[0, 1, 20]], 1)\nschoolcharacteristicsb = schoolcharacteristicsb.drop(schoolcharacteristicsb.columns[[0, 1]], 1)\nschoolenrollment = schoolenrollment.drop(schoolenrollment.columns[[0, 1]], 1)\nschoolenrollmentdetails = schoolenrollmentdetails.drop(schoolenrollmentdetails.columns[[0, 1]], 1)\nschoolinformation.head()",
        "text": "drop all of the duplicate column  . ",
        "id": 660
    },
    {
        "code": "lookahead_bias_days = 5\npredictive_factor = pricing.pct_change(lookahead_bias_days)\npredictive_factor = predictive_factor.shift(-lookahead_bias_days)\npredictive_factor = predictive_factor.stack()\npredictive_factor.index = predictive_factor.index.set_names(['date', 'asset'])",
        "text": "to create a predictive factor we ll cheat , we will look at future price to make sure we ll rank high stoks that will perform well and vice versa  . ",
        "id": 661
    },
    {
        "code": "\nlr = lm.LinearRegression()\nlr.fit(input_val[:, np.newaxis], output_val);\noutput_lr = lr.predict(input_val[:, np.newaxis])\nplt.plot(input_val, output_val, '.');\nplt.plot(input_val, output_lr, 'g');\nplt.xlabel('Floor Area')\nplt.ylabel('Price of House')\nplt.title('House pricing')\nplt.legend(['Original Data', 'Linear Regression'])\nplt.show()",
        "text": "now we will use linear regression to plot the",
        "id": 662
    },
    {
        "code": "\nmodel = larger_model()\nmodel.summary()\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))",
        "text": "train , like the previous two experiment , the model be fit over 10 epoch with a batch size of 200  . ",
        "id": 663
    },
    {
        "code": "import re\ntext = \"Clearly, he has no excuse for such behavior.\"\nfor m in re.finditer(r\"\\w+ly\", text):\n    print('%d-%d: %s' % (m.start(), m.end(), m.group(0)))",
        "text": "write a python program to find all adverb and their position in a give sentence  . ",
        "id": 664
    },
    {
        "code": "connectivity = kneighbors_graph(X_moons, n_neighbors=10, include_self=False)\nagg_cluster_model = AgglomerativeClustering(linkage=\"complete\", connectivity=connectivity, n_clusters=2,compute_full_tree=True)\ny_pred = agg_cluster_model.fit_predict(X_moons)\nplt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);",
        "text": "add connectivity constraint previously , we join cluster base soley on distance  .  here we introduce a [ connectivity constraint ] ( <url> ) base on k   -   near neighbor graph so that only adjacent cluster can be merge together  . ",
        "id": 665
    },
    {
        "code": "from Bio import AlignIO\nout=open('rbcL@mafftLinsi_aln_clipped.phy', 'w')\nAlignIO.write(clipped_aln, out, 'phylip-relaxed')\nout.close()",
        "text": "write clip alignment to file for downstream taxonomic curation use sativa  . ",
        "id": 666
    },
    {
        "code": "elnet_cv.fit(X_train,y_train)\nprint(elnet_cv.alpha_)\nridge_cv.fit(X_train,y_train)\nprint(ridge_cv.alpha_)\nlasso_cv.fit(X_train,y_train)\nprint(lasso_cv.alpha_)\nrf.fit(X_train,y_train)\nada.fit(X_train,y_train)\ngbr.fit(X_train,y_train);",
        "text": "fit the model to train data",
        "id": 667
    },
    {
        "code": "test_ids = []\nfor ID in df_test_0['Id'].values:\n    test_ids.append(ID)\ndef write_predictions(predictions, ids, outfile):\n    with open(outfile,\"w+\") as f:\n        \n        f.write(\"Id,Prediction\\n\")\n        for i, history_id in enumerate(ids):\n            f.write(\"%s,%d\\n\" % (history_id, predictions[i]))\nET_Test = ET.predict(X_test)\nwrite_predictions(ET_Test,test_ids,\"perSysCountsET.csv\")\nNN_Test = NN.predict(X_test)\nwrite_predictions(NN_Test,test_ids,\"perSysCountsNN.csv\")\nxgb_Test = xgb.predict(X_test)\nwrite_predictions(xgb_Test,test_ids,\"xgb.csv\")",
        "text": "part iii write to file",
        "id": 668
    },
    {
        "code": "set1 = set([1,2,3,4])\nset2 = set([3,4,5,6])\nset3 = set1.intersection(set2)\nset3\nset1 = set([1,2,3,4])\nset2 = set([3,4,5,6])\nset3 = set1 & set2\nset3",
        "text": "find the intersection of set    python set1 . intersection ( set2 )",
        "id": 669
    },
    {
        "code": "cv = h1.std()/x\nlr = scipy.stats.linregress(x, cv)\nplt.scatter(x, cv, s=2, label='data', alpha=0.3)\nplt.plot(X, lr.intercept + lr.slope*X,\n         label='fit', lw=2, color='k', ls='--')\nplt.legend()\nplt.xlabel('$\\mu$')\nplt.ylabel('$C_v$')\nprint('y = {0:.2g} + {1:.2g}x'.format(lr.intercept, lr.slope))",
        "text": "now let 's plot $ c _ v $ v $ \\mu $ for these healthy patient  . ",
        "id": 670
    },
    {
        "code": "D = np.random.uniform(0,1,5)\nprint(D)\nS = np.random.randint(0,10,5)\nprint(S)\nD_sums = np.bincount(S, weights=D)\nprint(D_sums)\nD_counts = np.bincount(S)\nD_means = D_sums / D_counts\nprint(D_means)",
        "text": "consider a one   -   dimensional vector d , how to compute mean of subset of d use a vector s of same size describe subset index ? (    -  )",
        "id": 671
    },
    {
        "code": "\ndef has_recipe(text_in):\n    try:\n        if \"recipe\" in str(text_in).lower():\n            return 1\n        else:\n            return 0\n    except: \n        return 0\ndata[\"recipe\"] = data[\"title\"].map(has_recipe)\n# data[\"recipe\"] = data[\"title\"].str.contains(\"recipe\")",
        "text": "let 's try extract some of the text content    -  exercise , 6 .  create a feature for the title contain  recipe  be the % of evegreen website high or low on page that have recipe in the the title ?",
        "id": 672
    },
    {
        "code": "df = pd.DataFrame(np.random.randn(10, 4))\ndf\npieces = [df[:3], df[3:7], df[7:]]\npieces\npd.concat(pieces)",
        "text": "merge/concatenate panda provide various facility for easily combine together series , dataframe , and panel object with various kind of set logic for the index and relational algebra functionality in the case of join / merge   -   type operation  .  see the merge section concatenate panda object together with concat ( ) ,",
        "id": 673
    },
    {
        "code": "\ntic = time()\nlogisticRegr = LogisticRegression(solver = 'lbfgs')\nlogisticRegr.fit(components, train_label)\ncomponents_test = pca.transform(test_img)\nscore = logisticRegr.score(components_test, test_label)\ntoc = time()\nprint('The total time is %s seconds' % (toc-tic))\nprint('The classification accuracy is %s ' % score)",
        "text": "fit a logistic regression model to the approximation of the train image with 95 % of explain variance  .  compute the accuracy of the classifier and the time need to train the model  .  compare it to the one obtain in 2 . 3 .  what do you observe ?",
        "id": 674
    },
    {
        "code": "symbol = \"BAM\"\nprint(sdb.merge(trades,\n            tkr.filter(\"symbol='%s'\" % symbol)\n          ).nonempty())\nprint(sdb.merge(quotes,\n            tkr.filter(\"symbol='%s'\" % symbol)\n          ).nonempty())",
        "text": "look up trade by symbol string join with the auxiliary tkr  array to look up data by ticker symbol name  .  here be example that count the number of trade and quote for bam   . ",
        "id": 675
    },
    {
        "code": "\nRo1=allprior[allprior.reordered==1]\nRo1.shape[0]/allprior.shape[0]\nRo0=allprior[allprior.reordered==0]\nRo0.shape[0]/allprior.shape[0]\nRo1.groupby('user_id')['product_name'].value_counts()",
        "text": "what product appear in all customer a order ?   -   these product will have high probability be reorder by customer a allprior dataframe must be use here since alltrain only have 1 order per user with no order history",
        "id": 676
    },
    {
        "code": "def vol(rad):\n    v=(4.0/3.0)*3.14*rad**3\n    return v\nvol(3)",
        "text": "function and method  _  _  _  _    -  write a function that compute the volume of a sphere give it radius   - ",
        "id": 677
    },
    {
        "code": "import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n \nmod = ols('Audience_Review_Score ~ Marvel_DC',\n                data=data).fit()\n                \naov_table = sm.stats.anova_lm(mod, typ=2)\nprint (aov_table)",
        "text": "one   -   way anova part 2",
        "id": 678
    },
    {
        "code": "import pandas as pd\nimport re\nFILENAME = 'dailies.txt'\nDATE_REGEX = r'(\\d+\\/\\d+\\/\\d+)'\nDREAM_REGEX = r'dream\\s*\\((.*)\\)'\nEAT_TIME_REGEX = r'eat\\s*at\\s*(\\d*:*\\d*)\\s*'\nEAT_REGEX = r'[\\(\\s\\,]*(([\\w\\s\\d\\.\\/]*))[\\)\\,]'",
        "text": "daily this notebook be use for process daily journal  . ",
        "id": 679
    },
    {
        "code": "\",  \".join( \"%s = %d\" % (name,val) for name,val in a1.items())",
        "text": "items ( )   -  be return a list contain both the list but each element in the dictionary be inside a tuple  .  this be same a the result that wa obtain when zip function wa use   -   except that the order ha be shuffled  by the dictionary  . ",
        "id": 680
    },
    {
        "code": "df.iloc[df.index.get_loc(stamp, method='ffill')]",
        "text": "find the row near to timestamp that be also before timestamp",
        "id": 681
    },
    {
        "code": "def content_loss(current, computed):\n    _, height, width, number = computed.shape\n    size = height * width * number\n    return tf.sqrt(tf.nn.l2_loss(current - computed) / size)",
        "text": "difference between content layer of result image and content layer of content image  . ",
        "id": 682
    },
    {
        "code": "with open('important_words.json', 'r') as f: \n    important_words = json.load(f)\nimportant_words = [str(s) for s in important_words]\ndef remove_punctuation(text):\n    import string\n    return text.translate(None, string.punctuation) \nproducts['review_clean'] = products['review'].apply(remove_punctuation)\nfor word in important_words:\n    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))",
        "text": "a before , work with a hand   -   curated list of important word extract from the review data  .  also perform 2 simple data transformation , 1 .  remove punctuation use [ python 's build   -   in ] ( <url> ) string manipulation functionality  .  2 .  compute word count ( only for the important _ words )",
        "id": 683
    },
    {
        "code": "already_dl = np.array([f[:len(f)-5] for f in listdir('1.data/user_data') if \\\n                       isfile(join('1./data/user_data',f))])\nalready_dl2 = np.array([f[:len(f)-5] for f in listdir('1.data/user_data2') if \\\n                        isfile(join('1.data/user_data 2', f))])\nall_dls = np.concatenate((already_dl,already_dl2))\nlen(all_dls) == len(already_dl) + len(already_dl2)\nlen(all_dls)",
        "text": "load list of file already download",
        "id": 684
    },
    {
        "code": "import numpy as np\nimport pandas as pd\nfrom pandas_datareader import data as wb\ntickers = ['MSFT', '^GSPC']\ndata = pd.DataFrame()\nfor t in tickers:\n    data[t] = wb.DataReader(t, data_source='yahoo', start='2012-1-1', end='2016-12-31')['Adj Close']  \nsec_returns = np.log( data / data.shift(1) )\ncov = sec_returns.cov() * 250\ncov_with_market = cov.iloc[0,1]\nmarket_var = sec_returns['^GSPC'].var() * 250\nMSFT_beta = cov_with_market / market_var\nMSFT_er = 0.025 + MSFT_beta * 0.05",
        "text": "obtain data for microsoft and s   -   p 500 for the period 1st of january 2012   -   31st of december 2016 from yahoo finance  .  let s   -   p 500 act a the market  .  calculate the beta of microsoft  .    assume a risk   -   free rate of 2 . 5 % and a risk premium of 5 %  .    estimate the expect return of microsoft  . ",
        "id": 685
    },
    {
        "code": "\nmedian_ibus = df.groupby('Style')['IBUs'].mean().sort_values().dropna()\nmedian_ibus.head()\nmedian_ibus.to_frame().head()\nmedian_ibus.plot(kind='barh',x='Style',y='IBUs',figsize=(20,25))",
        "text": "list the median ibus of each type of beer  .  graph it   -  put the high at the top , and the nan one at the bottom  .  i want a nice graph , too   -   do n't let them all be squish together , either  . ",
        "id": 686
    },
    {
        "code": "clf2 = DecisionTreeClassifier(random_state = 0, max_depth = 3).fit(X_train, y_train)\nprint('Accuracy of Decision Tree classifier (max_depth = 3) on test set: {:.2f}'\n     .format(clf2.score(X_test, y_test)))",
        "text": "set max decision tree depth to avoid overfitting",
        "id": 687
    },
    {
        "code": "num = 600851475143\ni = 2\nwhile i * i < num:\n    while num % i == 0:\n        num = num // i\n    i = i + 1\nprint(num)",
        "text": "the prime factor of 13195 be 5 , 7 , 13 and 29 .  what be the large prime factor of the number 600851475143 ?",
        "id": 688
    },
    {
        "code": "diffs = []\nfor _ in range(10000):\n    bootsamp = sample_data.sample(200, replace = True)\n    coff_mean = bootsamp[bootsamp['drinks_coffee'] == True]['height'].mean()\n    nocoff_mean = bootsamp[bootsamp['drinks_coffee'] == False]['height'].mean()\n    diffs.append(coff_mean - nocoff_mean)\nnp.percentile(diffs, 0.5), np.percentile(diffs, 99.5) \n# statistical evidence coffee drinkers are on average taller",
        "text": "for 10,000 iteration , bootstrap sample your sample data , compute the difference in the average height for coffee and non   -   coffee drinker  .  build a 99 % confidence interval use your sample distribution  .  use your interval to start answer the first quiz question below  . ",
        "id": 689
    },
    {
        "code": "fortune = pd.read_csv('fortune1000.csv', index_col = \"Rank\")\nsectors = fortune.groupby(by = \"Sector\")\nfortune.head(3)\nsectors.get_group('Apparel')['Profits'].sum()\nsectors.max()\nsectors.min()\nsectors.sum()\nsectors.mean()\nsectors.get_group('Engineering & Construction')\nsectors['Revenue'].sum()\nsectors['Employees'].sum()\nsectors['Profits'].max()\nsectors['Profits'].min()\nsectors['Employees'].mean()\nsectors[['Revenue', 'Profits']].sum()",
        "text": "method on the groupby object and dataframe column",
        "id": 690
    },
    {
        "code": "def lat_two(dictionary):\n    a = list(dictionary.keys())[0] \n    b = list(dictionary.values())[0] \n    c = list(dictionary.keys())[1] \n    d = list(dictionary.values())[1] \n    n = c - a\n    k = d - b\n    num = n + k\n    denom = n\n    return choose(num, denom)\nlat_two({0 : 0, 5 : 3})",
        "text": "two point lattice path       lat _ two ( dict )     this function will take a dictionary a an argument that will consist of two point  .  the outcome will be the number of lattice path that be possible between the two point  .  the dictionary shall be write in the form   { a , b , c , d }    . ",
        "id": 691
    },
    {
        "code": "\nfrom bokeh.models import CategoricalColorMapper\nsource = ColumnDataSource(df)\ncolor_mapper = CategoricalColorMapper(factors=['Europe', 'Asia', 'US'],\n                                      palette=['red', 'green', 'blue'])\np.circle('weight', 'mpg', source=source,\n            color=dict(field='origin', transform=color_mapper),\n            legend='origin')\noutput_file('colormap.html')\nshow(p)",
        "text": "colormapping color each glyph by a categorical property",
        "id": 692
    },
    {
        "code": "Z = np.arange(50)\nprint(Z)\nZ = Z[::-1]\nprint(Z)",
        "text": "reverse a vector ( first element become last )",
        "id": 693
    },
    {
        "code": "library(dplyr)\nentity_views <- read.table(\"../results/sql_queries/entity_views.tsv\", header=FALSE, sep=\"\\t\")\ncolnames(entity_views) <- c('entity_id','page_views')",
        "text": "statistic for entity page view",
        "id": 694
    },
    {
        "code": "df = df.drop(['id','date', 'lat', 'long','zipcode'], axis =1)\ndf.head()",
        "text": "just drop some irrelevant attribute",
        "id": 695
    },
    {
        "code": "\nmy_dict['k1'] += 100\nmy_dict['k1']\nmy_dict['k1'] *= 2\nmy_dict['k1']",
        "text": "python ha a build   -   in method of do a self subtraction or addition ( or multiplication or division )  .  we could have also use += or    -  for the above statement  . ",
        "id": 696
    },
    {
        "code": "predict_test = test.map(lambda x: (x[0],x[1]))\npredictall_test = model.predictAll(predict_test).map(lambda  x: ((x[0],x[1]),x[2]))",
        "text": "< font color = purple size = 4 >   prediction on test data",
        "id": 697
    },
    {
        "code": "import pandas as pd\nufo = pd.read_csv('http://bit.ly/uforeports')\nufo.head()\nufo.drop('Colors Reported', axis=1, inplace=True)\nufo.head()\nufo.drop(['City', 'State'], axis=1, inplace=True)\nufo.head()\nufo.drop([0, 1], axis=0, inplace=True)\nufo.head()",
        "text": "how do i remove column from a panda dataframe ?",
        "id": 698
    },
    {
        "code": "t = \"Clearly, he has no excuse for such behavior.\"\nfor i in re.finditer(r\"\\w+ly\", t):\n    print('%d-%d: %s' % (i.start(), i.end(), i.group(0)))",
        "text": "find all adverb and their position in a give sentence",
        "id": 699
    },
    {
        "code": "\nresponse = sagemaker.describe_endpoint(EndpointName=endpoint_name)\nstatus = response['EndpointStatus']\nprint('EndpointStatus = {}'.format(status))\nsagemaker.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\nendpoint_response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\nstatus = endpoint_response['EndpointStatus']\nprint('Endpoint creation ended with EndpointStatus = {}'.format(status))\nif status != 'InService':\n    raise Exception('Endpoint creation failed.')\nendpoint_response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\nstatus = endpoint_response['EndpointStatus']\nprint('Endpoint creation ended with EndpointStatus = {}'.format(status))\nif status != 'InService':\n    raise Exception('Endpoint creation failed.')",
        "text": "finally , now the endpoint can be create  .  it may take sometime to create the endpoint   - ",
        "id": 700
    },
    {
        "code": "import ast\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\npd.set_option('display.max_rows', 10000)\nfile_name = 'phi5_auth_sent_data_v3.txt'\ndir_location = '~/Downloads'\nrel_path = os.path.join(dir_location, file_name)\nabs_path = os.path.expanduser(rel_path)\nwith open(abs_path) as f:\n    r = f.read()\nd = ast.literal_eval(r)\ndf = pd.DataFrame(d)\ndf = df.drop('tally_of_sent_word_lengths')",
        "text": "next , i offer some simple view of the file phi5 _ auth _ sent _ data _ v3 . txt  use the data analysis library panda  . ",
        "id": 701
    },
    {
        "code": "\nduplicates = movies[movies.title.duplicated()].title\nduplicates\nmovies[movies.title.isin(duplicates)]\n## note the actors list: despite having the same title, these are different movies.",
        "text": "check if there be multiple movie with the same title , and if so , determine if they be actually duplicate",
        "id": 702
    },
    {
        "code": "color = np.dtype([(\"r\", np.ubyte, 1),\n                  (\"g\", np.ubyte, 1),\n                  (\"b\", np.ubyte, 1),\n                  (\"a\", np.ubyte, 1)])\ncolor",
        "text": "create a custom dtype that describe a color a four unsigned byte ( rgba ) (    -  ) (   -  hint  -  , np . dtype )",
        "id": 703
    },
    {
        "code": "mg.set_intra_edges_weights(layer=0,weight=1)\nmg.set_intra_edges_weights(layer=1,weight=2)\nmg.set_intra_edges_weights(layer=2,weight=3)",
        "text": "set weight to the edge",
        "id": 704
    },
    {
        "code": "import numpy as np\nX = np.arange(11)\nX[(X > 3) & (X <= 8)] *= -1\nX",
        "text": "give a 1d array , negate all element which be between 3 and 8 , in place  .  (    -  )",
        "id": 705
    },
    {
        "code": "\nfor county in df_merged[\"FIPS\"]:\n    rowlist = df_merged[df_merged['FIPS'] == county].index.tolist()\n    syphilis_cases = df_completely_clean[df_completely_clean[\"FIPS\"] == county].Cases.tolist()\n    df_merged.set_value(rowlist[0], 'Cases', syphilis_cases[0])\ndf_merged.head()\ndf_merged.describe()\nplt.scatter(np.log10(df_merged[\"Population\"]), df_merged[\"Cases\"]/df_merged[\"Population\"])\nplt.scatter(df_merged[\"FIPS\"], df_merged[\"Cases\"]/df_merged[\"Population\"])",
        "text": "replace the number of chlamydia case with the number of syphilis case",
        "id": 706
    },
    {
        "code": "response = requests.get('http://api.nytimes.com/svc/search/v2/articlesearch.json?q=hipster&begin_date=19950101&end_date=19951231&sort=oldest&api-key=bfab4def30f54e33ad18f97a4d13fcbc')\nhipster_data = response.json()\nfirst_hipster = hipster_data['response']['docs'][0]\ntitle = first_hipster['headline']['main']\nprint(title)\nfirst_paragrah = first_hipster['lead_paragraph']\nprint(first_paragrah)",
        "text": "what 's the title of the first story to mention the word hipster  in 1995 ? what 's the first paragraph ?",
        "id": 707
    },
    {
        "code": "\nimport pandas as pd\npath = '../data/'\nurl = path + 'train.csv'\ntitanic = pd.read_csv(url, index_col='PassengerId')\ntitanic.head()\ntitanic['female'] = titanic.Sex.map({'male':0, 'female':1})\ntitanic.head()",
        "text": "read the data into a panda dataframe",
        "id": 708
    },
    {
        "code": "percentage = len(bird_dmg)/float(len(bird_safe))\nbird_safe_undersample = bird_safe.sample(frac=percentage,random_state=0)\ndf_balance = bird_dmg.append(bird_safe_undersample)\ndf_balance=df_balance.sample(n=len(df_balance))",
        "text": "one way to combat class imbalance be to undersample the large class until the class distribution be approximately half and half  .  here , we will undersample the large class in order to balance out our dataset  .  this mean we be throw away many data point  . ",
        "id": 709
    },
    {
        "code": "def experiment_fn(run_config, params):\n    feature_cols = [tf.contrib.layers.real_valued_column(\"\",\n        dimension=NUM_FEATURES)]\n    estimator = tf.contrib.learn.DNNClassifier(\n        feature_columns=feature_cols,\n        hidden_units=[512, 256],\n        n_classes=NUM_CLASSES,\n        config=run_config)\n    return tf.contrib.learn.Experiment(\n        estimator=estimator,\n        train_input_fn=train_input_fn,\n        train_steps=NUM_STEPS,\n        eval_input_fn=test_input_fn)",
        "text": "alternatively   -  define experiment a model be wrap in an estimator , which be then wrap in an experiment  .  once you have an experiment , you can run this in a distribute manner on cpu , gpu or tpu  . ",
        "id": 710
    },
    {
        "code": "a5 = np.ones(10) * 5\na5",
        "text": "create an array of 10 five",
        "id": 711
    },
    {
        "code": "c=DecisionTreeClassifier(max_depth=6).fit(X_train,y_train)\nprint(c.score(X_train,y_train))\nprint(c.score(X_test,y_test))\ny_preds = c.predict(X_test)\nprint(classification_report(y_test,y_preds))",
        "text": "so , we can choose max _ depth a 6 or 7",
        "id": 712
    },
    {
        "code": "cabin = pd.DataFrame()\ncabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\ncabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\ncabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\ncabin.head()",
        "text": "extract cabin category information from the cabin number *select the cell below and run it by press the play button   - ",
        "id": 713
    },
    {
        "code": "cv2.circle(img, (447, 63), 63, (0, 0, 255), -1)\nplt.imshow(img);plt.show()",
        "text": "draw circle to draw a circle , you need it center coordinate and radius  .  we will draw a circle inside the rectangle draw above  . ",
        "id": 714
    },
    {
        "code": "((y_odds_Setosa >= 1) == (ys.Setosa == 1)).mean()",
        "text": "activity , now , use just the odds , confirm the model accuracy",
        "id": 715
    },
    {
        "code": "def factors(number):\n    if number <= 0 or number % 1 !=0:\n        print('Not a positive integer. Program stopped.')\n        exit\n    else:\n        factorsList = []\n        \n        from math import sqrt\n        for i in range(1, int(sqrt(abs(number))) + 1):\n            if number % i == 0:\n                factorsList.append(i)\n                \n        import numpy as np\n        factorsList += list(number / np.array(factorsList))\n        factorsList = list(set(factorsList))\n        \n        factorsList = [int(e) for e in factorsList]\n        return sorted(factorsList)\n\ntest = [1, 8, -1, 0.3]\nlist(map(factors, test))",
        "text": "iii   -   2   -   3 .  change your new function  factor  to accept only positive integer value , your code should return a message if an invalid valid wa input   - ",
        "id": 716
    },
    {
        "code": "svd_model = TruncatedSVD(n_components=50, random_state=0)\ntrain_x2 = svd_model.fit_transform(tfidf_train_x2)\ntest_x2 = svd_model.transform(tfidf_test_x)",
        "text": "dimensionality reduction of train and test data use latent semantic index ( lsi )",
        "id": 717
    },
    {
        "code": "dir_ex_coeff_fname = 'results/usALEX - direct excitation coefficient dir_ex_t beta.csv'\ndir_ex_t = np.loadtxt(dir_ex_coeff_fname, ndmin=1)\nprint('Direct excitation coefficient (dir_ex_t):', dir_ex_t)\ngamma_fname = 'results/Multi-spot - gamma factor.csv'\ngammaM = np.loadtxt(gamma_fname, ndmin=1)\nprint('Multispot Gamma Factor (gamma):', gammaM)",
        "text": "load the   -  direct excitation coefficient  -  ( $ d _  { dirt } $ ) from disk ( compute in [ usalex   -   correction   -   direct excitation physical parameter ] ( usalex   -   correction   -   direct excitation physical parameter . ipynb ) ) ,",
        "id": 718
    },
    {
        "code": "a = np.arange(15)\nnp.set_printoptions(threshold=np.nan)\na",
        "text": "< span style=  color , red  > 24 . how to print the full numpy array without truncate ?   q .  print the full numpy array a without truncate  .  如何在不截断数组的前提下打印出完整的numpy数组?",
        "id": 719
    },
    {
        "code": "print(type(42)) \nprint(type(42.0)) \nprint(type('42.0')) \nprint(type(\"42.0\")) \nprint(type(\"\"\"42.0\"\"\")) \nprint(type([1, 2])) \nprint(type([1] + [2])) \nprint(type(1 + 2)) \nprint(type(print)) # built in function",
        "text": "question   -   12 ,   -  explain the result of each line  . ",
        "id": 720
    },
    {
        "code": "import cy_math\nimport numpy as np\nprint(cy_math.py_plus(3, 4))\nprint(cy_math.py_mult(3, 4))\nprint(cy_math.py_square(3))\nxs = np.arange(10, dtype='float')\nprint(cy_math.py_sum(xs))",
        "text": "use the extension module in python",
        "id": 721
    },
    {
        "code": "thinkplot.Scatter(heights, 10**weights, alpha=0.01)\nfxs, fys = thinkstats2.FitLine(heights, inter, slope)\nthinkplot.Plot(fxs, 10**fys)\nthinkplot.Config(xlabel='height (cm)', ylabel='weight (kg)', legend=False)",
        "text": "make the same plot but apply the inverse transform to show weight on a linear ( not log ) scale  . ",
        "id": 722
    },
    {
        "code": "A = array([[1, 2], [3, 4]])\nA\nB = A\nB[0,0] = 10\nB\nA",
        "text": "copy versus  deep copy     -  to achieve high performance , assignment in python usually do not copy the underlay object  .  this be important for example when object be pass between function , to avoid an excessive amount of memory copy when it be not necessary ( technical term , pas by reference )  . ",
        "id": 723
    },
    {
        "code": "log2 = LogisticRegression(penalty='l2')\ngrid_searchl2cv5 = GridSearchCV(estimator = log2, param_grid = hparameters, cv=5, verbose = 1,\n                           n_jobs = -1, return_train_score = True)\ngrid_searchl2cv5.fit(X_train, y_train)\ngrid_searchl2cv5.best_estimator_\ngrid_searchl2cv5.best_params_\ngrid_searchl2cv5.best_score_",
        "text": "gridsearchcv for different value of c , cross validation ( cv ) = 5 , penalty=l2",
        "id": 724
    },
    {
        "code": "vcov_u = s2_u * np.linalg.inv(X1.T.dot(X1))\nvcov_u.round(2)",
        "text": "ols variance   -   covariance matrix of coefficient",
        "id": 725
    },
    {
        "code": "words = []\nfor dataset in [\"positive_tweets.json\", \"negative_tweets.json\"]:\n    for tweet in twitter_samples.tokenized(dataset):\n        words.extend(tweet)\nlen(words)",
        "text": "create a list of all word before perform sentiment analysis , let 's first inspect the dataset a little bite more by create a list of all word  . ",
        "id": 726
    },
    {
        "code": "data\ndata.set_index('nation');\ndata[['nation','year','population']]",
        "text": "notice the dataframe  be sort by column name  .  we can change the order by index them in the order we desire by pass in a list of column name ,",
        "id": 727
    },
    {
        "code": "def filter_words(word_list, letter):\n    return filter(lambda x: x[0]==letter, word_list)\nl = ['hello','are','cat','dog','ham','hi','go','to','heart']\nfilter_words(l,'h')",
        "text": "use filter to return the word from a list of word which start with a target letter  . ",
        "id": 728
    },
    {
        "code": "from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_images, \n                                                  train_labels, \n                                                  test_size=0.1, \n                                                  random_state=42)\nprint('Training features: ' + str(len(X_train)))\nprint('Training labels: ' + str(len(y_train)))\nprint('Validation features: ' + str(len(X_val)))\nprint('Validation labels: ' + str(len(y_val)))",
        "text": "split to train and validation",
        "id": 729
    },
    {
        "code": "p_diffs = []\nfor _ in range(10000):\n    new_page_converted = np.random.binomial(145311, pnew)\n    old_page_converted = np.random.binomial(145274, pold)\n    diffs = (new_page_converted / 145311) - (old_page_converted / 145274)\n    p_diffs.append(diffs)",
        "text": "simulate 10,000 $ p _  { new }   -  p _  { old } $ value use this same process similarly to the one you calculate in part   -  a .  through g  -  above  .  store all 10,000 value in   -  p _ diffs  - ",
        "id": 730
    },
    {
        "code": "\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(units=16, \n                         activation='relu', \n                         kernel_regularizer=regularizers.l2(0.01),\n                         input_shape=(number_of_features,)))\nnetwork.add(layers.Dense(units=16, \n                         kernel_regularizer=regularizers.l2(0.01),\n                         activation='relu'))",
        "text": "create neural network architecture with weight regularization in kera , we can add a weight regularization by include use include kernel _ regularizer=regularizers . l2 ( 0 . 01 )  a late  .  in this example ,  0 . 01  determine how much we penalize high parameter value  . ",
        "id": 731
    },
    {
        "code": "def allApproxOccurenceOfPatter(pattern, genome, d):\n    lenPattern = len(pattern)\n    res = []\n    for i in range(0, len(genome)-lenPattern+1):\n        tempPattern = genome[i : i+lenPattern]\n        if(hammingDistance(tempPattern, pattern) <= d):\n            res.append(i)\n    return res\nresult = allApproxOccurenceOfPatter(\"GTTCGGTGTC\", \"ATGAAAATCACATGCCTCAGGACCCCTCCAATTTAGGCGAATTCAGCTTGTTTAAATATTCAGCTCGATAAGACATTAGCAATTTAATTGGGAGAGCCCATGTCTTGGCGCGCCAAAAAGCTCGGGTCGGACAGAAACCTCTGACCTAGCAACAATATAAATGGATCTCCGAGTACACAATCAGCAACACCTCTTTGCTACGCCGAACATTCTTCTTGACTACGTCATTGTTCTTCCACGGGGCAATCGTTGCGTGGGTCATTTGTCCTAAGCCAAAGAAAACAGTACGGTAGATGTGGGGTATAGCGAATCCATAGCATATTAAACAACGGTGGACAGCCACTATGAGGTGGTTTAAAAAGATCGCATTCTGAGCCCGAATTATCCCAATTCACATTCAGCCGAAGGTCTCAGAACAATGACGCATCCAGGCGTCAGTATGCGTTCTATTACCCGTCCTGGCGCACGAATGTACTCATTCGCGCTCTGTGGTACTAGGCGTAACAGGCAAATTAACAAGTAGGGGACCCGTACAATACTGCTTCAGAGAGAAGGAGCCGGCCCTTGCTCATCTAATGGTACAATTATAAGTTGTAGTGCGGTGGTGACTATTTGAGACCCGGCTATCCGTCAGGCCACAGCCATCTTATCGCTCTCGCTATTAAGTCCTTGTTAAACCCTAAACGACTCTGATGCATAGAGTGAGGGCTATGCAGCAGTGTCGTATTAGCTGACGCTTCGAACGGATCTGTGGTCGTTTTTATGGGGCCTGAAATCTGGAGGTTATACTGCACAATATCCCTTGAATGCTTGCTCGATCAATGCAGCTACACCGAGCATAGCCGGAGCCATCTACCTCTCCATCGTAAAGAGATTTCAGAGTATGCCTTGCAACCCTTAGCATAGGATGAGTCGAAGAGTTTATGGCTCCCATTCAACTACTACTTACTGAAGGCTACCAGGTGAGGTGAAAATACCTAGTTCGACGAAACCGGCGTAGCGCTGAAGCATGGGAATTCGGTCAGCGGACAAAACATGGACAGGGCCGGCTACTGGCAGTACGGGGTAAGAATGTTCAACGACCGGTTCAAGGTGACATCTTAGCCTGTTCTGTGAGAACTCACTGATGTACAAAGGCCAGGCATGTCGGCATTTACGCCATAACCTTGACCTCCAAGGAAGAGTGGCGGCCACACTCCGGGCAACGTACGGCTCCGTGGCGAACGCGTAAGCTGCGGGGGAATCCTCCCAGAGAAAAGAGCCTTTGCCCGGCTAGAAGCGTAACTTGGCCCGAGGAGTCCTCTCATACGAGGTGAACGTACGCAGGACTCGCCACTGGAACTCCTGGATAAGAGCTTACGGGACCTATTAAGAGTGTTACGTTAATCGTTGTAGATGTTGCTGGAAGTTAAATCCAGGGATCTTGGCCCTCAATAGCCGTTAAGGATCTCCTTAGCGTCGCGACATATCGCTTCCGCCTCCCCCTGTGTCAGGTATCGGACATATAACCCTCCGACGGCAAATCACTAGAGATTTAGGTGAACACCACGCTCTCGGCTTTGTAGGTTATGTTGCCAATGGGGGAGTATTCGGTAAGAGAGCGCTCTCTTTCTCATGCTGAAGTGGCTCAACTGAATTAGCTGGTACTAAACCGCCGATTACAATAAATCGCGCGGCTTACCGCTTGGGAGTAGAGCGACGAGAGTCCCCACCACGGAGAAGCACTTAGTGGGAGAGACCAAACGAGAATGCAGTCGTTCTACCGAAGGCTTATGGGTCCGGGGTTCATAATCTGATGGCGGCATATGACCTCCTCCAAGGGGATCTGTGTGCCTAGACTTTCTACTATTATGTATGTCAGACCGTATTCGCCTGATTTCGGTCACGCGGCTCAAGCCCAAGGGGGGTAGGCTATACACATGGAAGATTCGCGGGATTTTAAGTACACTCTAGACATTCCGCGCGTACCGGCTAATGCCATGCCAGGTTAGGCTGGTCAACGTTAGGGTATGCTCCATATACGGATTCAAACGCCATTATATCTTGGGACTAATGGCTCTTGCAGAATCAGTTACTAGGGGCCTCCGTCAAATGCCCTGATGTTCTCACCTTCGGCGACCCGACAATGGACAACAAATCCCACGCCCAGCCCAAGCTCGCAACCGACACAGTAGGTCGTGCTCCAGTAGTATCCCGCATGCGTTACTTATACGCGACTTAAGAATAATGGTGCGTATTCCAGTTTTACTTGGATTTGTCATTCTTGTCCCAATCACAATTTCGTAAATCGTGTCCCTAAGTCCCGTATGTGCTTAATGAAGATAAATTTGAATGAGGTGAACGGATCACCCTCCTGCTTTTTAGAGCAGGGCCCAACGCAAGATAGCTTAGATTGCAAAACTACTCTCCACTACTGGAGCTGGATGGTCCATTCAGATGGTCCGTACGTCGCAAATCAGCGGTTTAACCGTGATCCGACTGATAAGGCCGACGGAGAACAGCTTGTATGCGAACATACGTGCGGGTTACCATAAACCAGAGTGCGTGGGAAGATTCGACTCGCAATGCCTCATGGTAGGAAGGCCAGGCGCGCTAACCGGCGCGGACGTAACCGGGTTACGAATTCGAAGATACTCAATGTTATCTGGCATGCTAAGTATCAATACTCCCCTCGCCCGTATTTGCACCCGATTACTGTTCCTTCTGCCGCAAATTCCCCATTACGCCAAAAGCTTGTACCTCGTACTAAATGTATGGCAGCTAGGCAGGATATATCGGATAGCTACTAACTCGACTTGTGGATTGTCGAGTGACCATCGACCAACGCGCAAACCCCTATGATTAGCGTCAGTGGAGCTACTATCCCTCAAGCAGACTTCTTCCTTTCGGTTCACTACCTTTGTCGTTTCGTCGGACATGGGAACTCGACCCAGACAAATGAACGTATAGGGGGCTTCGGCTCGTCCAGTCTAGTCTTGTAGAAGACACGTGACTGGATGATATAATAGGGCCCCAAGTACGAGCGGCTCACCTGCTGCGGCCGGCAAAGACCGTCGATCAACCCGTTAGAGTGCTGATACTGTTGCATCCCATGTAGACACTGAAGCCCGCTTGTGCATGACGGCACTTCTATGCGGACTGCGTTCGAGAGTATCAAACGGGGCCCCTTCATGGGCCTGTAATCCGGGCCATTCCATGTCTCTCTCCACCAAGTCCCCAACTGGTAATTCCGCCTTTCCTGGCGACTACTTATCCGTTGTTTACGTAGGCAACGGCGGATATCGAAGAAAAACTACGAACCCCCGGATCAGGGCAATGGTAATACGAGCTGAACGGGAGGTCTCTTCTCGCCGTAACCACGCAAGTTCTGGTAATAGCCGGTTACTGTGTAGAGCTGCTCTTGGCGTGCCAACTTCGTGATAGGCACAAAAGTCGGGCCCTACGCTAAAGCGACTGGCAGGGCAAGCGATCTTTGATCCTTGCAGAGGAAGATAATTTTGAAAGGTCCGAGTATGGATGCTTTGACGAAGCCCGTGTTGGCGCAAAGGGCTATCTCACTAGACTTCAGCTCTGAGTTCAGCTACTACTGTGGTCGCGGACGTTGTGTCAACCCCGAGATTGAAGTCGTAACGCTCTCCATTAGGTCCGCGTGACACGTATCCATGGGGACGCCATGAGGGGACGACAGCTGCAACCGAATTAGGTGAGCTTCGCGGTAACCAACTGCTAAAGCCTCGAACAGCCATGTGGTCGCCAGTTTGCGAGCCAATCTATATTACACTGGCCTATCCCGCCCCTGGGAGAAGGTAATGGAGGCCACTTATATGTAGTTAATAGTAATTACATCGCTTCGACGCTATCAACTGTGGTAAGCACCCATTAGTTCAGCATAAGCTCCATCCCCGAGTGGGGAGCATAATTAAGGGAGGGGCGTCGCTAACGATACCCATGCCTTACTCCTTCTCCCGAAGATGAAGAACGTCTGATGATACCATCCTATCCGGGTCTTTGATGGGCTACCATGCTCTATATACTCTGAGATGCGTGTGGCGTCGTTAAACGATCCTACGGACGGTTGTAGCAGCGCTGTGATACACACGATATAGATCACGCCAACGTGGCAAGATTTCGAGTTTAGGTCAGGCCACGTCACATCCCTCACCCATGTGGAGCTCTGTTGATGGGCTAGATATTCCGTAACAGGGACGACTGGGCTCTGCTTCGCATGTGAGAGTAGAGCATCGAGTCAACGGTCATGAATACGGACATTCTATATTTCCGAAGGAAGGCAATTGTGAGGAGAGCGCCGTGATCTAGGGCGTATAGTCTTCGCAGTGTTCTACTCCCTGTGCCCCCCCTCCCGCCGCGATGAGTGGTGTTGTGTTCAGAAGCACCTGGAGCTATGGTGTCCGGGGGGCTAAACACGGTTAGCAGTAGGAGTCCCATGTCGGACAATGTAGTAGGGGTTAGTTCGGTGAGTTCTCTGCAAAGTATGCCAGCGCAATGTGGTCTGTGAATCGGAGTCTGTCCTACCCTATGCCGCCGTCCGCCAACAGAGTGGTTCCCCGCCGGGACGAAGTGTAGGCTCGTTAAGATCAGCTGGACTAAGCTAGAAACTCCGTGCGACATGACTACCGCACAAAGGCGTGCGAAGCATGTCTCCCATGACAACTCTGCCCGCTAGCCAATTACTCACTAAATCTAACAGAGCTGGTAGGGAAGCTAGGCGACTCAAATGAATGTGCTGGATACCCCCTTAAGGTAGGAATTCGCTAGACTTTTCCCTGCGGTAAGCCTGCCCTGCCCAATTGTCTGGAATGTGTGAACAGCTCAACCGGTTCTTGACACTTGGTCTCTAATATGCTGCCAGAGTCTTTAACTAGTATGAGCTCAACGGGGGGCATTCCACGGTGCCTAAAATCCTAAAATGTGATGATCTGGAAATTTGTATACTGTCGCTGGTGGGATTCAATAGCAAGACAAAGGCGTTTGGACACCGGGATGTATAATTCACTCCGGAAGAGTGCCGGCTTGATCCATGCGGTTGGGTTCCACTCCGAACTTAATGAACGTAAATTGAGGGGATACTAATAGGATAATACTGTGTGCAAGTCGGCCTTATCTTAAACGGTGCTCATCATTTCATATTGAGGTGAGTGCATACGAGGCCTGCCAAATGAACCTTGCCCAACGCTCAGTTGACTGTCCGATCTGCTCCTTAGGTCGCCGAAATCCGGTTTAGATGGATGCAGGCCCCGTAACCGGACTATAGTTATGGTGCTCTCTGTGGCCTTTTGCTGCTTATCGCATGTAGCATGTAGTTTCTTCCTTCAGTATGTTAATTCGGCATATTCTCATGAGCGTGGTTGAGAGAAAATTGCTTCATTTGCATTATTTAGGGTCAGGGCCACAGACGGTTATGTCGCCGGTCTGGTTCGATCCGCCACCCAGCATCTGATCGTATAATCAAGACGTAACAAAACGAAGACCCTGCAAGCAGTCGATATAGCGACATAATCCCAGACGAAATGGCCCACCGTGGGCCTGCTGCTCTCTGAGCTGGACTTATTACGATTGTCAGTTGGTTCCTACTGTGGCATGGTTTGGATGCTCGAAAGATGGATTGCATTAATTATGCAACTGCTGAGTTTCGAGAAGTTGCAGAGTGAGTACGTTACGGTTCGGTACCAGGCCCACGCCAAATTAATCCCTGGTGCGGAGTACCTAGTCGCGGGCATACTTAGCGCTTGTTGAAGCAGAATCACAGCGTAAGCACGACAGGGCCGTATCGACGTTCGTGGACGTGTAGAGTCAGCTGACACTAGCGAAATGACTCGCCATTTGTAAGTTTCTCAGCGTAGCTTCATATTTTTGCTTGTGGTTAGCCCTCTTATAAAGAGGTACCGTCTAGAGCACGCACAAGGGTAGGCTGAGCCCACGAATCGCATATATTACGGGAACGCTTAAGATGAACTGACTTCTCCGTAACGAATTAATGCTTACCATGACGAGGCGGTCGTTGGTTTATATCACATCGCTCAGCCTGGTGCTCCTGCATGCGTTCTGAATGAACCTGGTGCATGTCAAGCCCCAAATCGGCCAAAAACTAGGAGAGGGGAGCTTTGATATAGTTGGACTTGTTCCAGTACCGACGTAGAACGAACGAACCAACACAACGGAGACACCTGAAGGGCCACAAGCAATGTGGTTGGATCTCATTGAGTTATTACAAAGCTCCAAATAGTAACTTGACCATAGTCCCTAGAGCAATACTGTACCCTACCCAGGTCTCTGACCCTATCTGATCAGTCTGCTAGAATTTCTTCATGCTCAGGCTTGAACAGTTTAATCATTAAAAAATCCTTGGCACCAGCCCTTTTGACCGAAGCGGCCTCTGAAAACACCAAACCAGTACAGAAACGAATGCTAAACGCGATCATGGAACTTCCCTAGCACGAATCCCGTTTATCTATCGAGTCATTGCTTAGCCTCCGGATTAGGTGGAAGGGCGTCTTTAAGGTGCGTCCTTGATCTCCCCGGGACAGTGGTACGCAGCAACGTGCTGTTGAAAGAGGCCACAATATAGACGTGGAGGTAAGCGCAGTACAAAAGTTCTCCATCCACCGTATTACCACAACGGAGGCGTAATTAGACTTCCCTCATATGTGTATACACAGAGCTTGACGTACGAACGTTCACTGCGGCCAGAACAGAGTAGCATTATGCTCAGTTAGATGCCACGGCTGGAACTAGCACGACTACGCCGCTTAATCGCTCACGCATCATAGTAAAAAGGTTACAGCCGGTCAAGGGAGAAGAAGGGGCCGACTCGGCAGCGCGGGAGCATTATTTGGCTTGTATACACAAAACAGCCGGACATGGCGCAACCGCTCTAAATATAAAGTACGGGTTAGCCTTCTGTATCCCTATTACGAAGCAAGGCTTGTGAAACTGTGGGCCGATTTATCGAGTTCAGCAGTTGTCTCTCGATGTTGTTATTGGAATCGAAGCGAAACGGATTGACCGTTTCCTGATTCACGTTAAGAGAGCAGCTCTTGGTGGGCAGTAGCAAGATTCGGTTTAATCATACTCCGACACGGGTTACCCCATGTACGGGCGGGGATAAAAGATAGTTAGTCATGAACAGTACAATGGTGTATCCCACAGCGCAGGACCACCGGGGAGGCCCCCTCGATGGACTCCCCAACAATGCTTGGCATAACATTTATGCCGTTCTAGGCCGTGCCGTTGAGTGGTTCATGACCGCGAGCCAGTATCCAGCGAACGAAAGTTTCGCCGATGCGCAACCAAACATACAGAGCTGATCCCTTAATTGGTGCCCAATACATGATATGAAACATCTGTAGAATTGGAGGTTCAGGACTAGTGGATCACAACATTGGGTCCAATGCTGTCCTAATTCCACGCCATGATAAACAGCGACGGTGAGAGACGCTGAGTGGACGTAATGGTCATAGCGAAATTAAGCCAAGGCTGAGGAAACCAAGAAGTGTCTTCGTTTGGAACCATTTGCGTAGTCCTTGAAAGGTCTTGCGAGATTATACCGGGATCTCATAGAGTTACTAAAACGATAATTTACCCATAAACAGTGTAGCAACAATCACTTGGAGGGCGGCGATTGAGTCGACGTAGGGACCGTCGGCTGGAGCTGCGCAGGACTATTATCCGACAGTGCTGATCGGATTTGTGACCGGTCTCCTCTCGTGGTCCGTAGAGCGTCACCGGATGCTGCCATTTGGCCGGCGCCTCTCACCATCGGGCTCCCACAAGGCTAAAAAGTGGGGGATCGTGTGCCTTTCGCCCTGACCATTATACGTAACGTAACTTGTCGCCACCTTCATTACGCCGTCAAGCTTCTTTTTATCTCACTGTCCCGCCGGCATAACACAGTGTCTTTCAACTACTTGATCAACCTTCCTTCATTCGATTGCAACCCAGATTCCCCTGGCGCTTTTCCTATCTACGGTGAGAAACGGGGGTGAGACTCTGTGTAGACTAATACGTCTAATGTGCTTAGCGTCTACGAGGGCACCAATAGAATCAATGAGAGGCCTCTGATTCTCCTATTCACTCGCACCACACCTTCTCAAAAGCATAAATCCGTGGATCCTATAATTCAAGCGCGGTTCCATGGGTACTGGAGCTGCGCAACAGGCTCCGCCGCCTCGTCCTGCGTACAGACTCAAACTTGCTAAATGAGTCCGCAAGGCGCAAATGGTGCCAGGTGGTTATGTCTTTGGTCAATCGACCTGTTAAGTGAATACTTTCTTGTCCGACTATAGCTCCTGGTACCGTTCGTTTCAGCCGCACTTATTCGAACAGGTCGACCCAGGGCGAAAGAGTCTTATCAAGAGTAAGTGCCAGTCCTAATCTTGCTTCACAGCACGCCGGAACTCAGTTAGGTAATCGAACCAGACCGGGGGGACTGCGCTAGATTCCGGACAAATTAATACATACGTAGTGTGTTTAACCAAGGGGGGCGACTGCGCCGGTCATTCGGGCAATTATTTGCGCGACACCAGTATATCTGAATTCGCATGATTGACCCTATCCTTGCCTTAGACCGGCACGGTGCGTCTCAAACAGCCTAAAACGCCATACTCCGAGAACCATGCCACGACTCATAGTCCAGAGATTGAGCTTGTGCGCCTCTGGCCGTAGCAGCTGATTGCAGATCTATTTGTAATACAATCCGAATAATTCCCGGCATTTTTAGGTTTTAGTTACCAGGATTACAACACCAACTTAGTCATTTTTGCGTTGACATGGGGCGAACAACAGGGACGACCGTTCCGACCAATACGGTTGGGAAAACGGTAACCTGCTCCCTAGCCGCCGTATGCAGTGGTCTAAAACTGCTGGTTCATGTCCCTATATGAATTATGCTAAAGGATCGCAACCCACATTTTAGGCAAGGATGAGATAAAAGCGCCTCACGCTAAACCTAGCATGAACGAGTAGAGGCGATAATCACATGCCCCCTTAAGAGGCGCAGACTAAGATCCAGAAACCTTTAAGTGTGTCCGCAGCTGGAAGATTACCGAGGATGCTGAAAGTATTAGTGTTGACCCTATGTACATATGCTGCTGAGTCCTATTCTGGAGTTATATCTTCTTATTGGGATCACGATGTAGGTATGAAAGCGTTTCGATAACTGGAATAGCCTTGAGAGGTAAGGAAGCTCCGATGGTTGACGCACCATGTTAATGATTGTCTACGGGCACATCGCCTGGTCATCCGAGAATGGACGGTGCCACGGGCCGTGTGTATGCGTAAGGAGCTTAAATGTCCAGGGATTCTTACATATGCGTCACTATTCACGGCGGCCATATAGATGCCGGAGCAGTTGTTAGCTACGGAGCCAGTCGGTCTGACTCCTTTCTGAGAATGATAAGTTCTCTAAGCAAGATCCCTATGGCGTCATTCGTTCCTCGGAAGCTTCAATGTGGATTCATCTTAACACCTTTGTGCTGCGACTCGGAGCCCTATGTGCATACTTAGGCAAAGGATTCACGAGATGAAATTTACAGACCTCTTCGCCTTCTAAATCCAGAGGCATCCGCTTATCACGCCGGCGGCGACTGGTAAACTTACGTTGTCCAGGTATCCTCAAATCGGTAGGGATGTTTCTGTGGTACGCCTACATACTATGAACCGACTTCTAGATTCTGCAAAGGTTGCTTCCATTAGCGATCGACGCGGGCGCGCCTTCCAGGGCAGTGCGTACGCCGGGAGCCCATAACGCCAGGAGTGACCCCATGGCCGACCATTAGGTGCGACGTGACCGCACCCTCAAGATACCTCCACGGCTCTCTAATGTTAGTCTCGGTTGCCTGGAGATGTTGAACCACGAACACGCGCCCGGTTTGCTCTGTCCACAGTTTATGATCAATCTGACATACTGGCAACCAGAACACGACACTCCTCGACTCGCGCCCGAGCGCTAATTTACGCGAATCGACTAGTGTACGTGTTCAAAATCCACTCATGTCGTGCTTGTCCATGGAAATCAGTAGTTCAAGTTGCGGATCGGTCCACCTACACGGACGGGTCTATATCGGAATACCCAACCGATAGATCTAAATTATCTCCTGTTAGACAATCAGAATCCCACGCTATAAGTGCGGTTAACCTGCGAGATGTTTCGAGTCACTTATTGGCCTCCTGGTAGTAGTGATTAAAAAATAAACAAACTAATCCAATCAAGTTAAGACACAATTAATCAGCGGCATGATATCCACGACGGTTCTGGAGTGCGAAGAGCGGATTTAGAAATCCGTGGAGAATGACGCCACACTAGTGCATAAAAACTAAACACCACAACAGTGTAGCTTGTCAAGTTCCTACACAACACTATGGGAACTGTATGGAGCACAAATTGATGAGGCGCGACGGGGAAGCGTCAAGTATGCAAATTGTGGCGTCTCTTTCCCCCCCGGAATTGTTTAAACCAGATGCTATCACACGACATGTCTCGTTGGATACATTATCTAGACTCCTCGTCGAAGCTAGTCGACCGATGCAGATATTAAATCCGTACAGATGATCGGACCCACTTCGTTGGCATAGAATTCTTCATGCCTGAATGCGAGAGAGCCCCTTTCCGAAATAAGCTACTTGGCTCAAATTGGACGTTGAGTACATGAAGTGGAAATGCCGCGGATTTCCCTAAAACTTGCCGCGGTGGATCTGTCGACATCTGTTGGGATTAATTTAATTTCGCCCATTAATGGGAGGCGCCTGAAACACTCCTGGCTGAGTGACGTGTCATATCATAGCCAAAGAGGGCCACTATCAATCCGCGGATCACATTGCAACTCGGCTATACTTTTAGCAGGGACCTCCATCACATATTACTCCGCACAAACATGTTTCATACACAAGTCTGCACCCGACAAGCACTGGATAAATCTAGAGTATGGTGTGGGTCACCGAACATACGCACCCGAGGCAGACAACAAAGATCCTCACGACCTACCACCGATTTCCTCGTCAAAATGACAGGGTGCTTCCCGAGGGGTACAAAGGACCGACTCAGCCTCCTCAGCGCTCTCTGGCTCAGATGAGATTAAAGTGCCGCTCAAACGTACTCCTAAAGACAGACACATGCTGTGCTTAGTTAAGTAGTCTGGATGTGGATCAAGATGCGAACTCCGACTACAATCATTCGACTACCGTCGTCTGCGCACGAAGTAGGTGGACGCCTAGACTCGCCGCGCCCGGTTGCATTTTCGCCTAGACCGAGCGCCTCCTCTAAGCCCCGTCGATAACCATTCTTGGAACTGGGTCTCCAAATTGCAAGCTATCAGGCAATGAGTCGCTCGACTGTGATTCTTATGTCTACCGAGCCCCCTTTTCCAATCAAACTGATACTCTTGTCCGCCAACGCTAGGTTACCTCATGCCATGTAAACGTCTGGACCGTGAGGCGGTCTTTGATTCAACTAACAACCGACAAATATAGAAGACAAGTAACGATTTTCAGGCTTCGGCACACTGGCTGCATTACCTGCTATTCCACTTTGAGCGAGCCGACCCACGTTATGGTACACGATAACTCATGACTTCCATGGATCGTGTATAGGTTTCCACCAACCGAGAAAAGGGCGAGGACCCGTCCCTGAGCCAATCTTACAAGTGACCTATCGCCCGAATCCCCCTTGCACGTCAACTAGAAGCGCTGTGGAATCTATGAAACACTAGATGACCGCCGAAGACGTTATCCGCTCATGACGGATGGCAATGACCACAAGGGCGACTCCGGCAGGGTCTTGCTTGTGAATAGAGACTTAGGTCTGCTAGCTCCGTATGTTATGGACGCTTTTTTCTTCGTAGCATACGTGCTTCCTATGTTAGGGACCTACTGCCTTAGCCATATGTCTGAAAATCGCAACCCAGCGGGCTGACATGTACTATATTAGGTGCCAGATAAGGTTCGTGGTGTAAAGGGATGAAGATAGTGGCAATCGGCACAATTAGATCATTCCCGTTATCAATCATCTCTGTTCTGCCTGGATCGTTGGTAAAGTGATAATGCTTGAACCCAATCATAGCATGACACTTACCAGCCATTCCCCCCCAGTATACTAAAGCAACCAATAGCGATGCCATATCATTATGCAGAAAACAATTAAAAGGATGCAAAGTTATTTCTTCGCAAAGTGGCCCTGAATTATCCCTCATCGGTGTGATGGCAAACAGTTTTAAGTCTAATACAACAACGGTTGTGCATCGTTATGGGAATCGAATCCATAGCCGTAATCTGACGGCGTCTCCGGGTATTGAAGTCATGGCTCCGAAACAGGTGCTTCCTACTGTAGCGGCATGGGGACGTACCGAGTTTGCGGACCCCACTAGTTCGTTTGTTACGAAAACGGACCGTTCCAGAATGCATACCGGAGCATAACACGATCTTACGGGGTATGTACAATACAACACTTTCCAACGCTAGCGTACCTTCCCCGTAACGGTATAGGATGCTCGTCCGGTCCTGAGTCATCCTGAAGTCTTGAATCAAGGGTGCTGTACATGTGACATGCAACCACAGTCTCTCTCCAAAACGTAGTCCAGTAACGCGAAGCTCGGGCCTCGCCGTTCTGGGATTATCGGCAGTGTTCTACGCGAATAGCCGTCTACCCGACATACTATGAGAGGGCATCCATAGTTAGATCCAAAAACTGTTAACCCAATTCCCTCCCGACGCTTTGGCATTGGATGGGATTTAACCTGGACGGTCTCTAAAATATGAATCGTACTATCCACGTGCCCTACCGGAAAATGTTAAGTGAAAAGGCGGGAGAATCGGCCATAGCGCTTGATCTGGCATACCGGGACGTTAAGACGCCGTCAGATCCGACAGACTGCGATGGGGCAGTGTGAGTCCGCTGTTAAAGTCTTGTGGTCATTCTATGATTACAAGTACAGTTCAAAACTTTACGAGCGTTCGTGTTTTTTTTGAACACACTTAGGTTTTGTCAACTTGATGACCCTGCGCGATTCTTTCATATCACCGTAATGTATAGAGGTCTCCCTGGAGGTAAGGCATATTCGTTACCCACTTGTCCATCCTGAAAAGTAACGAATGCCACATGATGAACGAATCTGCTCAGCTGACTTACAGTCGGGAATCATGTTTGCTTCTCGCGGCCCGGTACCACTCCTATAGTAGGGCACGTGGCCAGCTTTCGTGAAGGCCACTCCACTTCGCGCGCCGTGTGGTGGATTCTGTCACTAAAGAATCGAATAGCGAGACAGCGCTACAATAAGCTACCCTGTGTCCTCAACACATGGATTGCTGCCACTCTATTTTGATTGGGTAAGCTCAAGCGGGATGTTACAGCCCTTTCCAGTCCCGACGCCTGGACATCAAAGCGACCTCGGGGTATGCAATTTCTACCTGATGTTGGCTGACGGTACGAATGCTGTTACCGTCGCATAAACGCTCGTTCTTGATCACATCGACGACACTAGCGGACACCTGTTCAATGCGAGGTAATTCGCAGAAGAGCCTACGGAAAGTCGCAACAGTATTCACCCTGTAGGGTATTTCATCATACCACGGGTATGTTACTTCCATTTTCTACCGTGAGGAGAACTGGATCAGCGAATTGGGTACTGACCCTTGTTAGATCGCCGGATGTACCAGGGTTGTGTGGGCGCTCCTCTGCAGGAAAAGGACCGAGGCTCAATGGAGAACTGGACGGTGGGTCCGATAGCTGAAGCATGAGTATATCAAAAGTCGCATGGTAGCTCCAGCACTTACGACTCAGGGCCAACTCCAGCATCTCTTTGTTAGGACTGTTCTGGTTAGGTACGCGGTGTACACCCTTTTACCGGGTTTGACAGGAACTTTCCCTGAATGCTCGAACATGACTTTTCGGCCTTTGAGTCGACCCAGGGGATGAGGGCAGGTTGTACATGCGAATGTGTCAGTCGCGACAGAACTCTTTAGACAAAACCGGGAGTTATCCTCATGGGCAAGAGGAAAGTGCACTAGTGTAGATCAAGCTTTGATCGGTACCTTACACCTCAGGCTGGTACCGAGTGCCTACCCGCTAGGCCTGCACCGATGATCGAGGTCGTCGTCACAGGGAGCCGTCACGGTTCCAAGAGGGAGACAAGTCTTGTGGCATTATCCTGGCGAGATCATCACGACGTCTGCAGACAGCATAATCCCCACTCAATGGATGTAGAGAAAAGTTGCCGCGACGCTAGGTACACTGCGGAGTCTGTCTTTAATGAAAATTTCAGCAACGGTGTCAAGACCTGATAGGTAACATCTTAGTATGTTAAAGTCTCACGTATGAGGTGACGCCTCTAGGCGTATTTCTTAGACCGGAATTACCAGACTACCCTCCCCGAAGCCAGAGATGCTGTGGCCGAGTCGCTGGGGCACATAGATGGAGGGTACTGTGGGGGCTGATTACACGCACTAGGAGGGAATGACAGTGTCTAGTATTGGTATCGTCTCGACATTCCTGCCCCCCAACGCACCTGAGTTATCCGTGGGACGCGAGCATACATACTGGACCCTGCGGCTGTTCCCTTACCGTACCGCAGACCCCGCCTACCCTAGAGATTATACCAGGCCAAGCGAAGACCCTATGGCGCCTGGGACAGCGTCGAGGGGTGTAATCGATCTCCCTCGAAGATTTCGTGAATGCATAGGGGGCATCGAGCTAGGCCTACCAGATTGATCTCCCCACAGCCGGCCCGGTCGGATGACGGGGACGCCCGTGATCCCATTCCACCGGTATTTGATGAACACCTGCCAACGGTCCTGCTCGACGGTACTATTAGACTGCGATAAGACTGATATCATCAGAAAATTAAAATTTGGGAACCTCTGGACGGGCGCAGCATTCATCCAGCCTCTCACATTAATTCGTCATCGCCCCTCCGACCTCCGCCGGATCAATTGCGGGTTTACATTCCTTAAAAGCGTTGGTATTTAGACGAAGGCTTTACCTCGCTTTATAGTAACTGGTGCTAGCGCACTCATCACCCCTGTCACATGCAGAGCGATACTACGACTCCTTAAAGCGACAGTTAGTCGCCACTCGAAATACGAGATCCCAGAGTAATTCGAGAAGACCCCCATACAATTCGCTATTGCATGCCAATGGAATAATTGGACGTAGACGCTAGTATCTGAAACCTGCCCTTAAGAAGAAGCCACTTGGCTGTCAATAACGGGATTACCCTATGGAGCTGGAAATTTTAAGACCGAATTGGTATACCCTTACCGGCATATTTCCGGAACGTAATCATACGGAAGTGTCGTTGCAACCACCCAGTTTTTGTGCTTCGCCCTTCGATAGGGAGTATCCGATAATCGAAACCAACCACTAGTAAGGGCGTATAAGTGGATCCAACGCGCATCGACCCTTGGAAGATATTAGTACCTAACAAGAGTCCCCCTGGCACTTCCGGCGGTTGAGTTTTCTGTTTGCCACACCCTCCTATGGGCTCAAGGGGTCCTGATTAGAACCAGAGGGACATATCTTCGGGGCCAATGGGTGCGAAATTCGGGGTCTAGAGCTCGCTCCGTCGCTTGCTCGAAGGCTAGTCTATTCCCCTCAAAACCCCGGACGCGATAGTCTTGAACGTGCGCACGTCCCAGCTCCAATCCTGTCCCAGGGCTGTACGGCTTGCAAATTGAAAGACTTCGCGATCAATCTGCATCACGCTATGGTAGCAAAACAGACAAGATGATAGTTGACTAGAACACTAGTCCACGATTTCCATTCAACAAGATAGGTCAACCTTCACAGCAAACTTGCCGATGTGCATGGGTGGTGTGGTCCATTACCACAGCTCCAACCAGTTGACTTGAAAAGCGCCAGCGGTATACTCGCCTACCGCGGCCTTGCCACCCGCTTCAAGGGAGGCGCCGAGCGCGCACATGAAGGCGGTAATGCAATCGTACAGCCACTATTCTGTAGTCTCGACAAAATTAAGACCGTACGCATTCTTGATATAACGATATTCAACAGATGCTGAGAGTAGAACCTCTATCAGAGCGTGATGGGTCGGATGAAGCTTACCGCATTCTAAGGGTTCAACTATTCGACTCATAGTTTCGCAGTGGGGCATTCAGCATTGGCCGTGCAGTGAGTTATTAGACGCCGATGGGCGATTGCCGAAGCAGTTACAGCGCCGGCTCCGGTTGTTGAATACAGTATCCGCAGGCCTCGTGATGAATCAATTCGTGCTTAGGCGGACTCACCGGTCTAAACCAAACGATTCTATGCTATGGTACAGGTGGCCCGGAGGTCCTAGCGGAGTATACGATGGGCGAAGATATCTGTTCATATCCGCTACAGTGATTCGACAAGATGCAAAGCCATGTTTGGGGAATCCCATACGGGGCTCTTGTGGTAAGTATAACCGAAATTGACTATGGCTGAGAACGCGGCATACATGCGTTGGCTTGCGCCTTAGGGAACCGGCTCCATTTGTGATCAGGATGAAACCTTGCAGACTCGAAGTGTCCTGGTGCGGCAAGGGGACGAGCCCATGTATTGATCTGCGGCAATACGTCGGAATAGAGTAAAACACCATGCAACATGATCTTCCCGTTGGCGTGGCCAAAATTGTGTTACTAGTTTAGGCGGATAGCCCCGCACTGCAGTGGCCTATCCTGGGTGAGTAGACACTGCGCCCTTGAGGTTTTCGATACTCATGGAAATATAAGACGTGCCCCTGGATTCGAGTCAAAGTATTACTTCTGTTGGTTTACAATACCAGGACGCGCACCTACACGGTCAGTCGCATCGAGTTGGAACCCTCTGGGCGTTAAAATAGTGCCTCCGAGCAGACGAGTTTTACTAGGATACGAGTCGGCAGTCGAAATGGTGATGGTCACTGTCAGCCATAGACCTTTGCGAGAACTCGCTGCTGAGCCCCACCGTGATATTTGCTACGTCCGGTCTTTTCCTCGAAATGGCCCAATTTCCAGATATTTGTCAATTGTTGTGAATCGGAGCAAGGCACCGTGGCTGTGACCAAACAAACTAAGCTGTTATAACCTGTAGTCTCTACGAAGTTAAAAACCGCTTTGATCCGATTCAAGTGTTTAATCCGGGCAAGGATTGCACTAAACGCCGCAATCATTAATAAGGTATGACTTCACCGAACAGTGAATAACCAAATAGCGGGACCAGGATACCGATCGAGTTGTTTTACCGGTGAATAGTCACGGATCCAGCTGGTCGCCCACGCACCCCTGTGTCCGCAGACTTCACTTTATTTTCGTTACTTTCAGAGGATGGTTACGCGGGTAGTCAGTCTATGGATGGACTGTGTCACCCCGTAGTGGGAAAATAGCTCTAATGCCTGATCTGGGCCTAGCGCCAACCCGCGGCTCGTCGGTGATGCCTTCGGCGCAGAGATCAAGCTAGCGTTTACTCCTAGGACGTGCCGGCTAGATGACCGCTACCATGCATTCGAACGCCATTATGTCTTAACAGCGCACAAAATAACCCTCTAACTGGCCGGGAATGAAAGGTTTACACTCTGTAAATTGCAACTTGATAGTCGTCTATAGAACCGTGCCGCGGGGCGTAATCTGAGATTGGCGGACTGGTGCCATCGAAGCCTTCCATTACAATAACTATTCAGGAGAGTTTCGAACACGAGCCATCACGGTAATCGGTGTGTCTACGTCAGACCCCACCCATCGCCTCCCTATGTTGCCTTTGGGGCACAGTTAAACTCCTAGACACCAGCGCCTGTATGCCAAGTACGGGGTGATACTTGCTGCGTTGAAGCATCTAAGTATACCCATGCGCGCCACTCAACTGCAGCCATTTAACCCTCTTAAGTGCGGCCCAGCAAGGGCGGGTTAAACGCAAGAATATTTATTAGGGAGTCGCCGTGACGACAGTGTCGGCTCTTACTTTCATGCAGTACAAGTACGCCATAGAACCACGGACTGGAGGGCTAGACCCATATTGAGGGTGGTCTAGTAATATTGAATTGAAACGTCCTAACGACGAGGCGCAGCCACGCACTAGCAAGGGCTAATATTGATCGGGTAAACATGAGGGCCAACAAACCACGGTATAGCTTGGACTGGTCAGAAGACGTGGGGATGGGTTCGACAAGTCGTGCTGTGAAGGATGTGGCTGACCCTTCTTTAGTGCGTGTTCGCGCAGGACCATTAATCAACACCTGTACGATACTGTCACCCATTAAACTTATAGTGACACCTCTTAGAAATTCTTCCGAGACTTTCAGGCCCCCATACAGGAACGACGCCTCTATTTTGGGAGTGTGGGGGAATCCGAAGTACGGCGAGCGTCATTATCCCAGCGCGTTCATCACAAATATGATACATATAACCCTTCCTAACGAGCTCAGCACACCCGGCCGTTGACGGCTACCACTGTCGGCGGCTCCAGGTTTGTTCTGTGCCATAGTCCTGGTAACAGGTCTTGAACTAATCGTTCCCGGCCACTAGAGGTATATATTGTGTCTTAAAACGGAATTAACACGACATGTTGGAGTCATGTAGATTCACGCTGATAGCGGGGAATAATGAGCCGCTGCGCAAATGAGTGGCAGACCGCCCTAGAGTTCAAGATAAACAAAGTGAGATAGCACGCATTCATGTTTAAACGCAGATCAACAATACCCGGTTAGCGAACGTCGGCAGGTAACCCACGTCCGATATTCCTTATGCCCTACACAGGGTAACCCCGTATACGATTCACAAAGATGTATCCAGTCTTTGCCACCTGAGTCTGACCATTCGACTACACGAATTTCGGCGTATTGCAGTAGTCGCAAAACAACGAGGGCACCCGTTGACGCGCCGCTGTTTCGCCAGCCTCTCGCGGGGCTAAGTATACCCCTATATAATGGATAGCGCTTTTTGGTTATAATAAGGGTCGCGACGCTACTATACAGTCTGTGCCACACCCATAGAGTTAACCCAAGGCCTTGTATATCATCCTCATACCTGTAACGTATCACACTCGAATTTAGTATAGTCGCCAAACCAGTGATTCTCGCGACTAAATCAGTCATTCAATCTGCTGCACAGTGACAACGCGACTCTCCATTATCATATGTCGACGGGTACCGTAAGCAGAGTCCGGCTAGCATCGAAAATATGTTCTCCTGGCTCTCAACGGTGGTTGAATATACTGAGGCTTCCAGAAGCTCGTGATTCCCCCGCGAGGACCGCAAGGGCTAAAAAGTAACTTTCCGCCACGCATCTTCGATGGTATCTTGCTATAAGACCGTAAAACCGGTGGGTGCCAATGTCCTGGGCTGTGAACATTGGCAACTGCATATAGTGATAACCGTGCATATGTGTTGCCCCAAATACGCTACATCCCCCCTATTTAGTACAAAAGTAGTACTCTTCTGTACGGTCATTAATCCAGCTAGAATACCCACGAGCACGAACTTGTCCAATAGAAAAGGCCCA\", 5)\nfor i in range(0, len(result)):\n    print(result[i])",
        "text": "ba1h   -   find all approximate occurrence of a pattern in a string",
        "id": 732
    },
    {
        "code": "hh = [1.0/2.0, 1.0/10.0, 1.0/50.0]\nfor h in hh:\n    t,y = trap(t0,T,0,h)\n    ye = yexact(t)\n    plt.semilogy(t,np.abs(y-ye))\n    plt.legend(hh)\n    plt.xlabel('t')\n    plt.ylabel('log(error)')",
        "text": "study the effect of decrease step size  .  the error be plot in log scale  . ",
        "id": 733
    },
    {
        "code": "def std_dev(column):\n    values = data_dictionary[column]\n    var = [(x - np.mean(values))**2 for x in values]\n    stdev = (np.sum(var)/len(values))**(0.5)\n    return(stdev)\nprint('Rate Standard Deviation:')\nprint(standard_deviation(data_dict['rate']),\"\\n\")\nprint('Math Standard Deviation:')\nprint(standard_deviation(data_dict['math']),\"\\n\")\nprint('Verbal Standard Deviation:')\nprint(standard_deviation(data_dict['verbal']))",
        "text": "write a function use only list comprehension , no loop , to compute standard deviation  .  print the standard deviation of each numeric column  . ",
        "id": 734
    },
    {
        "code": "g.close()\nh = open('test.txt', mode='at', encoding='utf-8')\nh.writelines(\n[\"We are learning Python\\n\",\n\"Which I was not aware of\\n\",\n\"how cool it is\\n\"])\nh.close()",
        "text": "appendin to text file use the   -  mode=at  -  also use the   -  writelines ( )   -  method , that can take a list of string  . ",
        "id": 735
    },
    {
        "code": "def trapz(x, y):\n    return 0.5*np.sum((x[1:]-x[:-1])*(y[1:]+y[:-1]))",
        "text": "exercise    -  1 write a function trapz ( x , y )  , that apply the trapezoid formula to pre   -   compute value , where  x  and  y  be 1   -   d array  . ",
        "id": 736
    },
    {
        "code": "arr = np.arange(10)\nnp.sqrt(arr)\nnp.exp(arr)\nx = randn(8)\ny = randn(8)\nprint(x)\nprint(y)\nnp.maximum(x, y) \narr = randn(7) * 5\nnp.modf(arr)\na = np.arange(24).reshape(4,2,3)\na.max()\nnp.maximum(a)",
        "text": "universal function , fast element   -   wise array function    -  通用函数,快速的元素级数组函数",
        "id": 737
    },
    {
        "code": "df_sfeature.plot(kind='scatter', x=sfeature+'d_std', y=sfeature+\"d_mean\", c=\"failure\", colormap=cmap_bold)",
        "text": "nice  .  see seperation between good and bad device  . ",
        "id": 738
    },
    {
        "code": "from urllib.request import urlretrieve\nimport pandas as pd\nfrom pandas import DataFrame\nimport matplotlib.pyplot as plt\nurl = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\nurlretrieve(url,'winequality-red.csv')\ndf = pd.read_csv('winequality-red.csv',sep = ';')\nprint (df.head())",
        "text": "import flat file from the web",
        "id": 739
    },
    {
        "code": "\nthemes = pd.DataFrame(columns=['code', 'name'])\nfor row in json_df.mjtheme_namecode:\n    themes = themes.append(json_normalize(row))\nthemes.reset_index(drop=True, inplace=True)\nthemes.head()\ntheme_counts = themes.name.value_counts()\nprint('Top 10 project themes:')\ntheme_counts.head(10)",
        "text": "top 10 major project theme",
        "id": 740
    },
    {
        "code": "import os\nimport pandas as pd\nfrom scipy.stats import zscore\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nimport pandas as pd\nimport io\nimport requests\nimport numpy as np\nfrom sklearn import metrics\nkey = \"qgABjW9GKV1vvFSQNxZW9akByENTpTAo2T9qOjmh\"  \nfile = '...location of your source file...'\npath = \"./data/\"\nfilename_read = os.path.join(path,\"reg-30-spring-2018.csv\")\ndf = pd.read_csv(filename_read)\nids = df['id']\nsubmit_df.to_csv('4.csv',index=False)\n\n\nsubmit(source_file=file,data=submit_df,key=key,no=4)",
        "text": "assignment   -   4 sample code the follow code provide a start point for this assignment  . ",
        "id": 741
    },
    {
        "code": "times_by_country = timeshigher.groupby('country', as_index=False)['inter_student_ratio'].mean()\ntimes_country = times_by_country.sort_values(ascending=False, by='inter_student_ratio').head(10)\ntimes_country",
        "text": "( 2   -   c ) aggregation of the data in ( b   -   international students/total student ratio ) by country  . ",
        "id": 742
    },
    {
        "code": "train_cols_main = [ 'has_milk','has_cocoa','has_palm',\n                   'has_water', 'has_flour' ,'has_ginger','has_honey','has_spice','has_onion',\n                   'is_plant','is_bev','is_sugar','is_frozen','is_fresh','text_count','intercept']\nlogit_train8 = sm.Logit(train['high_add'], train[train_cols_main])\nresult_train8 = logit_train8.fit()\nresult_train8.summary()\nresult_train8.conf_int()\nnp.exp(result_train8.params)",
        "text": "remove has _ wheatand has _ garlic  from the model",
        "id": 743
    },
    {
        "code": "sf['Country']\nsf['Country'].show()\ndef transform_country(country):\n    if country == \"USA\":\n        return \"United States\"\n    else:\n        return country\nsf['Country'].apply(transform_country)\nsf['Country'] = sf['Country'].apply(transform_country)\nsf",
        "text": "use the apply function to do advance transformation of data",
        "id": 744
    },
    {
        "code": "coef = modelList[0].coef_\ntopindx = np.argsort(coef)[0][::-1][:10]\nprint (Features[topindx])",
        "text": "top 10 important feature in linear model",
        "id": 745
    },
    {
        "code": "for dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\nprint (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())",
        "text": "fare    -  fare also ha some miss value and we will replace it with the median  .  then we categorize it into 4 range  . ",
        "id": 746
    },
    {
        "code": "signal = thinkdsp.UncorrelatedGaussianNoise()\nwave = signal.make_wave(duration=0.5, framerate=11025)\nwave.plot(linewidth=0.5)\nthinkplot.config(xlabel='time',\n                 ylabel='amplitude')",
        "text": "uncorrelated gaussian noise an alternative to uu noise be uncorrelated gaussian ( ug noise )  . ",
        "id": 747
    },
    {
        "code": "x=3 \ny=x \nx = 10\nprint (x,y)\n#x=10, y=3, because second line assigns y the value of x when it was 3",
        "text": "what be the value of y after run these statement in order ? of x ? why ?",
        "id": 748
    },
    {
        "code": "\nnumber_of_features = 1000\n(train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features)\ntrain_features = sequence.pad_sequences(train_data, maxlen=400)\ntest_features = sequence.pad_sequences(test_data, maxlen=400)",
        "text": "load dataset on movie review text",
        "id": 749
    },
    {
        "code": "R = np.arange(-4,4+1e-9,0.1)\nX,Y = np.meshgrid(R,R)\nprint(X.shape,Y.shape)",
        "text": "a an example , we would like to plot the l2   -   norm function $ f ( x , y ) = \\sqrt { x^2 + y^2 } $ on the subspace $ x , y \\in [   -   4,4 ]   -  first , we create a meshgrid with appropriate size ,",
        "id": 750
    },
    {
        "code": "\nprint(EmlOpt.dtypes)\nfor col in EmlOpt.columns[1:]:\n    print(EmlOpt[col].unique())\nprint(EmlOpt[EmlOpt.columns[1:]].sum())\nprint(pd.value_counts(EmlOpt[EmlOpt.columns[1:]].sum(axis=1)))\nprint(EmlOpt.shape)\nEmlOpt_mod = EmlOpt[EmlOpt.columns[1:]] #EmlOpt_mod is a predictor",
        "text": "emlopt ( nothing much to change )",
        "id": 751
    },
    {
        "code": "loans['safe_loans'] = loans['bad_loans'].apply(lambda x: +1 if (x==0) else -1)",
        "text": "reassign the label to have +1 for a safe loan , and   -   1 for a risky ( bad ) loan  .  you should have code analogous to",
        "id": 752
    },
    {
        "code": "source.spectral_model.plot(source.energy_range, **opts)\nplt.errorbar(x=data['e_ref'],\n             y = data['dnde'],\n             yerr = data['dnde_err'],\n             fmt='.'\n            )\nfrom gammapy.utils.energy import Energy\nenergy = Energy.equal_log_spacing(\n    emin=source.energy_range[0],\n    emax=source.energy_range[1],\n    nbins=100,\n)\nflux = source.spectral_model.evaluate(\n    energy=energy.to('MeV').value,\n    amplitude = minuit.values['amplitude'],\n    index = minuit.values['index'],\n    ecut = minuit.values['energy_cut'],\n    reference=p['reference'].value,\n)\nplt.plot(energy, flux)",
        "text": "final plot add the fit model to the plot from above  . ",
        "id": 753
    },
    {
        "code": "df.sort_values(by=['expanded_income'], inplace=True)\ndf['expanded_income_percentile'] = df['weight'].cumsum()\ndf['expanded_income_s006_cumsum'] = df['s006'].cumsum()\ndf.sort_values(by=['aftertax_income'], inplace=True)\ndf['aftertax_income_percentile'] = df['weight'].cumsum()\ndf.sort_values(by=['e00900'], inplace=True)\ndf['e00900_percentile'] = df['weight'].cumsum()",
        "text": "add percentile of expand income , after   -   tax income , and e00900 by sort and sum cumsum  of normalize weight  . ",
        "id": 754
    },
    {
        "code": "ppl_inits = (dtst >> template_ppl_inits).run()",
        "text": "now we link the dataset to the template pipeline ,",
        "id": 755
    },
    {
        "code": "final_sig, time_stamps = ecg.process_signal(N=15)\nplot(final_sig,time_stamps,start=0,end=2000)\nplt.savefig('DetectedR_15.png',bbox_inches='tight')\nplot(final_sig,time_stamps,start=450,end=550)\nfinal_sig, time_stamps = ecg.process_signal(N=20)\nplot(final_sig,time_stamps,start=0,end=2000)\nplot(final_sig,time_stamps,start=450,end=550)",
        "text": "a figure show the first 2000 sample of the ecg signal with an  *  mark the detect r wave for n= 15 .  name the figure  detectedr _ 15 . jpg",
        "id": 756
    },
    {
        "code": "\nimport matplotlib.gridspec as gridspec\ngs = gridspec.GridSpec(2, 1) \nax1 = plt.subplot(gs[0, 0])\nax2 = plt.subplot(gs[1, 0])\nax1.plot()\nax2.plot()\nplt.show()",
        "text": "exercise make a plot that be compose of two plot , vertically stack of ,   -   the close price   -   the volume you can do this with matplotlib 's [ gridspec ] ( <url> )",
        "id": 757
    },
    {
        "code": "df_top20 = pd.DataFrame(df[['Name', 'Global_Sales']].head(20)).sort_values('Global_Sales', ascending=False)\ndf_top20",
        "text": "iv  .  what be the top 20 high gross game ?",
        "id": 758
    },
    {
        "code": "def add_elapsed(fld, prefix):\n    sh_el = elapsed(fld)\n    df[prefix+fld] = df.apply(sh_el.get, axis=1)",
        "text": "and a function for apply say class across dataframe row and add value to a new column  . ",
        "id": 759
    },
    {
        "code": "pd.scatter_matrix(feature_df[['q1_length', 'q2_length', 'q1_numwords', 'q2_numwords', \n                               'dmr_leven_dist', 'jaro_winker_dist','jaccard_sim_score', 'hamming_dist','unique_word_count',\n                                'normalized_diff_num_nouns', 'polarity_score_diff', 'subjectivity_score_diff','shared_words_normalized']], alpha = 0.3, figsize = (24,24), diagonal = 'kde');",
        "text": "scatter matrix of all textual feature",
        "id": 760
    },
    {
        "code": "net2 = LogReg().cuda()\nopt=optim.Adam(net2.parameters())\nfit(net2, md, n_epochs=1, crit=loss, opt=opt, metrics=metrics)\ndl = iter(md.trn_dl)\nxmb,ymb = next(dl)\nvxmb = Variable(xmb.cuda())\nvxmb\npreds = net2(vxmb).exp(); preds[:3]\npreds = preds.data.max(1)[1]; preds",
        "text": "we create our neural net and the optimizer  .  ( we will use the same loss and metric from above )  . ",
        "id": 761
    },
    {
        "code": "def outer_func():\n    message = \"Hi\"\n    def inner_func():\n        print(message)\n    return inner_func\nmyf = outer_func()\nprint(myf)\nprint(myf.__name__)\nmyf()\nmyf()\ndef outer_func(msg):\n    message = msg\n    def inner_func():\n        print(message)\n    return inner_func\nmyf1 = outer_func(\"Hi\")\nmyf2 = outer_func(\"Hola\")\n\nmyf1()\nprint(myf1.__closure__)\nmyf2()",
        "text": "closure be a record store a function together with an environment , a map associate each free variable of the function with the value or storage location to which the name wa bind when the closure wa create  .  note , unlike a regular function , closure allow the function to access those capture variable through the closure 's reference to them  . ",
        "id": 762
    },
    {
        "code": "\nthinkplot.Pdf(tank, color='0.7')\ntank.Update(17)\nthinkplot.Pdf(tank)\ntank.Mean()",
        "text": "exercise 7 ,   -  suppose we see another tank with serial number 17 .  what effect doe this have on the posterior probability ? update the suite again with the new data and plot the result  . ",
        "id": 763
    },
    {
        "code": "df_learning.tail(5)\ndata = np.asarray(df_learning)\nX, y = data[:, 3:-1], data[:, -1].astype(int)\nprint(\"Features\", X[1:5])\nprint(\"Labels\", y[1:5])\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(max_depth=5)\ncross_val_score(clf, X, y, cv=10)   \ndf_learning.columns[0+3]\nX.shape",
        "text": "analysis with sklearn ( easy classifier )    -  decision tree * decision tree , <url> * cross validation , <url>",
        "id": 764
    },
    {
        "code": "train_set = pandas.read_csv('train_set.csv')\ncv_set = pandas.read_csv('cv_set.csv')\ntrain_data = train_set[['wifi'+str(i) for i in range(1, len(train_set.columns) - 1)]]\ntrain_labels = train_set['room']\ncv_data = cv_set[['wifi'+str(i) for i in range(1, len(cv_set.columns) - 1)]]\ncv_labels = cv_set['room']\nprint(train_data[:10])\nprint(train_labels[:10])\nprint(cv_data[:10])\nprint(cv_labels[:10])",
        "text": "load the data and break it into train and cross   -   validation set  . ",
        "id": 765
    },
    {
        "code": "HR_SB_30 = bat_data.pivot_table(values=['HR','SB'], index=['playerID','yearID'],aggfunc='sum').reset_index()\nHR_SB_30[(HR_SB_30['SB'] >= 30) & (HR_SB_30['HR'] >= 30)]['playerID'].nunique()",
        "text": "how many player have hit 30 or more hr in a season while also steal ( sb ) 30 or more base ? ( number )",
        "id": 766
    },
    {
        "code": "from __future__ import print_function\nfrom time import time\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom sklearn.cross_validation import train_test_split\nimport numpy as np\nimport os\nimport pickle\nvalidDocsDict = dict()\nfileList = os.listdir(\"BioMedProcessed\")\nfor f in fileList:\n    validDocsDict.update(pickle.load(open(\"BioMedProcessed/\" + f, \"rb\")))\nvalidDocsDict2 = dict()\nfileList = os.listdir(\"PubMedProcessed\")\nfor f in fileList:\n    validDocsDict2.update(pickle.load(open(\"PubMedProcessed/\" + f, \"rb\")))",
        "text": "topic model two datasets this be a notebook for try to use topic model for classify set of text that be more syntactically similar than topically similar  .  this notebook attempt to distinguish between discussion and conclusion section of scientific paper  .  below we be load the two datasets for use  . ",
        "id": 767
    },
    {
        "code": "temp <- merge(PlayerMerge,PlayerMatches,by=\"player_api_id\")\nhead(temp)",
        "text": "merge the no _ of _ matches column that we get above with the data frame we make for gk _ overall",
        "id": 768
    },
    {
        "code": "type(42) \ntype(42.0) \ntype('42.0') \ntype(\"42.0\") \ntype(\"\"\"42.0\"\"\") \ntype([1, 2]) \ntype([1] + [2]) \ntype(1 + 2) \ntype(print) # Print is a built-in function in Python.",
        "text": "explain the result of each line ,",
        "id": 769
    },
    {
        "code": "vals = data.value\nvals\nvals[5] = 0\nvals",
        "text": "it important to note that the series return when a dataframe be indexted be merely a   -  view  -  on the dataframe , and not a copy of the data itself  .  so you must be cautious when manipulate this data ,",
        "id": 770
    },
    {
        "code": "def normalize(x):\n    return x/255\ntests.test_normalize(normalize)",
        "text": "implement preprocess function    -  normalize implement the normalize  function to take in image data ,  x  , and return it a a normalize numpy array  . ",
        "id": 771
    },
    {
        "code": "alameda = district_table.column(\"feature\").item(0)\neastbay = district_table.column(\"feature\").item(2)\nalameda_and_east_bay = np.append(alameda,eastbay)\nMap(alameda_and_east_bay, height=300, width=300)\n_ = project1.grade('q00')",
        "text": "to display a map  contain only two feature from the district _ table  , call map  on a list contain those two feature from the feature  column   -  question 0 . 0  -  draw a map of the alameda county water district ( row 0 ) and the east bay municipal utility district ( row 2 )  . ",
        "id": 772
    },
    {
        "code": "plot_feature_importance(rfc_best_model, X_train, max_features=16)\nm_oob = rfc_best_model.oob_decision_function_",
        "text": "auc and confusion matrix suggest that the gradient boost model be good than random forest",
        "id": 773
    },
    {
        "code": "\nsymbol = mx.sym.Group([internals[prob_layer], internals[conv_layer]])\nsave_dict = mx.nd.load(params)\narg_params = {}\naux_params = {}\nfor k, v in save_dict.items():\n    l2_tp, name = k.split(':', 1)\n    if l2_tp == 'arg':\n        arg_params[name] = v\n    if l2_tp == 'aux':\n        aux_params[name] = v\nmod = mx.model.FeedForward(symbol,\n                           arg_params=arg_params,\n                           aux_params=aux_params,\n                           ctx=ctx,\n                           allow_extra_params=False,\n                           numpy_batch_size=1)",
        "text": "build network symbol and load network parameter  . ",
        "id": 774
    },
    {
        "code": "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol='medv')\npred = model.transform(test)\nRMSE = evaluator.evaluate(pred)\nprint (f'The RMSE is {RMSE}')",
        "text": "now validate the model on the test set , and check the root mean square error  . ",
        "id": 775
    },
    {
        "code": "plt.bar(range(len(odtvmonths)), odtvmonths.values(), align='center')\nplt.xticks(range(len(odtvmonths)), odtvmonths.keys(),rotation='vertical')\nplt.xlabel(\"Months\")\nplt.ylabel(\"Number of Edit\")\nplt.title(\"Number of edit over Months for TV Series \")\nplt.show()\nprint(json.dumps(odtvmonths, indent = 4 ))",
        "text": "view of revison number for tv series on monthly basis",
        "id": 776
    },
    {
        "code": "yhat_ts = svc.predict(Xts)\nacc = np.mean(yhat_ts == yts)\nprint('Accuaracy = {0:f}'.format(acc))",
        "text": "measure the accuracy on the test sample  .  you should get about 96 % accuracy  .  you can get good by use more train sample , but it will just take long to run  . ",
        "id": 777
    },
    {
        "code": "with tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('.'))\n    test_accuracy = evaluate(X_test, y_test)\n    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\nwith tf.Session() as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('.'))\n    accuracy = evaluate(X_train, y_train)\n    print(\"Train Accuracy = {:.3f}\".format(accuracy))",
        "text": "test model on test set",
        "id": 778
    },
    {
        "code": "class_count.plot.bar(sort_columns=True,\n                     color=class_colors,\n                     fontsize=f_size,\n                     title='Titanic Passenger Count by Class');",
        "text": "display number of passenger in each class",
        "id": 779
    },
    {
        "code": "path = '/mnt/4_TB_HD/ramona/utils/features/'\ndt = '18-06-01-18-19'\nX_train, Y_train, X_test, Y_test, Y_train_cateogorial, Y_test_cateogorial, stage2_transf, stage2_transf_filtered = read_data(dt)\nstage2test_classes = pd.read_pickle('/mnt/4_TB_HD/ramona/utils/stage2_data/stage2test_classes.sav')\nstage2_y = stage2test_classes['Class'].values\nX_stack = np.vstack([X_train,X_test])\nY_stack = np.hstack([Y_train, Y_test])\nprint(X_stack.shape, Y_stack.shape)",
        "text": "load feature that be ready for classification",
        "id": 780
    },
    {
        "code": "caffe.set_device(0)  \nmodel_def = caffe_root + 'models/bvlc_reference_caffenet/deploy.prototxt'\nmodel_weights = caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'\nnet = caffe.Net(model_def,      \n                model_weights,  \n                caffe.TEST)     # use test mode (e.g., don't perform dropout)",
        "text": "load net and set up input preprocessing * set caffe to gpu mode ( tesla ) and load the net from disk  . ",
        "id": 781
    },
    {
        "code": "\ndef cleanTicket( ticket ):\n    ticket = ticket.replace( '.' , '' )\n    ticket = ticket.replace( '/' , '' )\n    ticket = ticket.split()\n    ticket = map( lambda t : t.strip() , ticket )\n    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n    if len( ticket ) > 0:\n        return ticket[0]\n    else: \n        return 'XXX'\nticket = pd.DataFrame()\nticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\nticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\nticket.shape\nticket.head()",
        "text": "extract ticket class from ticket number *select the cell below and run it by press the play button   - ",
        "id": 782
    },
    {
        "code": "predict_output(test_feature_matrix, weights[0])",
        "text": "question , what be the predict price for the 1st house in the test data set for model 2 ( round to near dollar ) ?   - ",
        "id": 783
    },
    {
        "code": "cabin = pd.DataFrame()\ncabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\ncabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\ncabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\ncabin.head()\ntitanic.head()",
        "text": "extract cabin category information from the cabin number *select the cell below and run it by press the play button   - ",
        "id": 784
    },
    {
        "code": "frame = DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'],\n                  columns=['d', 'a', 'b', 'c'])\nframe.sort_index()\nframe.sort_index(axis=1)",
        "text": "with a dataframe , you can sort by index on either axis ,",
        "id": 785
    },
    {
        "code": "import requests\nstart_time = 1420070400 \nend_time   = 1420156800 \nresponse = requests.get(\"https://api.stackexchange.com/2.2/questions?pagesize=100\" +\n                        \"&fromdate=\" + str(start_time) + \"&todate=\" + str(end_time) +\n                        \"&order=asc&sort=creation&site=stackoverflow\")\nprint(response)",
        "text": "let 's make a sample request to retrieve the question post on stack exchange on the first day of 2015 .  documentation of the stack exchange api can be find [ here ] ( <url> )  . ",
        "id": 786
    },
    {
        "code": "more_than_150 = data[data['income integer'] >= 150000]\ntravel_more = more_than_150[travel_q]\nprint('For those with a household income of 150,000 or higher: ', '\\n')\ntravel_more_counts = travel_more.value_counts()\ntravel_more_counts\ntravel_more_counts.plot.barh()",
        "text": "how far high   -   income survey respondent travel",
        "id": 787
    },
    {
        "code": "def plot_pair(reduced, clusters):\n    \n    temp = reduced[:,0:4]\n    temp = pd.DataFrame(temp, columns=['PCA1', 'PCA2', 'PCA3', 'PCA4'])\n    temp['clusters'] = clusters\n    g = sns.PairGrid(temp, hue='clusters', vars=['PCA1', 'PCA2', 'PCA3', 'PCA4'])\n    g.map_diag(plt.hist)\n    g.map_offdiag(plt.scatter)\n    ax = g\n    return ax\nk_means, clusters = cluster(reduced, random_state=check_random_state(0), n_clusters=4)\npg = plot_pair(reduced, clusters)",
        "text": "pair grid   -   write a function name plot _ pair ( )  that us [ seaborn . pairgrid ] ( <url>   -   ) to visualize the cluster in term of first four principal component  .  the plot on the diagonal should be histogram of correspond attribute , and the off   -   diagonal should be scatter plot   -  [ ] ( <url> )",
        "id": 788
    },
    {
        "code": "def convertyears(x):\n    if x > 9 and x < 20:\n        t = '20' + str(x)\n        return float(t)\n    elif x <= 9:\n        t = '200' + str(x)\n        return float(t)\n    elif x > 20:\n        t = '19' + str(x)\n        return float(t)\n    else:\n        return np.nan\ndf_merged17['taxdelinquencyyear'] = df_merged17['taxdelinquencyyear'].map(lambda a: convertyears(a))\ndf_merged16['taxdelinquencyyear'] = df_merged16['taxdelinquencyyear'].map(lambda a: convertyears(a))",
        "text": "second , the tax delinquency year be list a yy , with the first digit miss if it be a 0 .  since some of the year be from the 1990s , we need to fix this so that they will sort in the correct order ,",
        "id": 789
    },
    {
        "code": "lol_df['Business Major'] = ['yes','no','yes','yes','yes','no','yes']\nlol_df['Years Experience'] = [1,4,2,6,0,3,0]\nlol_df.head(5)",
        "text": "we can also add column ( they should have the same number of row a the dataframe they be be add to )",
        "id": 790
    },
    {
        "code": "classifier_onevsall = OneVsRestClassifier(svm.SVC(C=1000., probability=True, kernel='linear'))\nclassifier_onevsall.fit(train_x, train_y)\ny_predict_onevsall = classifier_onevsall.predict(test_x)\nconfusion_matrix(test_y, y_predict_onevsall)\nprint(\"Accuracy : \", accuracy_score(test_y, y_predict_onevsall))\nprint(classification_report(test_y, y_predict_onevsall, target_names=classes))",
        "text": "svm multiclass classifier ( onevsrest )",
        "id": 791
    },
    {
        "code": "filename = 'data/World_population_estimates.html'\ntables = read_html(filename, header=0, index_col=0, decimal='M')",
        "text": "the data directory contain a download copy of <url>",
        "id": 792
    },
    {
        "code": "regions = pd.DataFrame({'population': population,\n                       'area': area})\nregions\nregions.index\nregions.columns",
        "text": "now that we have this along with the population series from before , we can use a dictionary to construct a single two   -   dimensional object contain this information ,",
        "id": 793
    },
    {
        "code": "import tensorflow as tf\nx = tf.placeholder(tf.float32, (None, 32, 32, image_channles))\ny = tf.placeholder(tf.int32, (None))\none_hot_y = tf.one_hot(y, n_classes)\nkeep_prob = tf.placeholder(tf.float32)",
        "text": "x  be a placeholder for a batch of input image  .   y  be a placeholder for a batch of output label  . ",
        "id": 794
    },
    {
        "code": "times_by_region = timeshigher.groupby('region', as_index=False)['inter_student_ratio'].mean()\ntimesby_region = times_by_region.sort_values(ascending=False, by='inter_student_ratio').head(10)\ntimesby_region\nplot = sns.barplot(\"inter_student_ratio\", \"region\", data=timesby_region)\nplot.set(xlabel=\"International student ratio\", ylabel=\"region\")",
        "text": "( 2   -   d ) aggregation of the data in ( b   -   international students/total student ratio ) by region  . ",
        "id": 795
    },
    {
        "code": "W_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2",
        "text": "densely connect layer* now that the image size ha be reduce to 7x7 , we add a fully   -   connect layer with 1024 neuron to allow process on the entire image  .  we reshape the tensor from the pool layer into a batch of vector , multiply by a weight matrix , add a bias , and apply a relu  . ",
        "id": 796
    },
    {
        "code": "photo_data = misc.imread('./wifire/sd-3layers.jpg')\nred_mask   = photo_data[:, : ,0] < 150\ngreen_mask = photo_data[:, : ,1] > 100\nblue_mask  = photo_data[:, : ,2] < 100\nfinal_mask = np.logical_and(red_mask, green_mask, blue_mask)\nphoto_data[final_mask] = 0\nplt.figure(figsize=(15,15))\nplt.imshow(photo_data)",
        "text": "< p style=  font   -   family , arial , font   -   size,1 . 75em , color ,   -   2462c0 , font   -   style , bold  >   composite mask that take threshold on all three layer , red , green , blue",
        "id": 797
    },
    {
        "code": "scan = CTScan(np.asarray(candidates.iloc[negatives[600]])[0], \\\n              np.asarray(candidates.iloc[negatives[600]])[1:-1])\nscan.read_mhd_image()\nx, y, z = scan.get_voxel_coords()\nimage = scan.get_image()\ndx, dy, dz = scan.get_resolution()\nx0, y0, z0 = scan.get_origin()\nimage.shape # one scan, 194 images",
        "text": "check if my class work    - ",
        "id": 798
    },
    {
        "code": "def content_loss(current_tensor, computed_ary):\n    \n    _, height, width, number = computed_ary.shape\n    size = height * width * number\n    return tf.sqrt(tf.nn.l2_loss(current_tensor - computed_ary) / size)",
        "text": "difference ( loss ) between content layer of result image and content layer of content image  . ",
        "id": 799
    },
    {
        "code": "batRegex = re.compile(r'Bat(man|mobile|copter|bat)')\nmo = batRegex.search('Batmobile lost a wheel')\nmo.group()\nmo.group(1)",
        "text": "the ( wo ) ? part of the regular expression mean that the pattern wo be an optional group  .  the regex will match text that ha zero instance or one instance of wo in it  . ",
        "id": 800
    },
    {
        "code": "import helper\nimport problem_unittests as tests\nsource_path = 'data/small_vocab_en'\ntarget_path = 'data/small_vocab_fr'\nsource_text = helper.load_data(source_path)\ntarget_text = helper.load_data(target_path)",
        "text": "language translation train a sequence to sequence model on a dataset of english and french sentence that can translate new sentence from english to french   -  get the data since translate the whole language of english to french will take lot of time to train , i have train it with a small portion of the english corpus  . ",
        "id": 801
    },
    {
        "code": "A = np.array([[n+m*10 for n in range(5)] for m in range(5)])\nA\nA[1:, 1:]\nA[::2, ::2]",
        "text": "for multidimensional array the index slice work exactly the same way ,",
        "id": 802
    },
    {
        "code": "x[1] = \"berkeley\"\nx[1]",
        "text": "you can also directly change the value of an index ,",
        "id": 803
    },
    {
        "code": "fit_km = km(n_clusters=3)\nfit_km.fit(trans)\ncenter = fit_km.cluster_centers_\nplt.figure()\nplt.scatter(trans[:,0], trans[:,1], c=train_y)\nplt.xlabel(\"1st PC\", fontsize=20)\nplt.ylabel(\"2nd PC\", fontsize=20)\nplt.xticks([-30,0,30], fontsize=16)\nplt.yticks([-10,0,10], fontsize=16)\nplt.plot(center[:,0], center[:,1], \"r*\", markersize=16)\nplt.colorbar()\nplt.show()",
        "text": "use k   -   mean cluster method to seperate phase",
        "id": 804
    },
    {
        "code": "data = DataFrame([[1., 6.5, 3.], [1., NA, NA],\n                  [NA, NA, NA], [NA, 6.5, 3.]])\ncleaned = data.dropna()\ndata",
        "text": "with dataframe object , these be a bite more complex  .  you may want to drop row or column which be all na or just those contain any na  .  dropna by default drop any row contain a miss value ,",
        "id": 805
    },
    {
        "code": "enron_data[\"PRENTICE JAMES\"]['total_stock_value']",
        "text": "what be the total value of the stock belong to jam prentice ?",
        "id": 806
    },
    {
        "code": "auth = twitter.oauth.OAuth(Twitter['Access Token'],\n                           Twitter['Access Token Secret'],\n                           Twitter['Consumer Key'],\n                           Twitter['Consumer Secret'])\ntwitter_api = twitter.Twitter(auth=auth, retry=True)",
        "text": "authorize an application to access twitter account data",
        "id": 807
    },
    {
        "code": "myvec <- c(1, 10, 100, 1000)\nnames(myvec) <- c(\"ind1\", \"ind2\", \"ind3\", \"ind4\")\nprint(myvec[2])\nprint(myvec[c(2, 3)])\nprint(myvec[c(\"ind3\",\"ind4\")])\nprint(myvec[myvec > 100])",
        "text": "to subset a vector , use  [ ]  a in python  .  however , the index in r start from one , not zero   -  you can also use vector , name , and relational operator for subsetting a vector  .  this be a great advantage of use r  . ",
        "id": 808
    },
    {
        "code": "gs = ms.GridSearchCV(\n    estimator=ssvc,\n    param_grid={\n        'svc__C': [1e-15, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n        'svc__kernel': ['linear', 'rbf']\n    },\n    scoring='roc_auc',\n    cv=ten_fold_cv\n)\ngs.fit(X, y)\ngs.best_score_\ngs.best_estimator_",
        "text": "determine optimal  kernel and value of  c  by cross   -   validation use auc score  . ",
        "id": 809
    },
    {
        "code": "import pandas as pd\nmovies = pd.read_csv('http://bit.ly/imdbratings')\nmovies.head()\nmovies[(movies.duration >= 200) & (movies.genre == 'Drama')]  \nmovies.genre.isin(['Crime', 'Drama', 'Action']);\nmovies[movies.genre.isin(['Crime', 'Drama', 'Action'])];",
        "text": "how do i apply multiple filter criterion to a panda dataframe ?",
        "id": 810
    },
    {
        "code": "plt.figure(figsize=(18, 10))\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model_accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='best')\nplt.show()",
        "text": "plot the graph of accuracy",
        "id": 811
    },
    {
        "code": "plt.figure(figsize=(7,7))\nplt.imshow(p[0].astype('uint8'));\nplt.figure(figsize=(7,7))\nplt.imshow(arr_hr[idx].astype('uint8'));\ntop_model.save_weights(dpath+'sr_final.h5')\ntop_model.load_weights(dpath+'sr_final.h5')",
        "text": "this quality of this prediction be very impressive give that this model wa train the low re image  .  one take away here be that this suggest that the upsampling such a model be learn be somewhat independent of resolution  . ",
        "id": 812
    },
    {
        "code": "text_index = list(nonrelevant_train_df.columns).index(\"text\")\nnonrelevant_text = \"\"\nfor row in nonrelevant_train_df.iterrows():\n    nonrelevant_text += clean_text(row[1][text_index]) + \" \"\nwordcloud = WordCloud(max_font_size=40,\n                      relative_scaling=.5,\n                      max_words=max_words).generate(nonrelevant_text)\nplt.figure(figsize=(14,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.savefig('nonRelevantArticleWC.png')",
        "text": "generate a word cloud for non   -   relevant document text",
        "id": 813
    },
    {
        "code": "x_train,x_test,y_train,y_test = train_test_split(train['message'], train['label'], test_size=0.34, random_state=101 )",
        "text": "split the dataset for train   -   test",
        "id": 814
    },
    {
        "code": "import plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, plot, iplot\ninit_notebook_mode(connected = True)",
        "text": "choropleth map exercise [ full documentation reference ] ( <url> )    -  plotly import",
        "id": 815
    },
    {
        "code": "a = torch.ones(5)\nprint(a)\nb = a.numpy()\nprint(b)\na.add_(1)\nprint(a)\nprint(b)",
        "text": "convert a torch tensor to a numpy array",
        "id": 816
    },
    {
        "code": "_fname = 'results/usALEX - direct excitation coefficient dir_ex_t beta.csv'\ndir_ex_tM = np.loadtxt(_fname, ndmin=1)\nprint('Direct excitation coefficient (dir_ex_t):', dir_ex_tM)",
        "text": "load the   -  multispot direct excitation coefficient  -  ( $ d _  { dirt } $ ) from disk ( compute in [ usalex   -   correction   -   direct excitation physical parameter ] ( usalex   -   correction   -   direct excitation physical parameter . ipynb ) ) ,",
        "id": 817
    },
    {
        "code": "log_clf2 = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\nt0 = time.time()\nlog_clf2.fit(X_train_reduced, y_train)\nt1 = time.time()\nprint(\"Training took {:.2f}s\".format(t1 - t0))",
        "text": "okay , so softmax regression take much long to train on this dataset than the random forest classifier , plus it perform bad on the test set  .  but that 's not what we be interest in right now , we want to see how much pca can help softmax regression  .  let 's train the softmax regression model use the reduce dataset ,",
        "id": 818
    },
    {
        "code": "def targetGlasses(segments):\n    y = np.zeros(faces.target.shape[0])\n    for (start, end) in segments:\n        y[start:end + 1] = 1\n    return y\nglassesList = targetGlasses(fourEyes)\nX_train, X_test, y_train, y_test = train_test_split(faces.data, glassesList, test_size=0.25, random_state=0)",
        "text": "create train and test set for the new problem  . ",
        "id": 819
    },
    {
        "code": "import time\nimport random\nrandom.seed(a=100)\nshort_list = list(random.sample(range(1000000), 10))\nlong_list = list(random.sample(range(1000000), 10000))",
        "text": "drill implement a sort algorithm in python and see how it compare in sort our short and long list  .  you should be able to easily find guide on how to implement any of the algorithm  .  can you figure out why it run fast or slow than our other sort algorithm ? some good sort to try be , heap sort selection sort quicksort",
        "id": 820
    },
    {
        "code": "labels = ['ID', 'YEAR', 'NUMBER', 'GAMEID', 'TEAMID', 'LGID', 'GP', 'POINT']\ndf3 = pd.read_csv('AllstarFull.csv', names=labels, header=0)  \ndf3.head()",
        "text": "specify your own column label like this  . ",
        "id": 821
    },
    {
        "code": "df2=deepcopy(df)\ndf2.loc[df2['exp condition'].str.contains('refeldin'), 'Drug'] = 'Brefeldin'\ndf2.loc[df2['exp condition'].str.contains('DMSO'), 'Drug'] = 'DMSO'\ndf2.loc[df2['exp condition'].str.contains('onensin'), 'Drug'] = 'Monensin'",
        "text": "< a id=makingsense  >      -  make sense let 's add some human   -   understandable tag to the dataframe , so that we can plot what we want more easily   -  a tag for the incubation time   -   a tag for the treatment condition ( bic/dmso )",
        "id": 822
    },
    {
        "code": "\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')",
        "text": "load and prepare data",
        "id": 823
    },
    {
        "code": "\nfrom py2cytoscape.data.cyrest_client import CyRestClient\nimport pandas as pd\nfrom IPython.display import Image\ncy = CyRestClient()\ncy.session.delete()\nnetwork2 = cy.network.create_from('../sampleData/galFiltered.json')\nnetwork3 = cy.network.create_from('../sampleData/sample_yeast_network.xgmml', collection='My Collection')\nnetwork4 = cy.network.create_from('../sampleData/galFiltered.gml', collection='My Collection')\nnetwork_locations = [\n    '../sampleData/galFiltered.json',\n    '../sampleData/sample_yeast_network.xgmml',\n    '../sampleData/galFiltered.gml'\n]\nnetworks = cy.network.create_from(network_locations)\npd.DataFrame(networks, columns=['CyNetwork'])\ncy.layout.apply(network = network2)\n\n\nImage(network2.get_png(height=400))",
        "text": "load network from file , url or web service    -  to load network data from file , url or web service be your usual task  .  by use follow code , you can import data from such kind of data format  . ",
        "id": 824
    },
    {
        "code": "amazon = amazon[amazon['rating'] != 3]\namazon.head()",
        "text": "extract sentiment we will ignore all review with rat = 3 , since they tend to have a neutral sentiment  . ",
        "id": 825
    },
    {
        "code": "class_out=sw_classifier.classify_mnist(\"/home/xilinx/img_webcam_mnist_processed\")\nprint(\"Class number: {0}\".format(class_out))\nprint(\"Class name: {0}\".format(hw_classifier.class_name(class_out)))",
        "text": "launch bnn in software the inference on the same image be perform in sofware on the arm core",
        "id": 826
    },
    {
        "code": "from sklearn.cross_validation import train_test_split\nfinal_data.columns\nx = final_data.drop('not.fully.paid',axis=1)\ny = final_data['not.fully.paid']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)",
        "text": "train test split split our data into a train set and a test set   -  use sklearn to split your data into a train set and a test set a we ve do in the past   - ",
        "id": 827
    },
    {
        "code": "from sklearn.svm import SVC\nsvm1 = SVC(kernel='rbf')\nsvm1.fit(X_train, y_train);\nsvm1.predict(X_train)\nprint(svm1.score(X_train, y_train))\nprint(svm1.score(X_test, y_test))",
        "text": "now let 's try a svm with a gaussian kernel",
        "id": 828
    },
    {
        "code": "import cy_math\nprint(cy_math.py_plus(3, 4))\nprint(cy_math.py_mult(3, 4))\nprint(cy_math.py_square(3))\nxs = np.arange(10, dtype='float')\nprint(cy_math.py_sum(xs))",
        "text": "use the extension module in python",
        "id": 829
    },
    {
        "code": "import numpy as np\nimport tensorflow as tf\nimport keras\nimport scipy.misc as sm\nimport cv2\nimport h5py\nimport matplotlib.pyplot as plt\nimport time\nimport keras.backend as K\nimport os.path\nfrom keras import layers\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom keras.models import Model, Sequential\nfrom keras.layers.convolutional import Convolution2D\nfrom keras import losses\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom keras.preprocessing import image\nfrom keras.models import load_model",
        "text": "import package",
        "id": 830
    },
    {
        "code": "def draw_labeled_bboxes(img, labels):\n    for car_number in range(1, labels[1]+1):\n        \n        nonzero = (labels[0] == car_number).nonzero()\n        \n        nonzeroy = np.array(nonzero[0])\n        nonzerox = np.array(nonzero[1])\n        \n        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n        \n        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\n    return img",
        "text": "draw bound box for lables",
        "id": 831
    },
    {
        "code": "\nlst = ['a', 'a', 'a']\nlen(set(lst)) == 1\nall(x == lst[0] for x in lst)\nlst.count(lst[0]) == len(lst)",
        "text": "check if all element in a list be equal",
        "id": 832
    },
    {
        "code": "\nline=regress.intercept_ + regress.coef_[0] * X2[:,0] + regress.coef_[1]*X2[:,1]  \nplt.close()\nplt.figure(figsize=(8,6))\nplt.plot(X2[:,0],Y,'or', label='Given data points')\nplt.plot(sorted(X2[:,0]),sorted(line),'b-', label = 'Second order poly. Regression Fit') \nplt.xlabel('lift_kg ----->')\nplt.ylabel('putt_m ------->')\nplt.legend()\nplt.title('Fitting second order poly. regression to the dataset')\nplt.show()",
        "text": "$ \\textbf { plot the data and the fit model } $",
        "id": 833
    },
    {
        "code": "plt.plot(data.sepal_length, data.sepal_width, ls = '', marker = 'o')",
        "text": "scatter plot use panda series  . ",
        "id": 834
    },
    {
        "code": "scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype(np.float32))\nX_test_scaled = scaler.transform(X_test.astype(np.float32))\nlin_clf = LinearSVC()\nlin_clf.fit(X_train_scaled, y_train)\ny_pred = lin_clf.predict(X_train_scaled)\naccuracy_score(y_train, y_pred)",
        "text": "wow , 85 % accuracy on mnist be a really bad performance  .  this linear model be certainly too simple for mnist , but perhaps we just need to scale the data first ,",
        "id": 835
    },
    {
        "code": "for i in range(10):\n    trainer.train()\n    test_network()\nfor i in range(5):\n    trainer.train()\n    test_network()\nfor i in range(5):\n    trainer.train()\n    test_network()\nfor i in range(5):\n    trainer.train()\n    test_network()",
        "text": "train the network , for a give number of iteration  .  you can re   -   run this step many time , and it will keep learn but if you train too much you can end up overfitting the train data ( this be visible when the test set accuracy start to decrease )  . ",
        "id": 836
    },
    {
        "code": "print(\"RMSE for fake data:\",np.sqrt(metrics.mean_squared_error(fake_y_true, fake_y_pred)))",
        "text": "root mean square error  -  ( rmse ) be the square root of the mean of the square error ,   -  \\sqrt { \\frac 1n\\sum _  { i=1 } ^n ( y _ i   -   \\hat { y }  _ i ) ^2 }   - ",
        "id": 837
    },
    {
        "code": "int_types = [np.int8, np.int32, np.int64]\nfloat_types = [np.float32, np.float64]\nfor dtype in int_types:\n   print(\"min: {}\".format(np.iinfo(dtype).min))\n   print(\"max: {}\".format(np.iinfo(dtype).max))\nfor dtype in float_types:\n   print(\"min: {}\".format(np.finfo(dtype).min))\n   print(\"max: {}\".format(np.finfo(dtype).max))\n   print(\"eps: {}\".format(np.finfo(dtype).eps))",
        "text": "print the minimum and maximum representable value for each numpy scalar type",
        "id": 838
    },
    {
        "code": "net2 = LogReg().cuda()\nopt=optim.Adam(net2.parameters())\nfit(net2, md, epochs=1, crit=loss, opt=opt, metrics=metrics)\ndl = iter(md.trn_dl)\nxmb,ymb = next(dl)\nvxmb = Variable(xmb.cuda())\nvxmb\npreds = net2(vxmb).exp(); preds[:3]\npreds = preds.data.max(1)[1]; preds",
        "text": "we create our neural net and the optimizer  .  ( we will use the same loss and metric from above )  . ",
        "id": 839
    },
    {
        "code": "\nweather_m_drop_any = weather_m.dropna(how = 'all')\nweather_m_drop_any",
        "text": "drop all row that have have all value miss",
        "id": 840
    },
    {
        "code": "\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_test)\ny_pred.mean()\nfrom sklearn.metrics import mean_squared_error\nmse = mean_squared_error(y_test, y_pred, sample_weight=None, multioutput='uniform_average')\nprint(mse)\nrmse = np.sqrt(mse)\nprint(rmse)",
        "text": "part a , multiple linear regression multipe linear regression be an extension of linear regression  .  a such , there be assumption that must be satisfy before a linear regression can be use  .  these assumption be linearity , homoscedasticity , multivariate normality , independence of error , lack of multicollinearity  . ",
        "id": 841
    },
    {
        "code": "\np = figure(x_axis_label='Year', y_axis_label='Time', tools='box_select')\np.circle('Year', 'Time', source=source, selection_color='red', nonselection_alpha=0.1)\noutput_file('selection_glyph.html')\nshow(p)",
        "text": "selection and non   -   selection glyph",
        "id": 842
    },
    {
        "code": "squares[0]  \nsquares[-3:]  # slicing returns a new list",
        "text": "like string ( and all other build   -   in sequence type ) , list can be index and slice ,",
        "id": 843
    },
    {
        "code": "def processing_pipeline(img):\n    undist = undistort_img(img)\n    color_thresholded = colorthresh(undist)\n    grad_thresholded = gradthresh(color_thresholded)\n    combined_thresholded = combinethresh(grad_thresholded, color_thresholded)\n    warped_img, M, Minv = warp(combined_thresholded)\n    ploty, lefty, righty, leftx, rightx, left_fitx, right_fitx = find_lane(warped_img, undist, Minv)\n    curverad, vehicle_offset = calculate_curvature_offset(ploty, lefty, righty, leftx, rightx, undist.shape[1], undist.shape[0])\n    result = draw_lane(warped_img, undist, Minv, ploty, left_fitx, right_fitx, curverad, vehicle_offset)\n    return result",
        "text": "image process pipeline    -  basically call all the function define above in the desire sequence to successfully detect lane line on the image  . ",
        "id": 844
    },
    {
        "code": "from sklearn.datasets import load_digits\ndigits = load_digits()\nX = digits.data\ny = digits.target\nprint (X,y)",
        "text": "validate model one of the most important piece of machine learn be   -  model validation  -  , that be , check how well your model fit a give dataset  .  but there be some pitfall you need to watch out for  .  how might we check how well our model fit the data ?",
        "id": 845
    },
    {
        "code": "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\naccuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))",
        "text": "next , we  ll define correct prediction and accuracy metric to track how the network be do  .  the correct prediction formulation work by look at the index of the maximum value of the 2 output value , and then see whether it match with the train label  . ",
        "id": 846
    },
    {
        "code": "import requests\nendpoint = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/aggregate/{project}/{access}/{agent}/{granularity}/{start}/{end}'\nheaders={'User-Agent' : 'https://github.com/yawen32', 'From' : 'liy44@uw.edu'}\nparams = {'project' : 'en.wikipedia.org',\n            'access' : 'desktop',\n            'agent' : 'user',\n            'granularity' : 'monthly',\n            'start' : '2015070100',\n            'end' : '2017100100'\n            }\napi_call = requests.get(endpoint.format(**params))\npageviews_desktop = api_call.json()\nprint(pageviews_desktop)\nimport json\nwith open('pageviews_desktop-site_201507-201709.json','w') as outfile:\n    json.dump(pageviews_desktop,outfile)",
        "text": "get all pageviews by organic ( user ) on the desktop for english wikipedia from july 2015 through september 2017  . ",
        "id": 847
    },
    {
        "code": "\na = np.array([1,2,3,4])  \nb = np.array([(1.5,2,3), (4,5,6)])",
        "text": "numpy array declaration and common errors  - ",
        "id": 848
    },
    {
        "code": "\npops_cols = ['id', 'id2', 'geography', 'pop_census', 'pop_census_base', 'pop_10', 'pop_11', 'pop_12', 'pop_13', 'pop_14', 'pop_15', 'pop_16']\npops.columns = pops_cols\npops.head(1)",
        "text": "again , some of those column name be way too long  .  so we ll shorten them  . ",
        "id": 849
    },
    {
        "code": "pi_approximation(2)\nassert pi_approximation(0) == 0.0\nassert pi_approximation(1) == 4.0\nassert pi_approximation(2) == 4 - 4/3\nassert pi_approximation(10) == 4 * sum((-1)**i / (2*i+1) for i in range(10))\nassert pi_approximation.__doc__ != None",
        "text": "the approximation of $ \\pi $ obtain from the first two term of the series be $ \\tfrac83   -  verify that this be the answer you get ,",
        "id": 850
    },
    {
        "code": "X1pred = test1[['Overall Qual','Total Bsmt SF','Baths','Garage Area','Year Built','Gr Liv Area','TotRms AbvGrd','Fireplaces',\n       'Lot Frontage','Lot Area','Overall Cond']]\nss = StandardScaler()  \nss.fit(X1_train)   \nX1s_train = ss.transform(X1_train)  \nX1ss_test = ss.transform(X1pred)",
        "text": "part 3 d . 3 , preprocessing and scale for test data",
        "id": 851
    },
    {
        "code": "\na = 1\nprint('The real part of a is', a.real)\nprint('The imaginary part of a is', a.imag)",
        "text": "in python , *all* variable be also  thing   .  in the program jargon , these  thing  be call *objects  -  without go into detail that you wo n't need for this lecture , object have so   -   call  attribute  and  method  ( or function )  .  attributes  be information store about the object  .  for example , even simple integer be also  thing with attribute ' ,",
        "id": 852
    },
    {
        "code": "def read_csv(csv_file):\n    f= open(csv_file).read()\n    new_list = f.split(\"\\n\")\n    string_list = new_list[1:len(new_list)]\n    final_list = []\n    for string in string_list:\n        int_fields = []\n        string_fields = string.split(\",\")\n        for str in string_fields:\n            int_fields.append(int(str))\n        final_list.append(int_fields)\n    return(final_list)\ncdc_list = read_csv(\"US_births_1994-2003_CDC_NCHS.csv\")\ncdc_list",
        "text": "convert data into list of list",
        "id": 853
    },
    {
        "code": "result = []\nfor one_number in range(2000,3201):\n    if one_number % 7 == 0 and one_number % 5 !=0:\n        result.append(one_number)\nprint(\",\".join([str(x) for x in result]))",
        "text": "session1 assignment   -   2 problem statement write a program which will find all such number which be divisible by 7 but be not a multiple of 5 , between 2000 and 3200 ( both include )  .  the number obtain should be print in a comma   -   separate sequence on a single line  . ",
        "id": 854
    },
    {
        "code": "start_time = time.time()\nmodel = SVC(C=10, gamma=0.01, kernel=\"rbf\")\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_holdout)\nprint('Execution Time::',time.time() - start_time)\nprint(\"accuracy:\", metrics.accuracy_score(y_true=y_holdout, y_pred=y_pred))",
        "text": "svm ( non linear model , without pca , hyperparameter tune )",
        "id": 855
    },
    {
        "code": "x = np.random.gamma(6, size=200)\np=sns.distplot(x, kde=False, fit=stats.gamma);\nparams=stats.gamma.fit(x) \nparams",
        "text": "fit parametric distribution you can also use distplot ( ) to fit a parametric distribution to a dataset and visually evaluate how closely it correspond to the observe data ,",
        "id": 856
    },
    {
        "code": "plot_data(centroids+2, X, n_samples)",
        "text": "we can see that mean shift cluster ha almost reproduce our original cluster  .  the one exception be the very close cluster , but if we really want to differentiate them we could low the bandwidth  .  what be impressive be that this algorithm nearly reproduce the original cluster without tell it how many cluster there should be  . ",
        "id": 857
    },
    {
        "code": "def generate():\n    for x in range(10):\n        yield x\nZ = np.fromiter(generate(), dtype=float, count= -1) \nZ",
        "text": "consider a generator function that generate 10 integer and use it to build an array (    -  ) (   -  hint  -  , np . fromiter )",
        "id": 858
    },
    {
        "code": "import pandas as pd\ndata = pd.read_csv('iris.csv')\ndata.head()\ndata['sepal_width'].sort_values(ascending = True).head()\ndata['sepal_width'].sort_values(ascending = False).head()\ndata.sort_values(['sepal_width'],ascending = True).head()\ndata.sort_values(['species','petal_length'],ascending = [True,False]).head()",
        "text": "how do i sort panda dataframe or series",
        "id": 859
    },
    {
        "code": "def flatten(d, parent='', sep='_'):\n    items = []\n    for k, v in d.items():\n        key = sep.join([parent, k]) if parent else k\n        try:\n            items.extend(flatten(v, key, sep=sep).items())\n        except AttributeError:\n            items.append((key, v))\n    return dict(items)\nflatten(data[0])",
        "text": "flatten the nest dictionary recursively",
        "id": 860
    },
    {
        "code": "param_grid = {'min_samples_split': range(2,10),\n              'min_samples_leaf': range(1,10)}\nmodel_r = GridSearchCV(ensemble.RandomForestClassifier(), param_grid)\nmodel_r.fit(X_train, y_train)\nbest_index = np.argmax(model_r.cv_results_[\"mean_test_score\"])\nprint(\"Best index:\", model_r.cv_results_[\"params\"][best_index])\nprint(\"Mean test score:\", max(model_r.cv_results_[\"mean_test_score\"]))\nprint(\"Held-out:\", model_r.score(X_test, y_test))",
        "text": "let 's do another grid search to determine the best hyperparameters ,",
        "id": 861
    },
    {
        "code": "from sklearn.model_selection import GridSearchCV \nparam_grid = {\n    'bootstrap': [False],\n    'max_depth': [60, 70, 80],\n    'max_features': ['sqrt'],\n    'min_samples_leaf': [1, 2],\n    'min_samples_split': [2, 3],\n    'n_estimators': [1200, 1400, 1600]\n}\nrf = RandomForestRegressor()\ncv = KFold(n_splits=5, shuffle=True, random_state=7)\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n                           scoring=None,\n                           cv=cv, n_jobs=-1, verbose=3)\ngrid_search.fit(Xtrain, ytrain)",
        "text": "gridsearchcv slight increase in performance with the parameter suggest by randomizedsearchcv  .  next , we use gridsearchcv which iterate over all of the possible combination instead of randomly sample   -  note , user input require in the next section to create the gridsearch parameter grid base on randomizedsearch result   - ",
        "id": 862
    },
    {
        "code": "\ntrain_features_st, train_features, train_labels = shuffle(train_features_st, train_features, train_labels, random_state = 5)\nx_train, x_test, y_train, y_test = train_test_split(train_features, train_labels, test_size=0.1, random_state=200)\nx_train_st, x_test_st, y_train_st, y_test_st = train_test_split(train_features_st, train_labels, test_size=0.1, random_state=200)",
        "text": "split to train and validation set",
        "id": 863
    },
    {
        "code": "\nraceName = '20160820-Mauritius-HydrofoilProTour'\ndfResultsTemp = pd.read_csv(resultDir + raceName + '.csv')\ndfResultsTemp = dfResultsTemp.set_index(dfResultsTemp['HelmName'])\nraceColumns = ['R1','R2','R3','R4','R5','R6','R7','R8','R9','R10','R11','R12','R13','R14','R15','R16','R17']\ndfResultsTemp = cleanResults(raceColumns,dfResultsTemp,0,True)\ndfResults = mergeResults(raceColumns,raceName,dfResultsTemp,dfResults)",
        "text": "load result , august 2016   -   mauritius leg of the hydrofoil pro tour  -  [ raw result ] [ 1 ] [ 1 ] , <url>",
        "id": 864
    },
    {
        "code": "classifier_onevsall_lsa = OneVsRestClassifier(svm.SVC(C=1000., probability=True, kernel='linear'))\nclassifier_onevsall_lsa.fit(train_x_lsa, train_y)\ny_predict_onevsall_lsa = classifier_onevsall_lsa.predict(test_x_lsa)\nconfusion_matrix(test_y, y_predict_onevsall_lsa)\nprint(\"Accuracy : \", accuracy_score(test_y, y_predict_onevsall_lsa))\nprint(classification_report(test_y, y_predict_onevsall_lsa, target_names=classes))",
        "text": "svm multiclass classifier ( onevsrest )",
        "id": 865
    },
    {
        "code": "data = pd.read_csv(\"/data/mobile-sales-data.csv\")\ndata.head()\ntype(data)",
        "text": "load data from csv file into a dataframe",
        "id": 866
    },
    {
        "code": "bm_score = y.value_counts()[0] / len(y)\ntrain_score = model.score(X_train, y_train)\ntest_score = model.score(X_test, y_test)\nprint(\"Accuracy | Benchmark: {:0.3}, Train: {:0.3}, Test: {:0.3}\".format(bm_score, train_score, test_score))",
        "text": "accuracy score on benchmark , train and test set",
        "id": 867
    },
    {
        "code": "obst_flat = 'FLAT'\nobst_sci  = 'OBJECT'\nprint(\"Done\")",
        "text": "set the metadata value for the observation type ( metadata key obs _ type ) ,",
        "id": 868
    },
    {
        "code": "series3 = frame['d']\nframe\nseries3\nframe.sub(series3, axis=0)",
        "text": "if you want to instead broadcast over the column , match on the row , you have to use one of the arithmetic method  .  for example ,",
        "id": 869
    },
    {
        "code": "indices = where(mask)\nindices\nx[indices] # this indexing is equivalent to the fancy indexing x[mask]",
        "text": "function for extract data from array and create array    -  where the index mask can be convert to position index use the where function",
        "id": 870
    },
    {
        "code": "import numpy as np\nnorm_x = X.values\nfor i, name in enumerate(X):\n    if name == \"const\":\n        continue\n    norm_x[:,i] = X[name]/np.linalg.norm(X[name])\nnorm_xtx = np.dot(norm_x.T,norm_x)",
        "text": "condition number one way to ass multicollinearity be to compute the condition number  .  value over 20 be worrisome  .  the first step be to normalize the independent variable to have unit length ,",
        "id": 871
    },
    {
        "code": "header = cleaned_lines[0]\ndata = cleaned_lines[1:]\nheader\ndata[:5]",
        "text": "split the line into  header  and  data  variable the header be the first string in the list of string  .  it contain the column name of our data  . ",
        "id": 872
    },
    {
        "code": "init = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    output = sess.run(x)\n    print(output)",
        "text": "the tf . variable  class create a tensor with an initial value that can be modify , much like a normal python variable  .  this tensor store it state in the session , so you must initialize the state of the tensor manually  .  you ll use the tf . global _ variables _ initializer ( )  function to initialize the state of all the variable tensor  . ",
        "id": 873
    },
    {
        "code": "sentences = [['من!', 'أنت', 'وقالها'],\n             ['من!', 'أنت', 'وقالها']]\nsentences_tekonized = tokenizer.segment_sents(sentences)\nprint(sentences_tekonized)",
        "text": "segmente a list of sentence ( list of list of word )",
        "id": 874
    },
    {
        "code": "\nmissing_values_count = nfl_data.isnull().sum()\nmissing_values_count[0:10]",
        "text": "see how many miss data point we have  _  _  _  ok , now we know that we do have some miss value  .  let 's see how many we have in each column  . ",
        "id": 875
    },
    {
        "code": "\nplt.plot(cnnhistory.history['acc'])\nplt.plot(cnnhistory.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.plot(cnnhistory.history['loss'])\nplt.plot(cnnhistory.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()",
        "text": "plot the accuracy and loss graph",
        "id": 876
    },
    {
        "code": "\nlogit = LogisticRegression(penalty='l1', solver='liblinear', C=1, warm_start=False,\n                           fit_intercept=True, n_jobs=-1)\ncv_models(logit, 5)",
        "text": "logistic regression 2nd attempt run the model with randomizedsearchcv 's  best parameter   . ",
        "id": 877
    },
    {
        "code": "from sklearn.svm import LinearSVC\nsvm = LinearSVC() \nsvm.fit(X_train, y_train);\nsvm.predict(X_train)\nprint(svm.score(X_train, y_train))\nprint(svm.score(X_test, y_test))",
        "text": "let 's try a linear svm",
        "id": 878
    },
    {
        "code": "\nresponse = requests.get('http://api.nytimes.com/svc/books/v2/lists/2009-06-21/hardcover-fiction.json?api-key=3880684abea14d86b6280c6dbd80a793')\ndata = response.json()\nbook_result = data['results']\nprint(\"The hardcover Fiction NYT best-sellers on fathers day in 2009 are:\")\nfor i in book_result:\n    for item in i['book_details']:\n        print(\"-\", item['title'])",
        "text": "what book top the hardcover fiction nyt best   -   seller list on mother 's day in 2009 and 2010 ? how about father 's day ?",
        "id": 879
    },
    {
        "code": "x = ['Product A', 'Product B', 'Product C']\ny = [20, 14, 23]\ntrace0 = go.Bar(\n    x = x,\n    y = y,\n    text = y,\n    textposition = 'auto',\n    marker = dict(\n        color = 'rgb(158,202,225)',\n        line = dict(\n            color = 'rgb(8,48,107)',\n            width = 1.5\n        )\n    ),\n    opacity = 0.6\n)\ndata = [trace0]\nplotly.offline.iplot(data)",
        "text": "bar chart with direct label",
        "id": 880
    },
    {
        "code": "arr = np.arange(9).reshape((3,3))\narr\narr.flatten()\narr.flatten(1)\narr.ravel()",
        "text": "flatten reshape from a high dimensional to one dimensional order be call flatten",
        "id": 881
    },
    {
        "code": "\ncity = ox.gdf_from_place('Berkeley, California, USA')\nox.save_gdf_shapefile(city)\ncity = ox.project_gdf(city)\nfig, ax = ox.plot_shape(city, figsize=(3,3))\nplace_names = ['Berkeley, California, USA', \n               'Albany, California, USA']\neast_bay = ox.gdf_from_places(place_names)\nox.save_gdf_shapefile(east_bay)\neast_bay = ox.project_gdf(east_bay)\nfig, ax = ox.plot_shape(east_bay)",
        "text": "get boundary polygon and network from openstreetmap osmnx let you download spatial  place boundary  geometry from openstreetmap , save them to shapefiles , project them , and plot them  .  for a more in   -   depth demonstration of create these shapefiles , see [ osmnx example ] ( <url> )  . ",
        "id": 882
    },
    {
        "code": "\nfrom IPython.display import Image\ni = Image(filename='./notMNIST_small/A/MDEtMDEtMDAudHRm.png')\ndisplay(i)",
        "text": "let 's take a peek at some of the data to make sure it look sensible  .  each exemplar should be an image of a character a through j render in a different font  .  display a sample of the image that we just download  .  hint , you can use the package ipython . displ",
        "id": 883
    },
    {
        "code": "sum((mult_regression_model_one.predict(training_data[list(columns_one)].values) - training_data['price'].values) ** 2)\nsum((mult_regression_model_two.predict(training_data[list(columns_two)].values) - training_data['price'].values) ** 2)\nsum((mult_regression_model_three.predict(training_data[list(columns_three)].values) - training_data['price'].values) ** 2)",
        "text": "calculate r for each model",
        "id": 884
    },
    {
        "code": "z1 = np.complex(1, 2)\nz2 = np.complex(0.3, 0.7)\nz3 = np.complex(np.sqrt(3)/2, 1.0/2.0)\nz4 = square_and_subtract_one(z1)\nz5 = square_and_subtract_one(z2)\nz6 = square_and_subtract_one(z3)\nprint(\"The input \" + round_complex(z1) + \" produced the output \" + round_complex(z4))\nprint(\"The input \" + round_complex(z2) + \" produced the output \" + round_complex(z5))\nprint(\"The input \" + round_complex(z3) + \" produced the output \" + round_complex(z6))",
        "text": "let 's try call the previous function on these complex number ,",
        "id": 885
    },
    {
        "code": "import numpy\nimport math\nnumpy.random.seed(1)\nincome = income.reindex(numpy.random.permutation(income.index))\ntrain_max_row = math.floor(income.shape[0] * .8)\ntrain = income.iloc[:train_max_row]\ntest = income.iloc[train_max_row:]",
        "text": "we ll want to split our data into train and test set first  .  if we do n't , we ll be make prediction on the same data that we train our algorithm with  .  this lead to overfitting , and will make our error appear low than it be  . ",
        "id": 886
    },
    {
        "code": "\nl = [1,1,1,2,2,2,2,3,3,3,3,4]\nl\nset(l)\nl",
        "text": "notice how it wo n't place another 1 there  .  that 's because a set be only concern with unique element  .  we can cast a list with multiple repeat element to a set to get the unique element  . ",
        "id": 887
    },
    {
        "code": "def count_BP_unknown(): \n    counter ={}    \n    for line_num, line in enumerate(fastq[0:40]):  \n        if line_num%4==1:     \n            line=line.rstrip()   \n            for base_number, base in enumerate(line): \n                if base_number != (len(line)-1): \n                    kmer = line[base_number]+line[base_number +1]   \n                    if kmer in counter:    \n                        counter[kmer]+=1     \n                    else: counter[kmer]=1      \n    print(counter)\ncount_BP_unknown()",
        "text": "count the number of each pair of base , without assume you know in advance the possible pair ( i . e  .  start with an empty dictionary )  . ",
        "id": 888
    },
    {
        "code": "fig = plt.figure(figsize=(7,6))\nax = fig.gca()\nax.scatter(students['A']['days'], students['A']['morale'],\n           s=70, c='darkred', label='student A', alpha=0.7)\nax.set_xlabel('days', fontsize=16)\nax.set_ylabel('morale', fontsize=16)\nax.set_title('Morale over time\\n', fontsize=20)\nax.set_xlim([0, 85])\nplt.legend(loc='upper left')\nplt.show()",
        "text": "student a morale over time plot of student a 's morale over the day  .  the true function be also plot in yellow  . ",
        "id": 889
    },
    {
        "code": "test_dataset = dataset[dataset.index % test_indis == 0]\ntest_dataset.describe()",
        "text": "extract test dataset from dataset",
        "id": 890
    },
    {
        "code": "tanic['Single_male'] = single_dummy['Single_male']\ntanic['Single_female'] = single_dummy['Single_female']\ntanic.head()\ntanic.describe()",
        "text": "add single _ male to the tanic dataframe",
        "id": 891
    },
    {
        "code": "data = pd.read_csv(url, sep=\"\\s+\", parse_dates=[[0, 1, 2]])\ndata.head()",
        "text": "assign it to a variable call data and replace the first 3 column by a proper datetime index  . ",
        "id": 892
    },
    {
        "code": "model2 = KMeans(n_clusters=2)\nXs = [get_vector_short(context) for context in sentences1]\nmodel2.fit(Xs)\ncollect_stat_as_table(model2.labels_, true_indexes1, chains1)\nsave_as(u'әле_2_table2.csv', sentences1, model2.labels_)",
        "text": "kmeans with 2 cluster and with encoder",
        "id": 893
    },
    {
        "code": "mortality_dict  = {}\nfor country in document.iterfind('country'):\n    mortality = country.find('infant_mortality')\n    if mortality is not None:\n        mortality_dict[country.find('name').text] = float(mortality.text)\nmortality = pd.DataFrame(mortality_dict.items(), columns=['name', 'infant_mortality'])\nmortality.sort_values(by = 'infant_mortality').head(10)",
        "text": "country with the low infant mortality rate",
        "id": 894
    },
    {
        "code": "def find_n_reviews(df, movie_id):\n    \n    \n    return n_reviews\nn_toy_story = find_n_reviews(favorable, 1)\nprint(n_toy_story)\nassert_is_instance(n_toy_story, int)\ntest = [find_n_reviews(favorable, n) for n in range(1, 6)]\ntest_pd = favorable_pd.groupby('movieId').size()[:5].tolist()\nassert_equal(test, test_pd)",
        "text": "write a function that , give a movieid  , compute the number of review for that movie  . ",
        "id": 895
    },
    {
        "code": "arr = np.arange(9).reshape((3, 3))\narr[:, [1, 0, 2]]",
        "text": "< span style=  color , red  > 16 . how to swap two column in a 2d numpy array ?   q .  swap column 1 and 2 in the array   -  arr  - ",
        "id": 896
    },
    {
        "code": "results = smf.logit('gunlaw ~ age + age2 + educ + educ2 + C(sex)', data=gss).fit()",
        "text": "now we can run a logistic regression model",
        "id": 897
    },
    {
        "code": "rec = oo[oo.Edition >= 1984]\nrec.NOC.value_counts().head(3)",
        "text": "which three country have win the most medal in recent year ( from 1984 to 2008 ) ?",
        "id": 898
    },
    {
        "code": "imgnpl, imgnph = util.tikhonov_filter(pad(imgn), fltlmbd, npd)\nW = spl.irfftn(np.conj(spl.rfftn(D, imgnph.shape, (0, 1))) *\n               spl.rfftn(imgnph[..., np.newaxis], None, (0, 1)),\n               imgnph.shape, (0,1))\nW = W**2\nW = 1.0/(np.maximum(np.abs(W), 1e-8))\nlmbda = 4.8e-2\nopt = cbpdn.ConvBPDN.Options({'Verbose': True, 'MaxMainIter': 250,\n            'HighMemSolve': True, 'RelStopTol': 3e-3, 'AuxVarObj': False,\n            'L1Weight': W, 'AutoRho': {'Enabled': False}, 'rho': 4e2*lmbda})",
        "text": "set solver option  .  see section 8 of [ [ 41 ] ] ( <url> ) for detail of construction of $ \\ell _ 1 $ weight matrix $ w   - ",
        "id": 899
    },
    {
        "code": "hide_code\ny_train2_c = np.array([np.argmax(y) for y in y_train2])\ny_test2_c = np.array([np.argmax(y) for y in y_test2])\nclf = GradientBoostingClassifier().fit(x_train2.reshape(-1, 32*32), y_train2_c)\nclf.score(x_test2.reshape(-1, 32*32), y_test2_c)\nhide_code\nclf2 = RandomForestClassifier().fit(x_train2.reshape(-1, 32*32), y_train2_c)\nclf2.score(x_test2.reshape(-1, 32*32), y_test2_c)",
        "text": "let 's compare the result with classify algorithm  . ",
        "id": 900
    },
    {
        "code": "probs = gnb_model.predict_proba(X_test)\nprint(probs)\npredict = gnb_model.predict(X_test)\nprint(predict)\nmetrics.confusion_matrix(y_test,predict)\nprint(metrics.classification_report(y_test,predict))",
        "text": "predict and score the model",
        "id": 901
    },
    {
        "code": "oz = cast[cast['name'] == 'Frank Oz']\noz_roles = oz.groupby(['character']).size()\noz_roles[oz_roles > 1].sort_values()",
        "text": "< div class=  alert alert   -   success  >   exercise   ,     list each of the character that frank oz ha portray at least twice . ",
        "id": 902
    },
    {
        "code": "from azureml.core.runconfig import CondaDependencies\ncd = CondaDependencies.create()\ncd.add_tensorflow_conda_package()\ncd.add_conda_package('keras')\ncd.save_to_file(base_directory='./', conda_file_path='myenv.yml')\nprint(cd.serialize_to_string())",
        "text": "create myenv . yml we also need to create an environment file so that azure machine learn can install the necessary package in the docker image which be require by your score script  .  in this case , we need to specify package numpy  , tensorflow   . ",
        "id": 903
    },
    {
        "code": "from sklearn.feature_extraction.text import CountVectorizer\ncv= CountVectorizer(max_features= 1500)  \nX= cv.fit_transform(corpus).toarray()\ny= dataset.iloc[:,1].values",
        "text": "create bag of word model",
        "id": 904
    },
    {
        "code": "\nimport pandas as pd\nimport numpy as np\nraw_data = {'first_name': ['Jason', 'Jason', 'Tina', 'Jake', 'Amy'], \n        'last_name': ['Miller', 'Miller', 'Ali', 'Milner', 'Cooze'], \n        'age': [42, 42, 36, 24, 73], \n        'preTestScore': [4, 4, 31, 2, 3],\n        'postTestScore': [25, 25, 57, 62, 70]}\ndf = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'preTestScore', 'postTestScore'])\ndf",
        "text": "iterate through the row of multiple column in panda    -  author ,   -  [ chris albon ] ( <url> / ) , [   -   chrisalbon ] ( <url> )    -  date ,   -  repo ,   -  [ python 3 code snippet for data science ] ( <url> )    -  note ,   - ",
        "id": 905
    },
    {
        "code": "def get_school_dist(DBN):\n    return DBN[:2]\ncombined[\"school_dist\"] = combined[\"DBN\"].apply(get_school_dist)",
        "text": "add school district col for map",
        "id": 906
    },
    {
        "code": "trainprevloans.head()\ntrainprevloans['referredby'].fillna(value='Other', inplace=True)\nassert trainprevloans.isnull().sum().values.sum() == 0\ntitle_case(trainprevloans)\ntrainprevloans.info()",
        "text": "< a id=trainprevloans  >      -  trainprevloans . csv",
        "id": 907
    },
    {
        "code": "rmse_cv(model_lasso).mean()\ncoef = pd.Series(model_lasso.coef_, index = X_train.columns)\nprint(\"O modelo Lasso escolheu \" + str(sum(coef != 0)) + \" atributos e eliminou os outros \" +  str(sum(coef == 0)) + \" atributos do dataset para construção do modelo.\")\nimp_coef = pd.concat([coef.sort_values().head(1),\n                     coef.sort_values().tail(2)])\nmatplotlib.rcParams['figure.figsize'] = (4.0, 5.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")",
        "text": "on the first model lasso , it be find a mean rmse of 0 . 5915",
        "id": 908
    },
    {
        "code": "lasso_cv = lm.Lasso(A, fit_intercept = True)\nlasso_cv.fit(wine_predictors.as_matrix(), quality.as_matrix())\nprint(\"Predictor coefficients:\")\nfor col, coeff in zip(predictor_cols, lasso_cv.coef_):\n    print(\"{}: {}\".format(col, coeff))\nprint(\"Intercept: {}\".format(lasso_cv.intercept_))",
        "text": "lasso coefficient   -   intercept for each of the predictor a determine by the ridge weight vector  . ",
        "id": 909
    },
    {
        "code": "[i.upper() for i in strings]",
        "text": "with a list comprehension",
        "id": 910
    },
    {
        "code": "salary_data['is_senior'] = salary_data['title'].str.contains('Senior').astype(int)\nsalary_data['is_director'] = salary_data['title'].str.contains('Director').astype(int)\nsalary_data['is_manager'] = salary_data['title'].str.contains('Manager').astype(int)\nmodel = sm.logit(\"HighSalary ~ city + is_senior + is_manager\", data=salary_data).fit()\nmodel.summary()",
        "text": "create a few new variable in your dataframe to represent interest feature of a job title   -  for example , create a feature that represent whether senior  be in the title   -   or whether manager  be in the title   -  then build a new logistic regression model with these feature  .  do they add any value ?",
        "id": 911
    },
    {
        "code": "Counter2 = 1\nmin_country = {}\nfor key, value in airport_count.items():\n    if value <= Counter2:\n        Counter2 = value\n        min_country['country'] = key\n        min_country['count'] = value\nprint(min_country)\nprint(min(airport_count.values()))",
        "text": "find the country with the last entry",
        "id": 912
    },
    {
        "code": "dt_zps = pd.merge(pd.read_sql_table('Detected', engine),\n                  pd.read_sql_query(\"\"\"SELECT \n                                        Detected.id,\n                                        Simulated.app_mag as sim_mag,\n                                        Simulated.id as sim_id \n                                    FROM Detected\n                                        LEFT JOIN Reals\n                                            ON Detected.id==Reals.detected_id\n                                        LEFT JOIN Simulated\n                                            ON Simulated.id==Reals.simulated_id\"\"\", engine),\n                                      on='id', suffixes=('',''))\ndt_ois = pd.merge(pd.read_sql_table('DetectedOIS', engine),\n                  pd.read_sql_query(\"\"\"SELECT \n                                        DetectedOIS.id,\n                                        Simulated.app_mag as sim_mag,\n                                        Simulated.id as sim_id \n                                    FROM DetectedOIS\n                                        LEFT JOIN RealsOIS\n                                            ON DetectedOIS.id==RealsOIS.detected_id\n                                        LEFT JOIN Simulated\n                                            ON Simulated.id==RealsOIS.simulated_id\"\"\", engine),\n                                      on='id', suffixes=('',''))\n\n# dt_zps.IS_REAL = dt_zps.IS_REAL.astype(int)",
        "text": "query the database to obtain the table relate to the detection  . ",
        "id": 913
    },
    {
        "code": "import math\ndef isprime(x):\n    for k in range(2,int(math.sqrt(x))+1):\n        if x%k == 0:\n            return False\n    return True\nn = 2\ni = 3\nwhile n!=10001:\n    i += 2\n    if isprime(i):\n        n+=1    \nprint(i)",
        "text": "by list the first six prime number , 2 , 3 , 5 , 7 , 11 , and 13 , we can see that the 6th prime be 13 .  what be the 10,001st prime number ?",
        "id": 914
    },
    {
        "code": "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, train_size = 0.8)",
        "text": "finally , i split the dataset in train and test set ,",
        "id": 915
    },
    {
        "code": "\nimport numpy as np\nimport csv\nimport keras as kr\nimport tensorflow as tf \nimport sklearn.cross_validation as cv\niris = list(csv.reader(open('iris.csv')))[1:]\ninputs  = np.array(iris)[:,:4].astype(np.float)\n \noutputs = np.array(iris)[:,4]\noutputs_vals, outputs_ints = np.unique(outputs, return_inverse=True)\noutputs_cats = kr.utils.to_categorical(outputs_ints)\niris",
        "text": "use tensorflow to create model use tensorflow to create a model to predict the specie of iris from a flower  s sepal width , sepal length , petal width , and petal length  . ",
        "id": 916
    },
    {
        "code": "\ncount = movies['vote_average']\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nplt.scatter(x=movies['vote_average'], y=movies['budget'])\nplt.title('Budget by Vote Average')\nplt.xlabel('Vote Average')\nplt.ylabel('Budget by 100 million')\nplt.subplot(1, 2, 2)\nplt.scatter(x=movies['vote_average'], y=movies['revenue'])\nplt.title('Revenue by Vote Average')\nplt.xlabel('Vote Average')\nplt.ylabel('Revenue by billion') \nplt.show()",
        "text": "vote average and budget , revenue",
        "id": 917
    },
    {
        "code": "sentence = 'This is a sentence; please slice it.'\nsentence[0:4]\nsentence[5:7]\nsentence[8:9]\nsentence[10:18]\nsentence[20:26]\nsentence[27:32]\nsentence[33:35]",
        "text": "use slice to extract each word from",
        "id": 918
    },
    {
        "code": "unmatched_schools = schools.loc[schools['match_id'].isnull(), :].copy()\nprint(len(unmatched_schools), 'Schools don\\'t have a corresponding budget row')\nunmatched_schools['cleaned_name']",
        "text": "check for school with no budget record which school be not match with a budget row",
        "id": 919
    },
    {
        "code": "a = [1,2,3,4]\nb = [5,6,7,8]\nc = [-1,-2,1,2]\nlist(map(lambda x,y,z:x+y-z,a,b,c))",
        "text": "map ( ) can also be apply to more than one list but the list must have the same length  .  map ( ) will apply it lambda function to the element of the argument list , i . e  .  it first apply to the element with the 0th index , then to the element with the 1st index until the n   -   th index be reach ,",
        "id": 920
    },
    {
        "code": "cited_utility = cited_patents_level.groupby('patent_number')['cited_patent_number'].apply(lambda x: np.count_nonzero(x.str.contains('\\d{7}'))).reset_index().rename(index=str, columns={'cited_patent_number':'num_utility_cited'})\ncited_utility.head()\nmaster = pd.merge(master, cited_utility, how='left', on='patent_number')\nmaster.head()",
        "text": "number of cite utility patent",
        "id": 921
    },
    {
        "code": "optimized_features = optimized_solution('nbc')\noptimized_features",
        "text": "combine different feature together can often lead to a more accurate classification system  .  the optmize _ solution function wa build to go through different combination of classification feature , and yield out a result of feature that lead to the most accurate solution ,",
        "id": 922
    },
    {
        "code": "temp2 = df_merged[df_merged['regionidcounty'] == 3101]\ntemp2.groupby('regionidzip').count()\ntemp2['regionidzip'].mode()",
        "text": "all have the same county  .  get all entry in that county",
        "id": 923
    },
    {
        "code": "criteria_series = pd.Series(criteria_list, index=['R1', 'R3', 'R4'])\ndf.isin(criteria_series)",
        "text": "if the pass argument be a series , all value of df  will be check to see if they match any of the criterion value in specific row a define by the index of the series  .  all value in row of df  that be not check return false   .  note the value for row 2/column 1 be false  a compare to above  . ",
        "id": 924
    },
    {
        "code": "df.groupby('key1')['data1'].mean() \ndf.groupby('key1')[['data1']].mean() \ndf.groupby(['key1', 'key2'])[['data2']].mean()",
        "text": "select a column or subset of column index a groupby object create from a dataframe with a column name or array of column name ha the effect of select those column for aggregation  . ",
        "id": 925
    },
    {
        "code": "names_regex = re.compile(r'Agent \\w+')\nnames_regex.sub('CENSORED', 'Agent Alice gave the secret documents to Agent Bob.')",
        "text": "substitute string with the sub ( ) method the sub ( ) method for regex object be pass two argument , 1 .  the first argument be a string to replace any match  .  1 .  the second be the string for the regular expression  .  the sub ( ) method return a string with the substitution apply ,",
        "id": 926
    },
    {
        "code": "test_stat_dist = np.array([test_stat(run_model(data)) \n                           for i in range(1000)])",
        "text": "that 's the result of one simulate experiment  .  now we run the experiment 1000 time and collect the result  . ",
        "id": 927
    },
    {
        "code": "drinks.beer.describe()\ndrinks[[\"beer\",\"wine\",\"spirit\"]].plot(kind=\"box\")\nsns.boxplot(drinks[[\"beer\",\"wine\",\"spirit\"]])\nsns.boxplot(x=\"continent\",y=\"beer\",data=drinks)\nsns.boxplot(x=\"continent\",y=\"beer\",data=drinks)\nsns.stripplot(x=\"continent\",y=\"beer\",data=drinks,jitter=False)",
        "text": "boxplot another useful way of visualize the distribution of a variable  .  a box plot show the quantiles of a distribution  . ",
        "id": 928
    },
    {
        "code": "import numpy as np\nimport helper\n(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()",
        "text": "check point if you ever decide to come back to this notebook or have to restart the notebook , you can start from here  .  the preprocessed data ha be save to disk  . ",
        "id": 929
    },
    {
        "code": "g = sns.FacetGrid(titanic, col=\"Survived\", row=\"Pclass\", hue=\"Sex\", size=3)\ng.map(sns.kdeplot, \"Age\", shade=True)\nsns.despine(left=True, bottom=True)\nplt.show()",
        "text": "create conditional plot use three condition",
        "id": 930
    },
    {
        "code": "opt2 = cbpdndlmd.ConvBPDNMaskDictLearn.Options({'Verbose': True,\n            'MaxMainIter': 200, 'AccurateDFid': True,\n            'CBPDN': {'rho': 20.0*lmbda + 0.5},\n            'CCMOD': {'rho': 2e-1}}, dmethod='cns')\nd2 = cbpdndlmd.ConvBPDNMaskDictLearn(D0, sh, lmbda, W, opt2, dmethod='cns')\nD2 = d2.solve()",
        "text": "cdl with a spatial mask use [ cbpdndlmd . convbpdnmaskdictlearn ] ( <url> )  .  ( note that [ prlcnscdl . convbpdnmaskdcpldictlearn _ consensus ] ( <url> ) solve the same problem , but be substantially fast on a multi   -   core architecture  .  )",
        "id": 931
    },
    {
        "code": "safety_offset(source_diameter, -dsource_position, actual_collimator_size, collimator_offset + dcollimator_offset, \n              source_to_collimator, collimator_to_fp_end, \n              field_plate_spacing/2.0 - field_plate_guard_ring_offset)",
        "text": "bad case close trajectory to the guard ring with measure collimator offset and collimator size and estimate uncertainty in the collimator offset , and estimate uncertainty in beam source alignment  . ",
        "id": 932
    },
    {
        "code": "X.Edu.value_counts()\nX[X.Edu==\"None\"]['MarStat'].value_counts()\nX[X.MarStat==\"Forever Alone\"]['Edu'].value_counts()\nX[X.MarStat==\"Single\"]['Edu'].value_counts()\nX[X.MarStat==\"In a relationship\"]['Edu'].value_counts()\nglobal edu_flip_sw\ndef setEdu(x):\n    global edu_flip_sw\n    if x == \"None\":\n        edu_flip_sw *= -1\n        if edu_flip_sw == 1:\n            return \"Bachelor's degree\"\n        else:\n            return \"Some college\"\n    else:\n        return x\nedu_flip_sw = 1\nX['Edu'] = X['Edu'].map(setEdu)\nX.Edu.value_counts()",
        "text": "data cleanse   -   education feature",
        "id": 933
    },
    {
        "code": "plt.plot(days, morale_true, lw=7., c='gold', alpha=0.3, label='true function')\nplt.scatter(students['A']['days'], students['A']['morale'],\n            s=70, c='darkred', label='student A', alpha=0.7)\nplt.xlabel('days', fontsize=16)\nplt.ylabel('morale', fontsize=16)\nplt.title('Morale over time\\n', fontsize=20)\nplt.xlim([0, 85])\nplt.legend(loc='upper left')",
        "text": "< a id=student   -   a  >      -  student a 's morale over time    -  below we can plot student a 's morale at each day  .  the true function be also plot in yellow  . ",
        "id": 934
    },
    {
        "code": "clf = neighbors.KNeighborsClassifier()\nclf.fit(X_train, y_train)\nprint(clf)",
        "text": "build and train your model with train data",
        "id": 935
    },
    {
        "code": "\ndata.alchemy_category.unique()\ndata.groupby('alchemy_category').label.mean()\ndata.groupby('alchemy_category').label.mean().plot(kind='bar')",
        "text": "exercise , 4 .  doe category in general affect evergreeness ? plot the proportion of evergreen site for all alchemy category  . ",
        "id": 936
    },
    {
        "code": "from panda.methods.denoise import eog_regress\nY, S, N = good_data(111, 18, p_global)\n(S_hat, p_local) = eog_regress(Y, {}, p_global)\nsparklines(S, Y[p_global['eeg_chans']], S_hat, '')\nplt.show()",
        "text": "qualatative assessment ( good ) here we sample many signal and see whether or not visually our denoised signal be close to our latent signal than the noisy signal be  .  this be show affirmatively below  . ",
        "id": 937
    },
    {
        "code": "counties = shapes.merge(pops_change_1516, left_on='geoid', right_on='id2')\ncounties.shape \ncounties.head(1)",
        "text": "now we need to merge the dataframe contain the geographic data with the dataframe contain the population data  . ",
        "id": 938
    },
    {
        "code": "pdf = pd.read_csv(my_file_csv)\npdf['waterpct'] = (100.0*pdf.waterobs)/pdf.clearobs\npdf.head()\npdf.shape\npdf.dtypes\ndf_aus_yearly=pdf.groupby('year').sum()\ndf_aus_yearly['clearobs'].sum()  \ndf_aus_yearly['waterpix'].plot(kind='bar')\ndf_aus_yearly['waterpct'] = 100.0*df_aus_yearly.waterobs/df_aus_yearly.clearobs\ndf_aus_yearly['waterpct'].plot(kind='bar')",
        "text": "use panda to handle heterogeneous tabular data structure with label ax <url>",
        "id": 939
    },
    {
        "code": "red_wines_df = pd.read_csv('data/winequality-red.csv', delimiter=';')\nwhite_wines_df = pd.read_csv('data/winequality-white.csv', delimiter=';')\nred_wines_df.columns\nwhite_wines_df.columns\nred_wines_quality_df = red_wines_df.groupby('quality').mean()['fixed acidity'].reset_index()\nred_wines_quality_df.head()\nwhite_wines_quality_df = white_wines_df.groupby('quality').mean()['fixed acidity'].reset_index()\nwhite_wines_quality_df.head()\npd.merge(red_wines_quality_df, white_wines_quality_df, on=['quality'], suffixes=[' red', ' white'])",
        "text": "let 's read in a different data set , since we re look at combine multiple data source  . ",
        "id": 940
    },
    {
        "code": "unempl = Series([6.0, 6.0, 6.1], index=[2, 3, 4])\ndf_3['unempl'] = unempl\ndf_3",
        "text": "assign a series to a column ( note if assign a list or array , the length must match the dataframe , unlike a series ) ,",
        "id": 941
    },
    {
        "code": "model_vgg19_retrainable_layers = create_model_with_retrainable_layers(\"vgg19\", image_size=image_size)\nmodel_trained_vgg19_retrainable_layers, history_vgg19_retrainable_layers = train_model(epochs=20, lr=1e-5, model=model_vgg19_retrainable_layers, train_folder=train_folder,\n            test_folder=test_folder, train_batchsize=50, val_batchsize=10, image_size=image_size,\n                                                     filename=\"vgg19_lats4.h5\")\nplot_training_process(history_vgg19_retrainable_layers)\nshow_result(model=model_trained_vgg19_retrainable_layers, image_size=image_size, test_folder=test_folder)",
        "text": "retrain last 4 layer",
        "id": 942
    },
    {
        "code": "criterion = nn.BCELoss()\noptimizer_discriminator = torch.optim.Adam(params=discriminator.parameters(), lr=learning_rate_d)\noptimizer_generator = torch.optim.Adam(params=generator.parameters(), lr=learning_rate_g)",
        "text": "we define the loss function and the optimization scheme ( here   -  adam  -  ) for both the discriminator and the generator  . ",
        "id": 943
    },
    {
        "code": "ufo = pd.read_csv('http://bit.ly/uforeports')\nufo.head()\nufo.shape\nufo.drop('Colors Reported', axis=1, inplace=True)\nufo.head()\nufo.drop(['City', 'State'], axis=1, inplace=True)\nufo.head()\nufo.drop([0, 1], axis=0, inplace=True)\nufo.head()\nufo.shape",
        "text": "how do i remove column from a panda dataframe ?",
        "id": 944
    },
    {
        "code": "sess = tf.Session()\nnode3 = tf.add(node1,node2)\nprint(\"node3:\",node3)\nprint(\"sess.run(node3):\",sess.run(node3))",
        "text": "we can further build more complicate computation by combine tensor node with operation ( addition , multiplication , etc )   -  operation be also node in tensorflow   - ",
        "id": 945
    },
    {
        "code": "df['Cluster'].value_counts()\ngrouped = df['Cluster'].groupby(df['Name']) \ngrouped.mean() #average rank (1 to 100) per cluster",
        "text": "number of people per cluster",
        "id": 946
    },
    {
        "code": "Y = (labels=='positive').astype(np.int_)\nrecords = len(labels)\nshuffle = np.arange(records)\nnp.random.shuffle(shuffle)\ntest_fraction = 0.9\ntrain_split, test_split = shuffle[:int(records*test_fraction)], shuffle[int(records*test_fraction):]\ntrainX, trainY = word_vectors[train_split,:], to_categorical(Y.values[train_split].flatten(), 2)\ntestX, testY = word_vectors[test_split,:], to_categorical(Y.values[test_split].flatten(), 2)",
        "text": "train , validation , test set now that we have the word _ vectors , we re ready to split our data into train , validation , and test sets . here we re use the function to _ categorical  from tflearn to reshape the target data so that we ll have two output unit and can classify with a softmax activation function  . ",
        "id": 947
    },
    {
        "code": "array21 = np.array([[0,1],[1,0]])\nnp.tile(array21,(4,4))",
        "text": "create a checkerboard 8x8 matrix use the tile function (    -  ) (   -  hint  -  , np . tile )",
        "id": 948
    },
    {
        "code": "\ndf.createOrReplaceTempView('HVAC')\nspark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10').show()\ndf.where('BuildingAge >= 10').createOrReplaceTempView('OldBuildings')\nspark.sql('SELECT HVACproduct, COUNT(*) FROM OldBuildings GROUP BY HVACproduct').show()\nd1 = spark.sql('SELECT * FROM HVAC WHERE BuildingAge >= 10')\nd1.groupBy('HVACproduct').count().show()\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\nslen = udf(lambda s: len(s), IntegerType())\ndf.select('*', slen(df['Country']).alias('slen')).show()\nspark.udf.register('slen', lambda s: len(s), IntegerType())\nspark.sql('SELECT *, slen(Country) AS slen FROM HVAC').show()",
        "text": "embed sql query you can also run sql query over dataframes once you register them a temporary table within the sparksession  . ",
        "id": 949
    },
    {
        "code": "ex = pd.DataFrame(data = {'word': vocab, 'number of appearance': dist}).sort_values(by = 'number of appearance', \n                                                                                    ascending = False).head(500)\nsns.set_style(\"whitegrid\")\nax = sns.barplot( x=\"number of appearance\", y=\"word\", data=ex)\nplt.title('Frequency of words')\nplt.show()",
        "text": "we observe that some of the most represent word be relate to emotion (  great  ,  like  ,  enjoy  ,  love  ,   -  which be important for our application )  .  we also observe that the number of appearance be rapidly deacreasing  . ",
        "id": 950
    },
    {
        "code": "lst = ['a', 'a', 'a']\nlst\nlen(set(lst)) == 1\nall(x == lst[0] for x in lst)\nlst.count(lst[0]) == len(lst)",
        "text": "check if all the element in a list be equal",
        "id": 951
    },
    {
        "code": "urban_rides = urban[\"ride_id\"].count()\nsuburban_rides = suburban[\"ride_id\"].count()\nrural_rides = rural['ride_id'].count()\ntotal_rides = [urban_rides, suburban_rides, rural_rides]\nexplode = [.1,0,0]\nlabels = [\"Urban\", \"Suburban\", \"Rural\"]\ncolors = [\"coral\", \"lightblue\", \"gold\"]\nplt.pie(total_rides, labels=labels, colors=colors, autopct='%1.2f%%', startangle=55, explode=explode, shadow=True)\nplt.title(\"% of Total Rides by City Type\")\nsns.set()\nplt.show()",
        "text": "total ride by city type    - ",
        "id": 952
    },
    {
        "code": "ecom[ecom['Job'] == 'Lawyer'].info()\nsum(ecom['Job'] == 'Lawyer')\necom[ecom['Job'] == 'Lawyer'].count()",
        "text": "how many people have the job title of  lawyer  ?",
        "id": 953
    },
    {
        "code": "df2 = df.groupby(by=['Day of Week','Month']).count()['Reason'].unstack()\ndf2.head()\nplt.figure(figsize=(12,6))\nsns.heatmap(df2,cmap='coolwarm')\nplt.title('Count of 911 Calls in particular day and month')\nsns.clustermap(df2,figsize=(12,9))",
        "text": "each map represent the amount of the 911 call do every day in particular month   - ",
        "id": 954
    },
    {
        "code": "\nstoplist = set('for a of the and to in'.split())\ntexts = [[word for word in document.lower().split() if word not in stoplist]\n         for document in documents]\nprint(texts)\nfrom collections import defaultdict\nfrequency = defaultdict(int)\nfor text in texts:\n    for token in text:\n        frequency[token] += 1\n        \ntexts = [[token for token in text if frequency[token] > 1] for text in texts]\nprint(texts)\nfrom pprint import pprint\npprint(texts)",
        "text": "first , let  s tokenize the document , remove common word ( use a toy stoplist ) a well a word that only appear once in the corpus ,",
        "id": 955
    },
    {
        "code": "\nlevel2 =  train.groupby('cat2')\nrank_level2 = pd.DataFrame(level2.mean()).sort_values(by='price')\ntop_cat2 = rank_level2.tail(15).reset_index()\ntop_cat2_list = top_cat2.cat2.unique().tolist()\ntop_cat2_full = train.loc[train['cat2'].isin(top_cat2_list)]\nplt.figure(figsize=(20,20))\nsns.boxplot(y ='cat2',x= 'price', data = top_cat2_full, orient = 'h')\nplt.title('Top 15 second levels categories with highest prices ', fontsize = 30)\nplt.ylabel ('Second level categories', fontsize = 20)\nplt.xlabel ('Price', fontsize = 20)",
        "text": "top 15 second level category with high price",
        "id": 956
    },
    {
        "code": "seventh_pipeline = Pipeline([\n    ('pca', PCA()),\n    ('skb', SelectKBest(k=40)),\n    ('rf', RandomForestClassifier())\n])\nseventh_pipe_params = {\n    'rf__n_estimators':[10, 40, 100],\n    'rf__max_depth':[10,40,None]\n}",
        "text": "decide on a pipeline for data to pas through",
        "id": 957
    },
    {
        "code": "\nX_categorical = pd.get_dummies(abalone_data[categorical_columns]).astype(int).ix[:,:-1]\nX_numeric = abalone_data[numeric_columns]\nX_numeric[numeric_columns] = StandardScaler().fit_transform(X_numeric)\ny = abalone_data[target]\nX_final = pd.concat((X_numeric,X_categorical),axis=1)",
        "text": "now let 's preprocess it in the standard way i ve show you , 1 .  let 's convert the categorical column use one   -   hot encode 2 .  standard scale ( z   -   score ) the numeric column",
        "id": 958
    },
    {
        "code": "import torch.nn as nn\nimport torch.nn.functional as F\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.fc1 = nn.Linear(28 * 28, 1)\n    def forward(self, x):\n        \n        x = x.view(-1, 28 * 28)\n        \n        x = F.relu(self.fc1(x))\n        return x\nmodel = Net()\nprint(model)",
        "text": "define the network [ architecture ] ( <url> ) the architecture will be responsible for see a input a 784   -   dim tensor of pixel value for each image , and produce a tensor of length 10 ( our number of class ) that indicate the class score for an input image  .  this particular example us two hide layer and dropout to avoid overfitting  . ",
        "id": 959
    },
    {
        "code": "train[0][0], train[1][0]\nget_distance(train[0][0], train[1][0])",
        "text": "let 's use the function to compute , for example , the distance between the first two train point ,",
        "id": 960
    },
    {
        "code": "\nprint (\"Original dataframe\")\ntransformer.show(5)\nprint (' Multiplying by 20 a number if value in cell is greater than 20:')\nfunc = lambda cell: (cell * 20) if ((cell != None) and (cell < 20)) else cell\ntransformer.set_col(['price'], func, 'integer')\ntransformer.show(20)",
        "text": "set a custom transformation the core of this function be base on the user define function provide from the lambda function provide in the func  argument  .  in this example , cell that be not great than 20 , be multiply by 20 , the rest of them stay intact  . ",
        "id": 961
    },
    {
        "code": "smaller = int(input('What is your smaller number? '))\nlarger = int(input('What is your larger number? '))\nproduct = 1\nfor i in range(smaller+1, larger, 2):\n    print(i)\n    product = product*i\nprint(product)",
        "text": "suppose you want to ask the user for two number and multiply every other number between those number ( non   -   inclusive )  .  how would you do it ?",
        "id": 962
    },
    {
        "code": "t = np.linspace(-np.pi, np.pi, N)\nx = np.sin(t)\nfig, ax = plt.subplots(1)\nax.plot(x)\ny = linear_function(x, a, b)\nfig, ax = plt.subplots(1)\nax.plot(y, '.')\nfig, ax = plt.subplots(1)\nax.plot(x, y, '.')\nax.set_xlabel('x')\nax.set_ylabel('y')",
        "text": "and this linear relationship hold , even when the input be not a linear function",
        "id": 963
    },
    {
        "code": "from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_predtest)\ncm\nfrom sklearn.metrics import classification_report\ncr=classification_report(y_test,y_predtest)\nprint(cr)\nfrom sklearn.metrics import accuracy_score\naccuracy_test= accuracy_score(y_test, y_predtest)\nprint(accuracy_test)\nfrom sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_predtest)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplt.plot(false_positive_rate, true_positive_rate)\nplt.plot([0,1],[0,1],'r--')",
        "text": "step   -   7 evaluate the model prediction",
        "id": 964
    },
    {
        "code": "pd.set_option('display.max_columns', 100)\ndf.head()\npd.set_option('display.max_rows', 100)\ndf.shape\ndf.describe().T",
        "text": "take a high   -   level overview of the data",
        "id": 965
    },
    {
        "code": "z3 = [-0.9463*v1/0.9884 + 2.1279/0.9884 for v1 in x1]\nplt.scatter(x1,x2, c=ex_df['y'])\nplt.plot(x1,z3, linestyle='-', color='r')\nplt.title('statsmodels logistic regression line')",
        "text": "plot the line divide the x1 , x2 plane into the two region , from the statsmodel regression",
        "id": 966
    },
    {
        "code": "import em1ds as zpic\nelectrons = zpic.Species( \"electrons\", -1.0, ppc = 64, uth=[0.005,0.005,0.005])\nsim = zpic.Simulation( nx = 1000, box = 100.0, dt = 0.0999, species = electrons )\nsim.emf.solver_type = 'PSATD'\nBx0 = 2.0\nsim.emf.set_ext_fld('uniform', B0= [Bx0, 0.0, 0.0])",
        "text": "to study electromagnetic wave in a magnetize plasma , in particular wave propagate along the apply magnetic field , we initialize the simulation with a uniform thermal plasma , effectively inject wave of all possible wavelength into the simulation  .  the external magnetic field be apply along the  x  direction , and can be control through the bx0  variable ,",
        "id": 967
    },
    {
        "code": "A = rand(3, 3);\nx = fill(1, (3,));\nb = A * x;\nprint(A)\nprint(x)\nprint(b)",
        "text": "before we get start , let 's set up a linear system",
        "id": 968
    },
    {
        "code": "total_births = pd.pivot_table(all_names, values= 'births', index= 'year', columns='sex', aggfunc= sum)\ntotal_births.tail()",
        "text": "explore total birth by year",
        "id": 969
    },
    {
        "code": "error_rate = []\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,Y_train)\n    prediction_i = knn.predict(X_test)\n    error_rate.append(np.mean(prediction_i!=Y_test))\nerror_rate",
        "text": "choose a k value let 's go ahead and use the elbow method to pick a good k value   -  create a for loop that train various knn model with different k value , then keep track of the error _ rate for each of these model with a list   - ",
        "id": 970
    },
    {
        "code": "from __future__ import print_function\nfrom builtins import input\nfrom builtins import range\nimport pyfftw   \nimport numpy as np\nfrom scipy.ndimage import zoom\nimport imageio\nfrom sporco.dictlrn import cbpdndl\nfrom sporco import util\nfrom sporco import plot\nplot.config_notebook_plotting()",
        "text": "convolutional dictionary learn   -  this example demonstrate the use of [ dictlrn . cbpdndl . convbpdndictlearn ] ( <url> ) for learn a 3d convolutional dictionary from video data  .  the dictionary learn algorithm be base on the admm consensus dictionary update  . ",
        "id": 971
    },
    {
        "code": "brooklyn_beer[brooklyn_beer['Brewery'].str.contains('Sixpoint Craft Ales', na=False)]",
        "text": "what be the five most popular style of beer produce by sixpoint ?   - ",
        "id": 972
    },
    {
        "code": "from pymongo import MongoClient\nclient=MongoClient('localhost',27017)\ndb=client.pythonbicookbook\ncustomers=db.customers\ncustomers.find_one_and_update(\n    {\n        \"first_name\":\"Bob\",\"last_name\":\"Smith\"\n    },\n    {'$set':{'contacted':False,'updated_at':datetime.datetime.utcnow()}}\n)",
        "text": "update a single record use pymongo",
        "id": 973
    },
    {
        "code": "x = np.random.random_sample(100)*100\ny = x + np.random.normal(np.random.normal(0,15), 30, size=100)\nplt.figure(figsize=(10,8))\nplt.scatter(x, y, s=70, c='steelblue')\nplt.show()",
        "text": "run gradient descent on regression data first let make some x and y variable like we do yesterday  . ",
        "id": 974
    },
    {
        "code": "\ndef is_prime(n):\n    if n == 2:\n        return True\n    \n    for i in range(2,n):\n        if n % i ==0:\n            return False\n    return True\nsum=0\nfor j in range(2,2000):\n    if is_prime(j):\n        sum+=j\nprint(sum)",
        "text": "challenge 1 . 2 , summation of prime the sum of the prime below 10 be 2 + 3 + 5 + 7 = 17 .  find the sum of all the prime below two   -   thousand  . ",
        "id": 975
    },
    {
        "code": "\nresponse = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\npage_source = response.text\nsoup = BeautifulSoup(page_source, \"html5lib\")",
        "text": "collect information   -  believe it or not , that 's all you need to scrape a website  .  let 's apply these skill to scrape the [ 98th general assembly ] ( <url> )  .  our goal be to scrape information on each senator , include their , * name * district * party    -  2 . 1 first , make the get request and *soup* it",
        "id": 976
    },
    {
        "code": "logreg_q3 = linear_model.LogisticRegression(C=1e5)\nlogreg_q3.fit(summary_train_tfidf, Y_respon_train)\nprediction['Logistic_q3'] = logreg_q3.predict(summary_val_tfidf)\nprobability['Logistic_q3'] = logreg_q3.predict_proba(summary_val_tfidf)\nMis_val_q3['Mod1'] = sum(Y_respon_val['ResponsivenessHH']!=prediction['Logistic_q3'])/float(len(Y_respon_val))\nMis_train_q3['Mod1'] = sum(Y_respon_train['ResponsivenessHH']!=logreg_q3.predict(summary_train_tfidf))/float(len(Y_respon_train))",
        "text": "model 1   -   logsitic regression with l2 penalty   -   unbalance",
        "id": 977
    },
    {
        "code": "widgets.Dropdown(\n    options={'One': 1, 'Two': 2, 'Three': 3},\n    value=2,\n    description='Number:',\n)",
        "text": "the follow be also valid ,",
        "id": 978
    },
    {
        "code": "def create_target(segments):\n    y = np.zeros(faces.target.shape[0])\n    for (start, end) in segments:\n        y[start:end + 1] = 1\n    return y\ntarget_glasses = create_target(glasses)\nX_train, X_test, y_train, y_test = train_test_split(\n        faces.data, target_glasses, test_size=0.25, random_state=0)",
        "text": "create train and test set for the new problem",
        "id": 979
    },
    {
        "code": "\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=45)\nfrom sklearn.linear_model import LogisticRegression \nlogreg = LogisticRegression() \nlogreg.fit(X_train, y_train) \ny_pred_class = logreg.predict(X_test)\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))",
        "text": "train model with train and test split data",
        "id": 980
    },
    {
        "code": "\nparams = {'project' : 'en.wikipedia.org',\n            'access' : 'mobile-app',\n            'agent' : 'user',\n            'granularity' : 'monthly',\n            'start' : '2015070100',\n            'end' : '2017100100'\n         }\napi_call = requests.get(endpoint.format(**params))\nresponse_pageviews_mobile_app_201507_201709 = api_call.json()\nwith open('data/pageviews_mobile_app_201507_201709.json', 'w') as outfile:\n    json.dump(response_pageviews_mobile_app_201507_201709, outfile)",
        "text": "call the pageviews api ( documentation , endpoint ) that provide access to mobile app traffic data from july 2015 through september 2017 and save the raw result into a json source data file  . ",
        "id": 981
    },
    {
        "code": "\nparam_grid = {'C': np.logspace(-1, 1, 3), 'gamma': [.5, 1, 2, 3]}\ngrid_search = GridSearchCV(SVC(kernel='rbf', probability=True),\n                           param_grid =param_grid, cv=5,\n                           scoring='roc_auc',\n                           n_jobs=-1, error_score=0)\ngrid_search.fit(X_train_sc, y_train)\nsvm = grid_search\nprint(\"Best parameters: {}\".format(svm.best_params_))\nprint(\"Best cross-validation AUC: {:.4f}\".format(svm.best_score_))\nfpr_rate = 0.1\nauc, threshold = draw_roc_curve(y_test, X_test_sc, svm, fpr_rate, flag=1)\n# print(svm.score(X_test_sc, y_test))",
        "text": "nonlinear model , svm with kernel",
        "id": 982
    },
    {
        "code": "thinkplot.cdf(cdf_age, label='age', complement=True)\ndecorate(title='Distribution of age', \n         xlabel='Age (years)', \n         ylabel='Complementary CDF, log scale',\n         yscale='log')",
        "text": "here 's the complementary cdf on a log   -   y scale  .  interpretation ,",
        "id": 983
    },
    {
        "code": "response = thinkdsp.read_wave('180960__kleeb__gunshot.wav')\nstart = 0.12\nresponse = response.segment(start=start)\nresponse.shift(-start)\nresponse.normalize()\nresponse.plot()\nthinkplot.config(xlabel='time (s)', \n                 ylabel='amplitude', \n                 ylim=[-1.05, 1.05], \n                 legend=False)",
        "text": "up next be one of the cool example in think dsp  .  it us lti system theory to characterize the acoustic of a record space and simulate the effect this space would have on the sound of a violin performance  .  i ll start with a record of a gunshot ,",
        "id": 984
    },
    {
        "code": "\nviews = [50, 100, 200, 400, 800]\ny_a = [get_pdf(x, site_a[:view]) for view in views]\nlabels = ['Posterior After {} Views'.format(view) for view in views]\nfor y in (y_a):\n    plot_with_fill(x, y, label)\nplot_with_fill(x, y_prior, 'Prior')\nfor y, label in zip(y_a, labels):\n    plot_with_fill(x, y, label)\nplt.title('Site A')\nplt.xlabel('Click Through Rate')\nplt.xlim([0, 0.4]);",
        "text": "after 50 view , we re start to hone in on our prediction of *pa  -  overlay on the same graph the posterior after 50 view , 100 view , 200 view , 400 view and finally all 800 view  .  you should see a time progress that we get more certain of the true value of *pa  - ",
        "id": 985
    },
    {
        "code": "np.random.seed(0) \nsim1 = np.random.poisson(t1_avg, n)\nsim2 = np.random.poisson(t2_avg, n)\nassert len(sim1)==n\nassert len(sim2)==n\nassert sim1.dtype==np.dtype(int)\nassert sim2.dtype==np.dtype(int)\nassert np.abs(sim1.mean()-t1_avg) < 0.05\nassert np.abs(sim2.mean()-t2_avg) < 0.05",
        "text": "simulate  n  game for each team use a poisson distribution $ poi ( \\lambda ) $ with $ \\lambda $ choose appropriately for the team  .  store the number of goal for each team in a numpy array name sim1  and sim2  ,",
        "id": 986
    },
    {
        "code": "env = gym.make('CartPole-v0')\nstate = env.reset()\nimg = plt.imshow(env.render(mode='rgb_array'))\nfor t in range(200):\n    action = policy.act(state,\"det\")\n    img.set_data(env.render(mode='rgb_array')) \n    plt.axis('off')\n    display.display(plt.gcf())\n    display.clear_output(wait=True)\n    state, reward, done, _ = env.step(action)\n    if done:\n        break \nenv.close()",
        "text": "watch a smart agent  . ",
        "id": 987
    },
    {
        "code": "df.index = df['Data']\ndf.drop('Data', axis=1 , inplace=True)\nt = df.index.values\nx = df.iloc[:,0]\npl.plot(x)\npl.show\ndf.head()",
        "text": "transform the  data  column to index",
        "id": 988
    },
    {
        "code": "from sklearn.linear_model import SGDClassifier                                        \nsdgc = SGDClassifier(max_iter=300, power_t=0.25, shuffle=False)        \nsdgc.fit(X_train, y_train)                                                           \nfrom sklearn.metrics import mean_squared_error\nsdgc_prediction = sdgc.predict(X_test)                                   \nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test,sdgc_prediction)                         \nprint('The accuracy of the SGD Classifier is:',accuracy)\nprint(y_test[:5])                                               \nprint(sdgc_prediction[:5])\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(sdgc, X_train, y_train, cv=5)",
        "text": "< p style=  font   -   family , arial , font   -   size,1 . 75em , color , purple , font   -   style , bold  >   ( 4 ) sgd classifier",
        "id": 989
    },
    {
        "code": "Oracle = make_odd_oracle(2)\nb0 = TP(zero, zero, zero)\nb1 = TP(zero, one, zero)\nb2 = TP(one, zero, zero)\nb3 = TP(one, one, zero)\nPrint(r'$%s \\rightarrow %s$'%( dirac(b0), dirac(Oracle*b0) ))\nPrint(r'$%s \\rightarrow %s$'%( dirac(b1), dirac(Oracle*b1) ))\nPrint(r'$%s \\rightarrow %s$'%( dirac(b2), dirac(Oracle*b2) ))\nPrint(r'$%s \\rightarrow %s$'%( dirac(b3), dirac(Oracle*b3) ))",
        "text": "test a two bite oracle",
        "id": 990
    },
    {
        "code": "minimum = optimize.fmin(f, -2)\nprint(minimum)\nminimum2 = optimize.fmin(f, 2)\nprint(minimum2)",
        "text": "calculate numerically the two minimum of the function $ f   -  for which value of $ x $ do we have a minimum ?",
        "id": 991
    },
    {
        "code": "sf['Country']\ndef transform_country(country):\n    if country == 'USA':\n        return 'United States'\n    else:\n        return country\ntransform_country('Brazil')\ntransform_country('USA')\nsf['Country'].apply(transform_country)\nsf\nsf['Country'] = sf['Country'].apply(transform_country)\nsf\nsf['Country'] = sf['Country'].apply(lambda country: 'The Best' if country == 'United States' else country)\nsf",
        "text": "use the apply function to do an advance transformation",
        "id": 992
    },
    {
        "code": "median_per_column = housing_data_train_numeric.apply(lambda x: x.mean(),axis=0)\nnumeric_median_filled = housing_data_train_numeric.fillna(median_per_column,axis=0)",
        "text": "impute numeric data with median value",
        "id": 993
    },
    {
        "code": "import numpy as np\nacc = mt.accuracy_score(y,yhat)\nconf = mt.confusion_matrix(y,yhat)\nprint(\"accuracy of Random Forest Model\", acc )\nprint(\"confusion matrix of Random Forest Model\\n\",conf)\nprint('Cost Matrix \\n', cb)\nprint ('Cost of the Random Forest model =',find_expected_value(conf, cb))\nmodel_acc['Random-Forest']=acc*100\nmodel_fp['Random-Forest']=conf[1][0]\nmodel_tp['Random-Forest']=conf[0][0]\nmodel_cost['Random-Forest']=find_expected_value(conf, cb)\nmodel_time['Random-Forest']=(end_time - start_time)\n#print(model_time)",
        "text": "confusion , cost matrix and cost for random forest model with 10   -   fold stratify cross   -   validation",
        "id": 994
    },
    {
        "code": "mg.set_edges_weights(intra_layer_edges_weight=2,\n                     inter_layer_edges_weight=3)",
        "text": "weight can be add to the edge",
        "id": 995
    },
    {
        "code": "psu_fig = create_figure('MAD', 'Pearson', 'PSU')\nextra = setdefault_style()\npsu_fig.circle('MAD', 'Pearson', source=ColumnDataSource(psu), legend='Ross', color=Blues7[0], **extra)\npsu_fig.cross('MAD', 'Pearson', source=ColumnDataSource(other), legend='Other', color=Blues7[6], **extra)\nshow(psu_fig)\nbokeh.io.save(obj=psu_fig, \n              filename='/tmp/psu-mad-vs-pearson.html',\n              resources=bokeh.resources.CDN,\n              title='PSU MAD vs Pearson')",
        "text": "psu mad v pearson < a href=    -   index  > back to index",
        "id": 996
    },
    {
        "code": "a[:,:] = np.array([30,31,32])\na",
        "text": "assign 1d array to all row of 2d array",
        "id": 997
    },
    {
        "code": "X = tf.placeholder(X_train.dtype, shape=[None,n_inputs])\ny = tf.placeholder(y_train.dtype)",
        "text": "placeholder for input and label",
        "id": 998
    },
    {
        "code": "def test_lsgan_loss(score_real, score_fake, d_loss_true, g_loss_true):\n    score_real = torch.Tensor(score_real).type(dtype)\n    score_fake = torch.Tensor(score_fake).type(dtype)\n    d_loss = ls_discriminator_loss(score_real, score_fake).cpu().numpy()\n    g_loss = ls_generator_loss(score_fake).cpu().numpy()\n    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\ntest_lsgan_loss(answers['logits_real'], answers['logits_fake'],\n                answers['d_loss_lsgan_true'], answers['g_loss_lsgan_true'])",
        "text": "before run a gin with our new loss function , let 's check it ,",
        "id": 999
    },
    {
        "code": "rnd_batches = get_in_batches(val_path, batch_size=batch_size, shuffle=True)\nval_res = [model.evaluate_generator(rnd_batches, rnd_batches.samples) for i in range(5)]\nnp.round(val_res, 3)",
        "text": "validate the model performance on the val set run the evaluate generator return the cost and accuracy of the model  .  do it in a loop allow u to confirm that the performance be stable  .  result should be very similar for all run  .  this take  _ very _  long though ( a long a train or more  -  will have to look into this )",
        "id": 1000
    }
]